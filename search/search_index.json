{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mahsm Documentation","text":"<p>Build production-grade AI systems with declarative simplicity.</p> <p>mahsm is a Python framework that combines the best tools for building, tracing, and evaluating LLM-powered applications\u2014wrapped in a simple, declarative API.</p>"},{"location":"#what-is-mahsm","title":"What is mahsm?","text":"<p>mahsm integrates four powerful frameworks into a unified development experience:</p> <ul> <li>DSPy \u2192 Prompt engineering through programming</li> <li>LangGraph \u2192 Stateful, cyclical agent workflows  </li> <li>Langfuse \u2192 Production-grade observability</li> <li>EvalProtocol \u2192 Systematic evaluation &amp; testing</li> </ul> <p>Instead of learning four different APIs, you learn one: mahsm's declarative interface.</p>"},{"location":"#why-mahsm","title":"Why mahsm?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Building production LLM applications requires: 1. Smart prompting (DSPy's modules &amp; optimizers) 2. Complex workflows (LangGraph's state machines) 3. Deep observability (Langfuse's tracing) 4. Rigorous testing (EvalProtocol's evaluations)</p> <p>Each framework has its own API, patterns, and integration challenges.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>mahsm provides:</p> <pre><code>import mahsm as ma\nimport dspy\nimport os\n\n# 1. Configure once\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()  # Automatic tracing for everything\n\n# 2. Define agents declaratively\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.researcher = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.researcher(question=question)\n\n# 3. Build workflows visually\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"research\", Researcher())\nworkflow.add_edge(ma.START, \"research\")\ngraph = workflow.compile()\n\n# 4. Run &amp; automatically trace\nresult = graph.invoke({\"question\": \"...\"})\n# \u2705 All LLM calls traced to Langfuse\n# \u2705 Full execution graph visible\n# \u2705 Costs &amp; latencies tracked\n\n# 5. Evaluate systematically\n@ma.testing.evaluation_test(...)\nasync def test_quality(row):\n    return await ma.testing.aha_judge(row, rubric=\"...\")\n# \u2705 Results synced to Langfuse\n# \u2705 Model comparisons automated\n</code></pre> <p>Result: You write less code, iterate faster, and ship with confidence.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#declarative-api","title":"\ud83c\udfaf Declarative API","text":"<p>Define what you want, not how to build it:</p> <pre><code># Instead of manually chaining prompts...\n@ma.dspy_node\nclass MyAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n</code></pre>"},{"location":"#automatic-tracing","title":"\ud83d\udd04 Automatic Tracing","text":"<p>One line enables observability for all frameworks:</p> <pre><code>ma.tracing.init()\n# \u2705 DSPy modules traced\n# \u2705 LangGraph nodes traced\n# \u2705 Custom @observe functions traced\n</code></pre>"},{"location":"#unified-testing","title":"\ud83d\udcca Unified Testing","text":"<p>Test across models, prompts, and configurations:</p> <pre><code>@ma.testing.evaluation_test(\n    completion_params=[\n        {\"model\": \"openai/gpt-4o-mini\"},\n        {\"model\": \"openai/gpt-4o\"},\n    ]\n)\nasync def test_agent(row):\n    # Runs on both models, compares results\n    pass\n</code></pre>"},{"location":"#production-ready","title":"\ud83d\ude80 Production-Ready","text":"<ul> <li>Type-safe state management (TypedDict)</li> <li>Structured logging with Langfuse</li> <li>Automated evaluation pipelines</li> <li>Cost &amp; latency tracking</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mahsm\n</code></pre>"},{"location":"#your-first-agent-60-seconds","title":"Your First Agent (60 seconds)","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# Define state\nclass State(TypedDict):\n    question: str\n    answer: str\n\n# Define agent\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Build graph\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"question\": \"What is DSPy?\"})\nprint(result[\"answer\"])\n# Output visible in Langfuse UI automatically!\n</code></pre> <p>Next: Follow the Quick Start Guide for a complete walkthrough.</p>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#new-to-llm-development","title":"\ud83c\udf93 New to LLM Development?","text":"<p>Start here to learn the fundamentals:</p> <ol> <li>Installation - Set up your environment</li> <li>Core Concepts - Understanding the mahsm philosophy</li> <li>Your First Agent - Build a complete agent step-by-step</li> </ol>"},{"location":"#want-to-understand-the-building-blocks","title":"\ud83d\udd27 Want to Understand the Building Blocks?","text":"<p>Deep dive into each framework:</p> <ul> <li>DSPy Basics - Signatures, modules, optimizers</li> <li>LangGraph Basics - State, nodes, edges, routing</li> <li>Langfuse Basics - Tracing, observability, scoring</li> <li>EvalProtocol Basics - Testing, evaluation, metrics</li> </ul>"},{"location":"#ready-to-build","title":"\ud83d\ude80 Ready to Build?","text":"<p>Check out complete examples:</p> <ul> <li>Research Agent - Multi-step reasoning pipeline</li> <li>Multi-Agent System - Coordinated agent teams</li> <li>Evaluation Pipeline - Comprehensive testing setup</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>mahsm is built on four pillars:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Your Application                \u2502\n\u2502   (Agents, Workflows, Evaluations)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 mahsm API\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              mahsm Core                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  @dspy   \u2502 .tracing \u2502   .testing   \u2502    \u2502\n\u2502  \u2502  _node   \u2502  .init() \u2502 .evaluation  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  DSPy   \u2502 \u2502Langfuse \u2502 \u2502EvalProtocol \u2502\n   \u2502 Modules \u2502 \u2502 Tracing \u2502 \u2502    Tests    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502        LangGraph Workflows          \u2502\n   \u2502   (StateGraph, compile, invoke)     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Points: - DSPy powers intelligent prompting - LangGraph orchestrates execution - Langfuse traces everything automatically - EvalProtocol validates quality</p> <p>mahsm's <code>@dspy_node</code> decorator bridges DSPy modules and LangGraph nodes, while <code>ma.tracing.init()</code> instruments the entire stack.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udcd6 Documentation: You're reading it! Explore the sidebar \u2192</li> <li>\ud83d\udcac GitHub Discussions: Ask questions</li> <li>\ud83d\udc1b Issues: Report bugs</li> <li>\u2b50 Star the repo: Show your support</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Installation Guide \u2192 Set up mahsm</li> <li>Quick Start \u2192 Build your first agent  </li> <li>DSPy Overview \u2192 Learn prompt engineering</li> <li>LangGraph Overview \u2192 Learn workflows</li> </ul> <p>Ready to build? Let's go! \ud83d\ude80</p>"},{"location":"building-blocks/dspy/best-practices/","title":"DSPy Best Practices","text":"<p>TL;DR: Production-ready patterns for building robust, maintainable DSPy applications with mahsm.</p>"},{"location":"building-blocks/dspy/best-practices/#module-design","title":"Module Design","text":""},{"location":"building-blocks/dspy/best-practices/#keep-modules-focused","title":"\u2705 Keep Modules Focused","text":"<p>Each module should have a single, clear responsibility:</p> <pre><code># \u2705 Good: Focused modules\nclass QueryGenerator(dspy.Module):\n    \"\"\"Only generates search queries.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.ChainOfThought(\"question -&gt; search_query\")\n\nclass ResultSynthesizer(dspy.Module):\n    \"\"\"Only synthesizes results.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.synth = dspy.ChainOfThought(\"question, results -&gt; answer\")\n\n# \u274c Bad: Does too much\nclass MegaModule(dspy.Module):\n    \"\"\"Tries to do everything.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.do_everything = dspy.ChainOfThought(\"anything -&gt; everything\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-descriptive-signatures","title":"\u2705 Use Descriptive Signatures","text":"<p>Make your signatures self-documenting:</p> <pre><code># \u2705 Good: Clear and descriptive\n\"user_question, search_results -&gt; synthesized_answer, confidence_score\"\n\n# \u274c Bad: Vague\n\"input -&gt; output\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#match-signatures-to-state","title":"\u2705 Match Signatures to State","text":"<p>When using <code>@ma.dspy_node</code>, align signature fields with state keys:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]\n\n# \u2705 Signature matches state exactly\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer, confidence\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"building-blocks/dspy/best-practices/#validate-outputs","title":"\u2705 Validate Outputs","text":"<p>Always validate LLM outputs before using them:</p> <pre><code>import dspy\n\nclass SafeQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        result = self.qa(question=question)\n\n        # Validate output\n        if not result.answer or len(result.answer) &lt; 10:\n            # Fallback or retry logic\n            return {\"answer\": \"I don't have enough information to answer that.\"}\n\n        return {\"answer\": result.answer}\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#handle-api-failures","title":"\u2705 Handle API Failures","text":"<p>Wrap LLM calls with error handling:</p> <pre><code>import dspy\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass RobustQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10)\n    )\n    def forward(self, question):\n        try:\n            result = self.qa(question=question)\n            return {\"answer\": result.answer}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            # Log to monitoring system\n            raise\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"building-blocks/dspy/best-practices/#use-appropriate-module-types","title":"\u2705 Use Appropriate Module Types","text":"<p>Choose the right module for the task:</p> <pre><code># \u2705 Simple tasks \u2192 Predict (fast)\nclassifier = dspy.Predict(\"text -&gt; category\")\n\n# \u2705 Complex reasoning \u2192 ChainOfThought (better quality)\nreasoner = dspy.ChainOfThought(\"problem -&gt; solution\")\n\n# \u2705 Tool use \u2192 ReAct (agentic)\nagent = dspy.ReAct(\"question -&gt; answer\", tools=tools)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#cache-expensive-operations","title":"\u2705 Cache Expensive Operations","text":"<p>Cache module compilations and optimizations:</p> <pre><code>import dspy\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_optimized_qa():\n    \"\"\"Cache the optimized module.\"\"\"\n    qa = QA()\n\n    # Check if we have a saved version\n    try:\n        qa.load(\"optimized_qa.json\")\n        return qa\n    except FileNotFoundError:\n        # Optimize and save\n        optimizer = BootstrapFewShot(metric=accuracy)\n        optimized = optimizer.compile(qa, trainset=trainset)\n        optimized.save(\"optimized_qa.json\")\n        return optimized\n\n# Use cached version\nqa_module = get_optimized_qa()\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#batch-when-possible","title":"\u2705 Batch When Possible","text":"<p>Process multiple inputs together:</p> <pre><code>class BatchQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward_batch(self, questions):\n        \"\"\"Process multiple questions efficiently.\"\"\"\n        # DSPy handles batching internally\n        results = [self.qa(question=q) for q in questions]\n        return [r.answer for r in results]\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#testing","title":"Testing","text":""},{"location":"building-blocks/dspy/best-practices/#unit-test-your-modules","title":"\u2705 Unit Test Your Modules","text":"<p>Test modules independently:</p> <pre><code>import unittest\nimport dspy\n\nclass TestQA(unittest.TestCase):\n    def setUp(self):\n        # Use a mock LM for testing\n        lm = dspy.LM('openai/gpt-4o-mini', api_key=\"test\")\n        dspy.configure(lm=lm)\n        self.qa = QA()\n\n    def test_qa_returns_answer(self):\n        result = self.qa(question=\"What is 2+2?\")\n        self.assertIn(\"answer\", result)\n        self.assertIsInstance(result[\"answer\"], str)\n\n    def test_qa_handles_empty_question(self):\n        result = self.qa(question=\"\")\n        # Should handle gracefully\n        self.assertIsNotNone(result[\"answer\"])\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#test-with-real-examples","title":"\u2705 Test with Real Examples","text":"<p>Create a test dataset:</p> <pre><code>import dspy\n\ntest_cases = [\n    dspy.Example(\n        question=\"What is the capital of France?\",\n        expected_answer=\"Paris\"\n    ).with_inputs(\"question\"),\n\n    dspy.Example(\n        question=\"What is 2+2?\",\n        expected_answer=\"4\"\n    ).with_inputs(\"question\"),\n]\n\ndef test_accuracy():\n    qa = QA()\n    correct = 0\n\n    for case in test_cases:\n        result = qa(question=case.question)\n        if case.expected_answer.lower() in result[\"answer\"].lower():\n            correct += 1\n\n    accuracy = correct / len(test_cases)\n    assert accuracy &gt;= 0.8, f\"Accuracy too low: {accuracy}\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#configuration-management","title":"Configuration Management","text":""},{"location":"building-blocks/dspy/best-practices/#use-environment-variables","title":"\u2705 Use Environment Variables","text":"<p>Never hardcode API keys or configuration:</p> <pre><code>import os\nimport dspy\n\n# \u2705 Good: Environment variables\nlm = dspy.LM(\n    'openai/gpt-4o-mini',\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    max_tokens=int(os.getenv(\"MAX_TOKENS\", \"2000\")),\n    temperature=float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n)\ndspy.configure(lm=lm)\n\n# \u274c Bad: Hardcoded\n# lm = dspy.LM('openai/gpt-4o-mini', api_key=\"sk-...\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#create-configuration-classes","title":"\u2705 Create Configuration Classes","text":"<p>Organize configuration:</p> <pre><code>from dataclasses import dataclass\nimport os\n\n@dataclass\nclass DSPyConfig:\n    model: str = \"openai/gpt-4o-mini\"\n    api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    max_tokens: int = 2000\n    temperature: float = 0.7\n\n    @classmethod\n    def from_env(cls):\n        return cls(\n            model=os.getenv(\"DSPY_MODEL\", \"openai/gpt-4o-mini\"),\n            api_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n            max_tokens=int(os.getenv(\"MAX_TOKENS\", \"2000\")),\n            temperature=float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n        )\n\n# Use it\nconfig = DSPyConfig.from_env()\nlm = dspy.LM(config.model, api_key=config.api_key, max_tokens=config.max_tokens)\ndspy.configure(lm=lm)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"building-blocks/dspy/best-practices/#always-enable-tracing","title":"\u2705 Always Enable Tracing","text":"<p>Use mahsm's tracing in production:</p> <pre><code>import mahsm as ma\n\n# Enable at application startup\nma.tracing.init()\n\n# All DSPy calls are now traced to Langfuse!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#add-custom-metrics","title":"\u2705 Add Custom Metrics","text":"<p>Track custom metrics with Langfuse:</p> <pre><code>from langfuse.decorators import observe\n\n@observe(name=\"qa_pipeline\")\ndef run_qa(question: str):\n    result = qa_module(question=question)\n\n    # Log custom metrics\n    from langfuse import Langfuse\n    client = Langfuse()\n    client.score(\n        name=\"answer_length\",\n        value=len(result[\"answer\"])\n    )\n\n    return result\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#log-important-events","title":"\u2705 Log Important Events","text":"<p>Use structured logging:</p> <pre><code>import logging\nimport dspy\n\nlogger = logging.getLogger(__name__)\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        logger.info(f\"Processing question: {question[:50]}...\")\n\n        try:\n            result = self.qa(question=question)\n            logger.info(f\"Generated answer of length {len(result.answer)}\")\n            return {\"answer\": result.answer}\n        except Exception as e:\n            logger.error(f\"Failed to answer: {e}\", exc_info=True)\n            raise\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"building-blocks/dspy/best-practices/#use-chainofthought-for-complex-tasks","title":"\u2705 Use ChainOfThought for Complex Tasks","text":"<p>Enable reasoning for better outputs:</p> <pre><code># \u2705 Good for complex tasks\nreasoner = dspy.ChainOfThought(\"problem -&gt; solution\")\n\n# \u274c Bad for complex tasks (no reasoning)\npredictor = dspy.Predict(\"problem -&gt; solution\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#add-context-to-signatures","title":"\u2705 Add Context to Signatures","text":"<p>Guide the model with descriptions:</p> <pre><code># \u2705 Good: Descriptive\nsignature = \"question: a user's technical question -&gt; answer: a detailed, accurate response with examples\"\n\n# \u274c Bad: Vague\nsignature = \"question -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-few-shot-examples","title":"\u2705 Use Few-Shot Examples","text":"<p>Optimize with BootstrapFewShot:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\nqa = QA()\noptimizer = BootstrapFewShot(metric=accuracy, max_bootstrapped_demos=4)\noptimized_qa = optimizer.compile(qa, trainset=examples)\n\n# optimized_qa now includes learned examples\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"building-blocks/dspy/best-practices/#separate-concerns","title":"\u2705 Separate Concerns","text":"<p>Organize code into modules:</p> <pre><code>my_project/\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 dspy_config.py       # Configuration\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 qa.py                # QA module\n\u2502   \u251c\u2500\u2500 summarizer.py        # Summarizer module\n\u2502   \u2514\u2500\u2500 classifier.py        # Classifier module\n\u251c\u2500\u2500 workflows/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 research_pipeline.py # LangGraph workflows\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 metrics.py           # Evaluation metrics\n\u2514\u2500\u2500 main.py                  # Application entry point\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-type-hints","title":"\u2705 Use Type Hints","text":"<p>Make code maintainable with types:</p> <pre><code>from typing import Dict, Any, Optional\nimport dspy\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Answer a question.\n\n        Args:\n            question: The user's question\n\n        Returns:\n            Dictionary with 'answer' key\n        \"\"\"\n        result = self.qa(question=question)\n        return {\"answer\": result.answer}\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#deployment","title":"Deployment","text":""},{"location":"building-blocks/dspy/best-practices/#use-versioned-artifacts","title":"\u2705 Use Versioned Artifacts","text":"<p>Save and version optimized modules:</p> <pre><code>import dspy\nfrom datetime import datetime\n\ndef save_optimized_module(module, version: str = None):\n    \"\"\"Save module with timestamp.\"\"\"\n    if version is None:\n        version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    filename = f\"optimized_qa_{version}.json\"\n    module.save(filename)\n    print(f\"Saved to {filename}\")\n    return filename\n\n# Usage\noptimized_qa = optimizer.compile(qa, trainset=train)\nsave_optimized_module(optimized_qa, version=\"v1.0.0\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#implement-health-checks","title":"\u2705 Implement Health Checks","text":"<p>Monitor service health:</p> <pre><code>from fastapi import FastAPI, HTTPException\nimport dspy\n\napp = FastAPI()\n\n@app.get(\"/health\")\ndef health_check():\n    \"\"\"Check if DSPy is configured correctly.\"\"\"\n    try:\n        # Test configuration\n        lm = dspy.settings.lm\n        if lm is None:\n            raise Exception(\"DSPy not configured\")\n\n        return {\"status\": \"healthy\", \"model\": str(lm)}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#handle-rate-limits","title":"\u2705 Handle Rate Limits","text":"<p>Implement backoff strategies:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import RateLimitError\n\nclass RateLimitedQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    @retry(\n        retry=retry_if_exception_type(RateLimitError),\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=2, min=4, max=60)\n    )\n    def forward(self, question):\n        return self.qa(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"building-blocks/dspy/best-practices/#forgetting-superinit","title":"\u274c Forgetting super().init()","text":"<pre><code># \u274c Will break\nclass MyModule(dspy.Module):\n    def __init__(self):\n        # Missing super().__init__()!\n        self.predictor = dspy.Predict(\"input -&gt; output\")\n\n# \u2705 Correct\nclass MyModule(dspy.Module):\n    def __init__(self):\n        super().__init__()  # Always call this!\n        self.predictor = dspy.Predict(\"input -&gt; output\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#not-configuring-dspy","title":"\u274c Not Configuring DSPy","text":"<pre><code># \u274c Will error\npredictor = dspy.Predict(\"question -&gt; answer\")\nresult = predictor(question=\"Hello\")  # Error: DSPy not configured!\n\n# \u2705 Configure first\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\npredictor = dspy.Predict(\"question -&gt; answer\")\nresult = predictor(question=\"Hello\")  # Works!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#ignoring-optimization","title":"\u274c Ignoring Optimization","text":"<pre><code># \u274c Using unoptimized modules in production\nqa = QA()\n# Likely suboptimal performance\n\n# \u2705 Optimize first\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_qa = optimizer.compile(qa, trainset=train)\n# Much better performance!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#checklist-for-production","title":"Checklist for Production","text":"<p>Before deploying to production:</p> <ul> <li>[ ] All modules have <code>super().__init__()</code> calls</li> <li>[ ] DSPy is configured with proper LM</li> <li>[ ] Tracing is enabled (<code>ma.tracing.init()</code>)</li> <li>[ ] API keys are in environment variables</li> <li>[ ] Modules are optimized with real data</li> <li>[ ] Error handling is implemented</li> <li>[ ] Logging is configured</li> <li>[ ] Unit tests pass</li> <li>[ ] Integration tests pass</li> <li>[ ] Performance is acceptable (latency, cost)</li> <li>[ ] Rate limiting/backoff is handled</li> <li>[ ] Health checks are implemented</li> <li>[ ] Monitoring/alerts are set up</li> </ul>"},{"location":"building-blocks/dspy/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>LangGraph Overview \u2192 Build stateful workflows</li> <li>Production Deployment Guide \u2192 Deploy your app</li> <li>Examples \u2192 See complete applications</li> </ul>"},{"location":"building-blocks/dspy/best-practices/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Documentation - Official docs</li> <li>Langfuse Documentation - Tracing &amp; monitoring</li> <li>DSPy GitHub - Source code &amp; examples</li> </ul> <p>Ready for production? Check out Langfuse \u2192</p>"},{"location":"building-blocks/dspy/modules/","title":"DSPy Modules","text":"<p>TL;DR: Modules are reusable components that combine signatures with prompting strategies like chain-of-thought or ReAct.</p>"},{"location":"building-blocks/dspy/modules/#what-are-dspy-modules","title":"What are DSPy Modules?","text":"<p>DSPy Modules are the building blocks of your LLM pipelines. Each module: - Takes a signature (input \u2192 output specification) - Applies a prompting strategy (e.g., chain-of-thought, few-shot) - Returns structured outputs</p> <p>Think of modules as smart function wrappers that automatically generate and execute prompts.</p>"},{"location":"building-blocks/dspy/modules/#built-in-modules","title":"Built-in Modules","text":""},{"location":"building-blocks/dspy/modules/#1-dspypredict","title":"1. dspy.Predict","text":"<p>The simplest module\u2014direct prediction without reasoning.</p> <pre><code>import dspy\n\npredictor = dspy.Predict(\"question -&gt; answer\")\n\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre> <p>When to use: - Simple, straightforward tasks - When you don't need reasoning traces - Fast, low-token operations</p>"},{"location":"building-blocks/dspy/modules/#2-dspychainofthought","title":"2. dspy.ChainOfThought","text":"<p>Adds step-by-step reasoning before the final answer.</p> <pre><code>cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\nresult = cot(question=\"If a train travels 60 mph for 2.5 hours, how far does it go?\")\nprint(result.reasoning)  # \"Let me think step by step...\"\nprint(result.answer)      # \"150 miles\"\n</code></pre> <p>How it works: - Automatically adds a <code>reasoning</code> field to outputs - Prompts the model to \"think step by step\" - Better for complex reasoning tasks</p> <p>When to use: - Mathematical problems - Multi-step reasoning - When you want to see the model's thought process</p> <p>Example with mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass MathState(TypedDict):\n    problem: str\n    reasoning: str\n    solution: str\n\n@ma.dspy_node\nclass MathSolver(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.solver = dspy.ChainOfThought(\"problem -&gt; reasoning, solution\")\n\n    def forward(self, problem):\n        return self.solver(problem=problem)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(MathState)\nworkflow.add_node(\"solve\", MathSolver())\n# Both reasoning and solution are written to state!\n</code></pre>"},{"location":"building-blocks/dspy/modules/#3-dspyreact","title":"3. dspy.ReAct","text":"<p>Implements the ReAct pattern (Reasoning + Acting) for tool-using agents.</p> <pre><code>from dspy import ReAct\n\n# Define tools\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Your search implementation\n    return f\"Results for: {query}\"\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return str(eval(expression))\n\n# Create ReAct module\nreact = dspy.ReAct(\"question -&gt; answer\", tools=[search_web, calculate])\n\nresult = react(question=\"What is the population of Tokyo times 2?\")\nprint(result.answer)\n# Agent will:\n# 1. search_web(\"population of Tokyo\")\n# 2. calculate(\"37400000 * 2\")\n# 3. Return final answer\n</code></pre> <p>How it works: - Model alternates between Reasoning and Acting (tool calls) - Automatically generates tool calls based on the question - Continues until it has enough information</p> <p>When to use: - When your agent needs external tools (search, calculator, APIs) - Multi-step tasks requiring information gathering - Agentic workflows</p> <p>Example with mahsm:</p> <pre><code>@ma.dspy_node\nclass ResearchAgent(ma.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\", tools=tools)\n\n    def forward(self, question):\n        return self.react(question=question)\n\n# Define tools\ndef search_papers(query: str) -&gt; str:\n    \"\"\"Search academic papers.\"\"\"\n    return \"Paper results...\"\n\n# Use in workflow\nagent = ResearchAgent(tools=[search_papers])\nworkflow.add_node(\"research\", agent)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#4-dspyprogramofthought","title":"4. dspy.ProgramOfThought","text":"<p>Combines natural language reasoning with code execution.</p> <pre><code>pot = dspy.ProgramOfThought(\"problem -&gt; answer\")\n\nresult = pot(problem=\"Calculate the compound interest on $1000 at 5% for 3 years\")\n# Generates Python code, executes it, returns answer\nprint(result.answer)  # \"$1157.63\"\n</code></pre> <p>When to use: - Mathematical computations - Tasks requiring precise calculations - When you want guaranteed accuracy for arithmetic</p>"},{"location":"building-blocks/dspy/modules/#5-dspymultichaincomparison","title":"5. dspy.MultiChainComparison","text":"<p>Generates multiple reasoning chains and selects the best one.</p> <pre><code>mcc = dspy.MultiChainComparison(\"question -&gt; answer\", M=3)\n\nresult = mcc(question=\"What are the benefits of renewable energy?\")\n# Generates 3 different reasoning chains, picks the best\nprint(result.answer)\n</code></pre> <p>When to use: - High-stakes decisions - When you want diverse perspectives - Quality over speed</p>"},{"location":"building-blocks/dspy/modules/#custom-modules","title":"Custom Modules","text":"<p>Create your own modules by subclassing <code>dspy.Module</code>:</p> <pre><code>import dspy\n\nclass CustomPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize sub-modules\n        self.classifier = dspy.Predict(\"text -&gt; category\")\n        self.summarizer = dspy.ChainOfThought(\"text, category -&gt; summary\")\n\n    def forward(self, text):\n        # Step 1: Classify\n        category_result = self.classifier(text=text)\n        category = category_result.category\n\n        # Step 2: Summarize based on category\n        if category == \"technical\":\n            # Use chain-of-thought for complex content\n            return self.summarizer(text=text, category=category)\n        else:\n            # Simple summary for non-technical\n            return {\"summary\": text[:100]}\n</code></pre> <p>Key points: 1. Always call <code>super().__init__()</code> 2. Initialize sub-modules in <code>__init__</code> 3. Implement <code>forward()</code> method 4. Return a dict or DSPy prediction</p>"},{"location":"building-blocks/dspy/modules/#module-composition","title":"Module Composition","text":"<p>Combine modules to build complex pipelines:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, results -&gt; answer, sources\")\n        self.verify = dspy.Predict(\"answer, sources -&gt; confidence: float 0-1\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        results = self.search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        synthesis = self.synthesize(question=question, results=results)\n\n        # Step 4: Verify confidence\n        verification = self.verify(\n            answer=synthesis.answer,\n            sources=synthesis.sources\n        )\n\n        return {\n            \"answer\": synthesis.answer,\n            \"sources\": synthesis.sources,\n            \"confidence\": verification.confidence\n        }\n\n    def search_api(self, query):\n        # Your search implementation\n        return f\"Results for {query}\"\n</code></pre>"},{"location":"building-blocks/dspy/modules/#modules-in-mahsm","title":"Modules in mahsm","text":"<p>The <code>@dspy_node</code> decorator makes DSPy modules work seamlessly with LangGraph:</p>"},{"location":"building-blocks/dspy/modules/#basic-usage","title":"Basic Usage","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    question: str\n    answer: str\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Add to workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\n</code></pre>"},{"location":"building-blocks/dspy/modules/#multi-module-pipeline","title":"Multi-Module Pipeline","text":"<pre><code>class PipelineState(TypedDict):\n    question: str\n    search_query: str\n    results: str\n    answer: str\n\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.ChainOfThought(\"question -&gt; search_query\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n@ma.dspy_node\nclass Synthesizer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.synth = dspy.ChainOfThought(\"question, results -&gt; answer\")\n\n    def forward(self, question, results):\n        return self.synth(question=question, results=results)\n\n# Build workflow\nworkflow = ma.graph.StateGraph(PipelineState)\nworkflow.add_node(\"generate_query\", QueryGenerator())\nworkflow.add_node(\"search\", search_function)  # Regular function\nworkflow.add_node(\"synthesize\", Synthesizer())\n\nworkflow.add_edge(ma.START, \"generate_query\")\nworkflow.add_edge(\"generate_query\", \"search\")\nworkflow.add_edge(\"search\", \"synthesize\")\nworkflow.add_edge(\"synthesize\", ma.END)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#module-configuration","title":"Module Configuration","text":""},{"location":"building-blocks/dspy/modules/#model-selection","title":"Model Selection","text":"<p>Configure the model used by all modules:</p> <pre><code>import dspy\nimport os\n\n# Option 1: OpenAI\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 2: Anthropic\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 3: Local model\nlm = dspy.LM('ollama/llama3.1', api_base='http://localhost:11434')\ndspy.configure(lm=lm)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#per-module-configuration","title":"Per-Module Configuration","text":"<pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Use different models for different tasks\n        self.fast_predictor = dspy.Predict(\"input -&gt; output\")\n        self.complex_reasoner = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        # Configure different models per call\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o-mini')):\n            quick = self.fast_predictor(input=input)\n\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o')):\n            detailed = self.complex_reasoner(input=input)\n\n        return {\"quick\": quick.output, \"detailed\": detailed.output}\n</code></pre>"},{"location":"building-blocks/dspy/modules/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/modules/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use ChainOfThought for complex tasks <pre><code># \u2705 Better reasoning\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Compose modules for complex pipelines <pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"...\")\n        self.step2 = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Match module outputs to state fields <pre><code>class State(TypedDict):\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use ReAct for tool-using agents <pre><code>dspy.ReAct(\"question -&gt; answer\", tools=[...])\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mix Predict and ChainOfThought unnecessarily <pre><code># \u274c Inconsistent reasoning\nself.mod1 = dspy.Predict(\"q -&gt; a\")\nself.mod2 = dspy.ChainOfThought(\"q -&gt; a\")\n# Pick one strategy per pipeline\n</code></pre></p> </li> <li> <p>Forget to initialize parent class <pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        # \u274c Missing super().__init__()\n        self.predictor = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Create deeply nested modules <pre><code># \u274c Too complex\nclass A(dspy.Module):\n    def __init__(self):\n        self.b = B()\n\nclass B(dspy.Module):\n    def __init__(self):\n        self.c = C()\n# Keep it flat and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#comparison-table","title":"Comparison Table","text":"Module Use Case Reasoning Tool Use Speed Predict Simple tasks \u274c None \u274c No \u26a1\u26a1\u26a1 Fast ChainOfThought Complex reasoning \u2705 Yes \u274c No \u26a1\u26a1 Medium ReAct Tool-using agents \u2705 Yes \u2705 Yes \u26a1 Slow ProgramOfThought Math/code tasks \u2705 Yes (code) \u2705 Code exec \u26a1 Slow MultiChainComparison High-quality outputs \u2705 Multiple \u274c No \ud83d\udc0c Very slow"},{"location":"building-blocks/dspy/modules/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Optimizers \u2192 Automatically improve your modules</li> <li>Best Practices \u2192 Production tips</li> <li>Your First Agent \u2192 Build a complete agent</li> </ul>"},{"location":"building-blocks/dspy/modules/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Modules Docs - Official guide</li> <li>DSPy Examples - Real-world module usage</li> </ul> <p>Next: Learn about DSPy Optimizers \u2192</p>"},{"location":"building-blocks/dspy/optimizers/","title":"DSPy Optimizers","text":"<p>TL;DR: Optimizers (teleprompts) automatically improve your DSPy modules by learning from examples\u2014no manual prompt engineering needed.</p>"},{"location":"building-blocks/dspy/optimizers/#what-are-dspy-optimizers","title":"What are DSPy Optimizers?","text":"<p>DSPy Optimizers (also called teleprompts) are algorithms that automatically improve your modules by: - Learning from training examples - Generating better prompts - Adding few-shot demonstrations - Tuning instructions</p> <p>Instead of manually tweaking prompts, you define success criteria and let the optimizer find the best approach.</p>"},{"location":"building-blocks/dspy/optimizers/#why-use-optimizers","title":"Why Use Optimizers?","text":""},{"location":"building-blocks/dspy/optimizers/#manual-prompting-traditional","title":"Manual Prompting (Traditional)","text":"<pre><code># \u274c Manual iteration\nprompt = \"Answer the question: {question}\"\n# Try it... doesn't work well\nprompt = \"Think step by step and answer: {question}\"\n# Try again... better but not perfect\nprompt = \"You are an expert. Think carefully and answer: {question}\"\n# Keep iterating...\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#automatic-optimization-dspy","title":"Automatic Optimization (DSPy)","text":"<pre><code># \u2705 Define success metric\ndef accuracy(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# \u2705 Let optimizer find the best approach\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_module = optimizer.compile(my_module, trainset=examples)\n# Done! Module is automatically improved\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/optimizers/#1-metrics","title":"1. Metrics","text":"<p>A metric function measures success:</p> <pre><code>def my_metric(example, prediction, trace=None):\n    \"\"\"\n    Args:\n        example: Ground truth from trainset\n        prediction: Module's output\n        trace: Optional execution trace\n\n    Returns:\n        float or bool: Score (higher is better)\n    \"\"\"\n    # Simple exact match\n    return example.answer == prediction.answer\n\n# Or more nuanced\ndef f1_metric(example, prediction):\n    # Calculate F1 score\n    precision = calculate_precision(example, prediction)\n    recall = calculate_recall(example, prediction)\n    return 2 * (precision * recall) / (precision + recall)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-training-set","title":"2. Training Set","text":"<p>Examples with inputs and expected outputs:</p> <pre><code>import dspy\n\ntrainset = [\n    dspy.Example(\n        question=\"What is 2+2?\",\n        answer=\"4\"\n    ).with_inputs(\"question\"),  # Mark what's an input\n\n    dspy.Example(\n        question=\"What is the capital of France?\",\n        answer=\"Paris\"\n    ).with_inputs(\"question\"),\n]\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#3-compilation","title":"3. Compilation","text":"<p>The optimization process:</p> <pre><code>optimizer = BootstrapFewShot(metric=accuracy)\noptimized = optimizer.compile(\n    student=my_module,      # Module to optimize\n    trainset=trainset,      # Training examples\n    teacher=None            # Optional better model\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#built-in-optimizers","title":"Built-in Optimizers","text":""},{"location":"building-blocks/dspy/optimizers/#1-bootstrapfewshot","title":"1. BootstrapFewShot","text":"<p>Learns few-shot examples from your training data.</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Create optimizer\noptimizer = BootstrapFewShot(\n    metric=accuracy,\n    max_bootstrapped_demos=4,  # Number of examples to add\n    max_labeled_demos=4         # Max examples per prompt\n)\n\n# Optimize module\noptimized_qa = optimizer.compile(\n    student=qa_module,\n    trainset=train_examples\n)\n\n# optimized_qa now includes learned few-shot examples!\n</code></pre> <p>How it works: 1. Runs your module on training examples 2. Keeps successful predictions as demonstrations 3. Adds them to future prompts automatically</p> <p>When to use: - You have labeled training data - Few-shot learning helps your task - You want quick improvements</p> <p>Example:</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"),\n    dspy.Example(question=\"What is 3*3?\", answer=\"9\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use optimized version\nresult = optimized_qa(question=\"What is 5+5?\")\nprint(result.answer)  # More likely to be correct!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-mipro-multi-prompt-instruction-proposal-optimizer","title":"2. MIPRO (Multi-prompt Instruction Proposal Optimizer)","text":"<p>Advanced optimizer that tunes instructions and demonstrations.</p> <pre><code>from dspy.teleprompt import MIPRO\n\noptimizer = MIPRO(\n    metric=accuracy,\n    num_candidates=10,  # Number of prompt variations to try\n    init_temperature=1.0\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples,  # Validation set for selection\n    num_trials=20\n)\n</code></pre> <p>How it works: 1. Generates multiple prompt instruction variations 2. Tests each on training data 3. Selects best based on validation performance</p> <p>When to use: - You have both training and validation sets - You want the best possible performance - You can afford longer optimization time</p>"},{"location":"building-blocks/dspy/optimizers/#3-bootstrapfewshotwithrandomsearch","title":"3. BootstrapFewShotWithRandomSearch","text":"<p>Combines few-shot learning with random search over hyperparameters.</p> <pre><code>from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\noptimizer = BootstrapFewShotWithRandomSearch(\n    metric=accuracy,\n    max_bootstrapped_demos=4,\n    num_candidate_programs=10,  # Number of variations to try\n    num_threads=4               # Parallel evaluation\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#4-bayesiansignatureoptimizer","title":"4. BayesianSignatureOptimizer","text":"<p>Uses Bayesian optimization to find best prompts.</p> <pre><code>from dspy.teleprompt import BayesianSignatureOptimizer\n\noptimizer = BayesianSignatureOptimizer(\n    metric=accuracy,\n    n=20  # Number of optimization steps\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#advanced-teacher-student-optimization","title":"Advanced: Teacher-Student Optimization","text":"<p>Use a stronger model (teacher) to generate labels for a weaker model (student):</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Configure teacher (expensive, high-quality model)\nteacher_lm = dspy.LM('openai/gpt-4o', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Configure student (cheap, fast model)\nstudent_lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Create modules\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Teacher uses GPT-4o\nwith dspy.settings.context(lm=teacher_lm):\n    teacher = QA()\n\n# Student uses GPT-4o-mini\nwith dspy.settings.context(lm=student_lm):\n    student = QA()\n\n# Bootstrap student from teacher\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_student = optimizer.compile(\n    student=student,\n    teacher=teacher,  # Use teacher to generate examples\n    trainset=trainset\n)\n\n# optimized_student achieves near-teacher quality at student cost!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#optimization-in-mahsm","title":"Optimization in mahsm","text":"<p>Optimize mahsm modules just like regular DSPy modules:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define state\nclass QAState(TypedDict):\n    question: str\n    answer: str\n\n# Define module\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is DSPy?\", answer=\"A framework...\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=lambda e, p: e.answer in p.answer)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", optimized_qa)  # Use optimized version!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/optimizers/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Start with BootstrapFewShot <pre><code># \u2705 Simple and effective\noptimizer = BootstrapFewShot(metric=accuracy)\n</code></pre></p> </li> <li> <p>Use meaningful metrics <pre><code># \u2705 Task-specific\ndef f1_score(example, prediction):\n    return calculate_f1(example, prediction)\n</code></pre></p> </li> <li> <p>Use validation sets for selection <pre><code># \u2705 Prevents overfitting\noptimizer.compile(student, trainset=train, valset=val)\n</code></pre></p> </li> <li> <p>Start small, then scale <pre><code># \u2705 Iterate on 10 examples first\ntrainset_small = trainset[:10]\noptimized = optimizer.compile(module, trainset=trainset_small)\n# Then use full dataset\n</code></pre></p> </li> <li> <p>Save optimized modules <pre><code># \u2705 Don't re-optimize every time\noptimized.save(\"optimized_qa.json\")\nloaded = QA()\nloaded.load(\"optimized_qa.json\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Optimize without evaluation <pre><code># \u274c How do you know it's better?\noptimized = optimizer.compile(module, trainset=data)\n# \u2705 Always evaluate\nscore = evaluate(optimized, testset)\n</code></pre></p> </li> <li> <p>Use tiny training sets <pre><code># \u274c 2 examples won't help\ntrainset = [example1, example2]\n# \u2705 Use at least 20-50 examples\n</code></pre></p> </li> <li> <p>Over-optimize on training data <pre><code># \u274c Overfitting risk\nmax_bootstrapped_demos=100  # Too many!\n# \u2705 Use 3-5 demonstrations\nmax_bootstrapped_demos=4\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#evaluation-after-optimization","title":"Evaluation After Optimization","text":"<p>Always evaluate on a held-out test set:</p> <pre><code>from dspy.evaluate import Evaluate\n\n# Create evaluator\nevaluator = Evaluate(\n    devset=testset,\n    metric=accuracy,\n    num_threads=4,\n    display_progress=True\n)\n\n# Compare before and after\nbaseline_score = evaluator(original_module)\noptimized_score = evaluator(optimized_module)\n\nprint(f\"Baseline: {baseline_score}%\")\nprint(f\"Optimized: {optimized_score}%\")\nprint(f\"Improvement: {optimized_score - baseline_score}%\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#complete-optimization-pipeline","title":"Complete Optimization Pipeline","text":"<pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\nfrom dspy.evaluate import Evaluate\n\n# 1. Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 2. Create datasets\nfull_data = load_data()\ntrain, val, test = split_data(full_data, [0.7, 0.15, 0.15])\n\n# 3. Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# 4. Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match, max_bootstrapped_demos=4)\noptimized_qa = optimizer.compile(student=qa, trainset=train, valset=val)\n\n# 5. Evaluate\nevaluator = Evaluate(devset=test, metric=exact_match)\nbaseline_score = evaluator(qa)\noptimized_score = evaluator(optimized_qa)\n\nprint(f\"Baseline: {baseline_score:.1f}%\")\nprint(f\"Optimized: {optimized_score:.1f}%\")\n\n# 6. Save\nif optimized_score &gt; baseline_score:\n    optimized_qa.save(\"best_qa_model.json\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#comparison-table","title":"Comparison Table","text":"Optimizer Speed Quality Best For BootstrapFewShot \u26a1\u26a1\u26a1 Fast \u2b50\u2b50 Good Quick improvements MIPRO \u26a1 Slow \u2b50\u2b50\u2b50 Best Maximum quality BootstrapFewShotWithRandomSearch \u26a1\u26a1 Medium \u2b50\u2b50\u2b50 Better Balanced BayesianSignatureOptimizer \u26a1 Slow \u2b50\u2b50\u2b50 Better Complex tasks"},{"location":"building-blocks/dspy/optimizers/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices \u2192 Production DSPy tips</li> <li>Optimization Workflow Guide \u2192 Complete tutorial</li> <li>LangGraph Overview \u2192 Learn about workflows</li> </ul>"},{"location":"building-blocks/dspy/optimizers/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Optimizers Docs - Official guide</li> <li>DSPy Optimization Paper - Research paper</li> </ul> <p>Next: Explore Best Practices \u2192</p>"},{"location":"building-blocks/dspy/overview/","title":"DSPy Overview","text":"<p>TL;DR: DSPy turns prompt engineering into programming\u2014define what you want, not how to prompt for it.</p>"},{"location":"building-blocks/dspy/overview/#what-is-dspy","title":"What is DSPy?","text":"<p>DSPy (Declarative Self-improving Language Programs in Python) is a framework for building LLM applications through programming, not manual prompting.</p> <p>Instead of writing and tweaking prompts like this:</p> <pre><code># \u274c Traditional prompting\nprompt = \"\"\"\nYou are a helpful assistant. Given a question, provide a detailed answer.\n\nQuestion: {question}\nThink step by step and provide your reasoning.\n\nAnswer:\n\"\"\"\nresponse = llm.complete(prompt.format(question=\"What is DSPy?\"))\n</code></pre> <p>You write code like this:</p> <pre><code># \u2705 DSPy approach\nimport dspy\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.cot(question=question)\n\nqa = QA()\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)\n</code></pre> <p>Key Insight: You declare the structure (chain-of-thought reasoning), and DSPy generates the actual prompts automatically.</p>"},{"location":"building-blocks/dspy/overview/#why-dspy","title":"Why DSPy?","text":""},{"location":"building-blocks/dspy/overview/#1-composability","title":"1. Composability","text":"<p>Build complex pipelines from simple components:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, context -&gt; answer\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        context = search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        return self.synthesize(question=question, context=context)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-automatic-optimization","title":"2. Automatic Optimization","text":"<p>DSPy can automatically improve your prompts:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Define success metric\ndef validate_answer(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# Optimize the pipeline\noptimizer = BootstrapFewShot(metric=validate_answer)\noptimized_qa = optimizer.compile(QA(), trainset=examples)\n\n# optimized_qa now has better prompts learned from examples!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-model-agnostic","title":"3. Model Agnostic","text":"<p>Switch between models without changing code:</p> <pre><code># Use GPT-4o-mini\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Later, switch to Claude\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n# Your code stays the same!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/overview/#1-signatures","title":"1. Signatures","text":"<p>Signatures define input \u2192 output specifications:</p> <pre><code># Simple signature\n\"question -&gt; answer\"\n\n# Multi-input signature\n\"question, context -&gt; answer\"\n\n# With hints\n\"question -&gt; answer: a detailed, technical response\"\n</code></pre> <p>Learn more about Signatures \u2192</p>"},{"location":"building-blocks/dspy/overview/#2-modules","title":"2. Modules","text":"<p>Modules are reusable components that use signatures:</p> <pre><code># Built-in modules\ndspy.Predict(\"question -&gt; answer\")          # Basic prediction\ndspy.ChainOfThought(\"question -&gt; answer\")   # With reasoning\ndspy.ReAct(\"question -&gt; answer\")            # Tool-using agent\n\n# Custom modules\nclass MyAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        return self.predictor(input=input)\n</code></pre> <p>Learn more about Modules \u2192</p>"},{"location":"building-blocks/dspy/overview/#3-optimizers-teleprompts","title":"3. Optimizers (Teleprompts)","text":"<p>Optimizers automatically improve your modules:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot, MIPRO\n\n# Few-shot learning\noptimizer = BootstrapFewShot(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=data)\n\n# Advanced optimization\noptimizer = MIPRO(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=train, valset=val)\n</code></pre> <p>Learn more about Optimizers \u2192</p>"},{"location":"building-blocks/dspy/overview/#dspy-in-mahsm","title":"DSPy in mahsm","text":"<p>mahsm makes DSPy even easier by integrating it with LangGraph workflows:</p>"},{"location":"building-blocks/dspy/overview/#the-dspy_node-decorator","title":"The <code>@dspy_node</code> Decorator","text":"<p>Convert any DSPy module into a LangGraph node:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n\n# Use it in a workflow\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"researcher\", Researcher())  # \u2705 Works seamlessly\n</code></pre> <p>How it works: 1. <code>@dspy_node</code> wraps your DSPy module 2. Automatically extracts inputs from state 3. Merges outputs back into state 4. Handles Langfuse tracing</p> <p>Learn more about @dspy_node \u2192</p>"},{"location":"building-blocks/dspy/overview/#quick-example-building-a-qa-agent","title":"Quick Example: Building a Q&amp;A Agent","text":"<p>Let's build a complete Q&amp;A agent using DSPy + mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# 1. Configure DSPy\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 2. Define state\nclass QAState(TypedDict):\n    question: str\n    reasoning: str\n    answer: str\n\n# 3. Create DSPy module\n@ma.dspy_node\nclass QAAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 4. Build LangGraph workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QAAgent())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\"question\": \"What are the benefits of using DSPy?\"})\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Reasoning: {result['reasoning']}\")\n# \u2705 Automatically traced in Langfuse!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#when-to-use-dspy","title":"When to Use DSPy","text":""},{"location":"building-blocks/dspy/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Complex reasoning tasks requiring chain-of-thought</li> <li>Multi-step pipelines with intermediate outputs</li> <li>Optimizable systems where you can measure success</li> <li>Model-agnostic applications that need portability</li> </ul>"},{"location":"building-blocks/dspy/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot prompts (just use the LLM API directly)</li> <li>When you need exact prompt control (DSPy generates prompts)</li> <li>Streaming responses with partial updates (DSPy is batch-oriented)</li> </ul>"},{"location":"building-blocks/dspy/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/overview/#1-sequential-processing","title":"1. Sequential Processing","text":"<pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"input -&gt; intermediate\")\n        self.step2 = dspy.ChainOfThought(\"intermediate -&gt; output\")\n\n    def forward(self, input):\n        intermediate = self.step1(input=input)\n        return self.step2(intermediate=intermediate.intermediate)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-conditional-logic","title":"2. Conditional Logic","text":"<pre><code>class ConditionalAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.classifier = dspy.Predict(\"question -&gt; category\")\n        self.tech_expert = dspy.ChainOfThought(\"question -&gt; answer\")\n        self.general_expert = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        category = self.classifier(question=question).category\n\n        if \"technical\" in category.lower():\n            return self.tech_expert(question=question)\n        else:\n            return self.general_expert(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-tool-use-with-react","title":"3. Tool Use with ReAct","text":"<pre><code>class ToolUser(dspy.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\")\n        self.react.tools = tools\n\n    def forward(self, question):\n        return self.react(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Signatures \u2192 Learn how to define inputs and outputs</li> <li>DSPy Modules \u2192 Explore built-in modules like ChainOfThought, ReAct</li> <li>DSPy Optimizers \u2192 Automatically improve your prompts</li> <li>Best Practices \u2192 Tips for production DSPy code</li> </ul>"},{"location":"building-blocks/dspy/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official DSPy Docs - Comprehensive DSPy documentation</li> <li>DSPy GitHub - Source code and examples</li> <li>DSPy Paper - Research paper explaining DSPy</li> </ul> <p>Ready to dive deeper? Start with Signatures \u2192</p>"},{"location":"building-blocks/dspy/signatures/","title":"DSPy Signatures","text":"<p>TL;DR: Signatures are type specifications that tell DSPy what inputs your module needs and what outputs it should produce.</p>"},{"location":"building-blocks/dspy/signatures/#what-is-a-signature","title":"What is a Signature?","text":"<p>A signature in DSPy is like a function type hint\u2014it specifies: - What inputs the module receives - What outputs it should produce - Optional descriptions for each field</p> <p>Think of it as a contract between your code and the LLM.</p>"},{"location":"building-blocks/dspy/signatures/#basic-syntax","title":"Basic Syntax","text":""},{"location":"building-blocks/dspy/signatures/#string-signatures","title":"String Signatures","text":"<p>The simplest way to define a signature:</p> <pre><code>import dspy\n\n# Single input \u2192 single output\n\"question -&gt; answer\"\n\n# Multiple inputs \u2192 single output\n\"question, context -&gt; answer\"\n\n# Multiple inputs \u2192 multiple outputs\n\"question, context -&gt; answer, confidence\"\n</code></pre> <p>Format: <code>input1, input2 -&gt; output1, output2</code></p>"},{"location":"building-blocks/dspy/signatures/#example","title":"Example","text":"<pre><code># Create a predictor with a signature\nqa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n# Use it\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)  # Access output by name\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#adding-descriptions","title":"Adding Descriptions","text":"<p>You can add hints to guide the LLM:</p> <pre><code># Add output description\n\"question -&gt; answer: a concise, factual response\"\n\n# Add input descriptions\n\"question: a technical question -&gt; answer: a detailed explanation\"\n\n# Multiple fields with descriptions\n\"question: user query, context: relevant docs -&gt; answer: synthesized response, sources: list of citations\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#example-with-descriptions","title":"Example with Descriptions","text":"<pre><code>predictor = dspy.ChainOfThought(\n    \"question: a user's question about AI -&gt; answer: a detailed, technical explanation\"\n)\n\nresult = predictor(question=\"How does attention work in transformers?\")\nprint(result.answer)\n# Output will be more detailed and technical due to the hint\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#class-based-signatures","title":"Class-Based Signatures","text":"<p>For complex signatures, use classes:</p> <pre><code>import dspy\n\nclass QASignature(dspy.Signature):\n    \"\"\"Answer questions with detailed explanations.\"\"\"\n\n    question = dspy.InputField(desc=\"The user's question\")\n    context = dspy.InputField(desc=\"Relevant background information\")\n    answer = dspy.OutputField(desc=\"A comprehensive answer\")\n    confidence = dspy.OutputField(desc=\"Confidence score (0-100)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#using-class-signatures","title":"Using Class Signatures","text":"<pre><code># Pass the class (not an instance!)\npredictor = dspy.ChainOfThought(QASignature)\n\nresult = predictor(\n    question=\"What is DSPy?\",\n    context=\"DSPy is a framework for prompt programming...\"\n)\nprint(result.answer)\nprint(result.confidence)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#input-and-output-fields","title":"Input and Output Fields","text":""},{"location":"building-blocks/dspy/signatures/#inputfield","title":"InputField","text":"<p>Defines what the module receives:</p> <pre><code>import dspy\n\nclass MySignature(dspy.Signature):\n    # Basic input\n    query = dspy.InputField()\n\n    # With description\n    context = dspy.InputField(desc=\"Background information\")\n\n    # With format hint\n    examples = dspy.InputField(desc=\"Few-shot examples\", format=list)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#outputfield","title":"OutputField","text":"<p>Defines what the module produces:</p> <pre><code>class MySignature(dspy.Signature):\n    # Basic output\n    answer = dspy.OutputField()\n\n    # With description\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n\n    # With prefix (shown before the output in the prompt)\n    summary = dspy.OutputField(prefix=\"SUMMARY:\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/signatures/#1-simple-qa","title":"1. Simple Q&amp;A","text":"<pre><code>\"question -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#2-context-aware-qa","title":"2. Context-Aware Q&amp;A","text":"<pre><code>\"question, context -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#3-multi-output","title":"3. Multi-Output","text":"<pre><code>\"document -&gt; summary, key_points, sentiment\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#4-complex-reasoning","title":"4. Complex Reasoning","text":"<pre><code>class ReasoningSignature(dspy.Signature):\n    \"\"\"Solve complex problems with step-by-step reasoning.\"\"\"\n\n    problem = dspy.InputField(desc=\"The problem to solve\")\n    constraints = dspy.InputField(desc=\"Any constraints or requirements\")\n\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n    solution = dspy.OutputField(desc=\"The final solution\")\n    confidence = dspy.OutputField(desc=\"Confidence level (low/medium/high)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#5-classification","title":"5. Classification","text":"<pre><code>class ClassificationSignature(dspy.Signature):\n    \"\"\"Classify text into categories.\"\"\"\n\n    text = dspy.InputField(desc=\"The text to classify\")\n    categories = dspy.InputField(desc=\"Valid categories (comma-separated)\")\n\n    category = dspy.OutputField(desc=\"The chosen category\")\n    reason = dspy.OutputField(desc=\"Brief explanation for the choice\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#signatures-in-mahsm","title":"Signatures in mahsm","text":"<p>When using <code>@dspy_node</code>, signatures determine how inputs are extracted from state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\n# 1. Define state\nclass ResearchState(TypedDict):\n    question: str\n    context: str\n    answer: str\n    reasoning: str\n\n# 2. Create module with signature\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        # Signature matches state fields\n        self.research = dspy.ChainOfThought(\"question, context -&gt; answer, reasoning\")\n\n    def forward(self, question, context):\n        return self.research(question=question, context=context)\n\n# 3. Use in workflow\nworkflow = ma.graph.StateGraph(ResearchState)\nworkflow.add_node(\"researcher\", Researcher())\n\n# When the node runs:\n# - \"question\" and \"context\" are extracted from state\n# - \"answer\" and \"reasoning\" are written back to state\n</code></pre> <p>Key Point: Match your signature field names to your state keys for seamless integration!</p>"},{"location":"building-blocks/dspy/signatures/#advanced-dynamic-signatures","title":"Advanced: Dynamic Signatures","text":"<p>Create signatures programmatically:</p> <pre><code>def create_signature(input_fields, output_fields):\n    inputs = \", \".join(input_fields)\n    outputs = \", \".join(output_fields)\n    return f\"{inputs} -&gt; {outputs}\"\n\n# Example: Dynamic fields\nsig = create_signature([\"question\", \"context\"], [\"answer\", \"score\"])\n# Result: \"question, context -&gt; answer, score\"\n\npredictor = dspy.Predict(sig)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/signatures/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use descriptive field names <pre><code>\"user_question -&gt; detailed_answer\"  # \u2705 Clear\n</code></pre></p> </li> <li> <p>Add descriptions for ambiguous fields <pre><code>\"query: the user's search query -&gt; results: list of relevant items\"\n</code></pre></p> </li> <li> <p>Match state keys in mahsm <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use multi-output for intermediate reasoning <pre><code>\"question -&gt; reasoning, answer\"  # \u2705 Captures thought process\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Use vague names <pre><code>\"input -&gt; output\"  # \u274c Not descriptive\n</code></pre></p> </li> <li> <p>Mix concerns in one field <pre><code>\"query -&gt; answer_and_confidence\"  # \u274c Split into two outputs\n</code></pre></p> </li> <li> <p>Over-complicate <pre><code># \u274c Too many fields\n\"q, c1, c2, c3, c4 -&gt; a, r1, r2, r3, conf, meta\"\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#troubleshooting","title":"Troubleshooting","text":""},{"location":"building-blocks/dspy/signatures/#issue-llm-not-returning-expected-output","title":"Issue: LLM not returning expected output","text":"<p>Solution: Add more specific descriptions</p> <pre><code># Before (vague)\n\"text -&gt; category\"\n\n# After (specific)\n\"text: a customer review -&gt; category: one of [positive, negative, neutral]\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-output-format-is-inconsistent","title":"Issue: Output format is inconsistent","text":"<p>Solution: Use structured output hints</p> <pre><code>class StructuredSignature(dspy.Signature):\n    query = dspy.InputField()\n    answer = dspy.OutputField(desc=\"Answer in JSON format with keys: summary, details\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-state-keys-dont-match-signature","title":"Issue: State keys don't match signature","text":"<p>Solution: Ensure field names align</p> <pre><code># State has \"user_query\"\nclass State(TypedDict):\n    user_query: str\n\n# \u274c Signature uses \"question\"\ndspy.ChainOfThought(\"question -&gt; answer\")  # Won't find \"question\" in state!\n\n# \u2705 Match the key\ndspy.ChainOfThought(\"user_query -&gt; answer\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Modules \u2192 Learn about Predict, ChainOfThought, ReAct</li> <li>Your First Agent \u2192 Build a complete agent</li> <li>API Reference: @dspy_node \u2192 mahsm integration details</li> </ul>"},{"location":"building-blocks/dspy/signatures/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Signatures Documentation - Official guide</li> <li>DSPy Examples - Real-world signature usage</li> </ul> <p>Next: Explore DSPy Modules \u2192</p>"},{"location":"building-blocks/langgraph/overview/","title":"LangGraph Overview","text":"<p>TL;DR: LangGraph builds stateful, cyclical workflows for LLM agents\u2014think state machines for AI.</p>"},{"location":"building-blocks/langgraph/overview/#what-is-langgraph","title":"What is LangGraph?","text":"<p>LangGraph is a framework for building stateful, multi-step workflows with LLMs. Unlike simple chains (input \u2192 LLM \u2192 output), LangGraph enables:</p> <ul> <li>Cycles: Agents can loop, retry, and refine</li> <li>State: Persistent memory across steps</li> <li>Branching: Conditional routing based on outputs</li> <li>Parallelism: Run multiple nodes concurrently</li> </ul> <p>Think of it as a state machine where each node is an AI agent or tool.</p>"},{"location":"building-blocks/langgraph/overview/#why-langgraph","title":"Why LangGraph?","text":""},{"location":"building-blocks/langgraph/overview/#the-problem-with-chains","title":"The Problem with Chains","text":"<p>Traditional LLM chains are linear:</p> <pre><code># \u274c Linear chain - can't loop or branch\nquery \u2192 retrieve_docs \u2192 generate_answer \u2192 done\n</code></pre> <p>Real agents need to: - Loop until a condition is met - Branch based on intermediate results - Maintain state across steps</p>"},{"location":"building-blocks/langgraph/overview/#the-langgraph-solution","title":"The LangGraph Solution","text":"<pre><code># \u2705 Cyclic workflow with branching\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  generate   \u2502\n       \u2502   query     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   search    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502 synthesize  \u2502      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502   check     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502  quality    \u2502 if poor, retry\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502 if good\n              \u25bc\n            END\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/langgraph/overview/#1-state","title":"1. State","text":"<p>State is a <code>TypedDict</code> that flows through your workflow:</p> <pre><code>from typing import TypedDict, Optional\n\nclass ResearchState(TypedDict):\n    question: str\n    search_query: Optional[str]\n    findings: Optional[str]\n    answer: Optional[str]\n</code></pre> <p>Learn more about State \u2192</p>"},{"location":"building-blocks/langgraph/overview/#2-nodes","title":"2. Nodes","text":"<p>Nodes are functions or agents that process state:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n</code></pre> <p>Learn more about Nodes &amp; Edges \u2192</p>"},{"location":"building-blocks/langgraph/overview/#3-edges","title":"3. Edges","text":"<p>Edges connect nodes:</p> <pre><code># Simple edge\nworkflow.add_edge(\"node_a\", \"node_b\")\n\n# Conditional edge\nworkflow.add_conditional_edges(\n    \"checker\",\n    lambda state: \"retry\" if state[\"quality\"] &lt; 0.7 else END\n)\n</code></pre> <p>Learn more about Conditional Routing \u2192</p>"},{"location":"building-blocks/langgraph/overview/#4-graph-compilation","title":"4. Graph Compilation","text":"<p>Compile the workflow into an executable graph:</p> <pre><code>workflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"agent\", my_agent)\nworkflow.add_edge(ma.START, \"agent\")\nworkflow.add_edge(\"agent\", ma.END)\n\ngraph = workflow.compile()  # \u2705 Ready to run\n</code></pre> <p>Learn more about Compilation \u2192</p>"},{"location":"building-blocks/langgraph/overview/#quick-example","title":"Quick Example","text":"<p>Let's build a self-correcting Q&amp;A agent:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 1. Define state\nclass QAState(TypedDict):\n    question: str\n    answer: Optional[str]\n    quality_score: Optional[float]\n    iteration: int\n\n# 2. Define nodes\n@ma.dspy_node\nclass Answerer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityChecker(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.checker = dspy.Predict(\"question, answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, question, answer):\n        return self.checker(question=question, answer=answer)\n\ndef increment_iteration(state: QAState) -&gt; QAState:\n    \"\"\"Increment iteration counter.\"\"\"\n    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n\n# 3. Define routing\ndef should_retry(state: QAState):\n    \"\"\"Retry if quality is low and we haven't exceeded max iterations.\"\"\"\n    if state.get(\"iteration\", 0) &gt;= 3:\n        return ma.END  # Give up after 3 tries\n\n    quality = float(state.get(\"quality_score\", 0))\n    if quality &lt; 0.7:\n        return \"answer\"  # Retry\n    return ma.END  # Good enough!\n\n# 4. Build graph\nworkflow = ma.graph.StateGraph(QAState)\n\nworkflow.add_node(\"answer\", Answerer())\nworkflow.add_node(\"check\", QualityChecker())\nworkflow.add_node(\"increment\", increment_iteration)\n\nworkflow.add_edge(ma.START, \"increment\")\nworkflow.add_edge(\"increment\", \"answer\")\nworkflow.add_edge(\"answer\", \"check\")\nworkflow.add_conditional_edges(\"check\", should_retry)\n\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\n    \"question\": \"Explain quantum entanglement simply.\",\n    \"iteration\": 0\n})\n\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Quality: {result['quality_score']}\")\nprint(f\"Iterations: {result['iteration']}\")\n# \u2705 Agent retries until quality threshold is met!\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#langgraph-in-mahsm","title":"LangGraph in mahsm","text":"<p>mahsm enhances LangGraph with:</p>"},{"location":"building-blocks/langgraph/overview/#1-simplified-node-creation","title":"1. Simplified Node Creation","text":"<pre><code># Without mahsm\ndef my_node(state):\n    # Manual state extraction\n    question = state[\"question\"]\n    # Call LLM\n    response = llm.complete(question)\n    # Manual state update\n    return {\"answer\": response}\n\n# With mahsm\n@ma.dspy_node\nclass MyNode(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n# \u2705 State extraction/merging handled automatically\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-automatic-tracing","title":"2. Automatic Tracing","text":"<pre><code>ma.tracing.init()  # One line\n# \u2705 All LangGraph nodes traced to Langfuse\n# \u2705 All DSPy calls traced\n# \u2705 Custom functions with @observe traced\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-type-safe-state","title":"3. Type-Safe State","text":"<pre><code>class MyState(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 IDE autocomplete\n# \u2705 Static type checking\n# \u2705 Runtime validation\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#when-to-use-langgraph","title":"When to Use LangGraph","text":""},{"location":"building-blocks/langgraph/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Multi-step agents that need memory</li> <li>Cyclical workflows (retry, refine, iterate)</li> <li>Conditional branching based on outputs</li> <li>Complex orchestration of multiple agents</li> <li>Human-in-the-loop systems</li> </ul>"},{"location":"building-blocks/langgraph/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot completions (use DSPy directly)</li> <li>Purely stateless operations (no need for state management)</li> <li>Real-time streaming (LangGraph is batch-oriented)</li> </ul>"},{"location":"building-blocks/langgraph/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/langgraph/overview/#1-linear-pipeline","title":"1. Linear Pipeline","text":"<pre><code>START \u2192 agent1 \u2192 agent2 \u2192 agent3 \u2192 END\n</code></pre> <pre><code>workflow.add_edge(ma.START, \"agent1\")\nworkflow.add_edge(\"agent1\", \"agent2\")\nworkflow.add_edge(\"agent2\", \"agent3\")\nworkflow.add_edge(\"agent3\", ma.END)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-conditional-branching","title":"2. Conditional Branching","text":"<pre><code>                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      START \u2500\u2500\u2500\u2500\u2500\u25ba\u2502 router  \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                 \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 path_a \u2502       \u2502 path_b \u2502\n         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n                    END\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"router\",\n    lambda state: \"path_a\" if condition(state) else \"path_b\"\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-retry-loop","title":"3. Retry Loop","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502              \u2502\n        \u25bc              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  try  \u2502\u2500\u2500\u2500\u25ba\u2502   check   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u25bc\n                    END (if success)\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"check\",\n    lambda state: \"try\" if not success(state) else ma.END\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#4-multi-agent-collaboration","title":"4. Multi-Agent Collaboration","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u250c\u2500\u2500\u25ba\u2502 researcher\u2502\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n    \u2502                     \u25bc\n    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2502coordinator\u2502\u25c4\u2500\u2500\u2502synthesizer\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/overview/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict for state <pre><code>class State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Keep nodes focused <pre><code># \u2705 Single responsibility\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    # Only generates queries\n    pass\n</code></pre></p> </li> <li> <p>Handle missing state gracefully <pre><code>def my_router(state):\n    value = state.get(\"key\", default_value)\n    # ...\n</code></pre></p> </li> <li> <p>Use conditional edges for routing <pre><code>workflow.add_conditional_edges(\"checker\", route_function)\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Don't do this\ndef node(state):\n    state[\"key\"] = \"value\"  # Mutates input!\n    return state\n\n# \u2705 Do this\ndef node(state):\n    return {\"key\": \"value\"}  # Returns update\n</code></pre></p> </li> <li> <p>Create infinite loops without exit conditions <pre><code># \u274c No way to exit\nworkflow.add_conditional_edges(\"node\", lambda s: \"node\")\n\n# \u2705 Add exit condition\ndef router(state):\n    if state[\"count\"] &gt; 10:\n        return ma.END\n    return \"node\"\n</code></pre></p> </li> <li> <p>Over-complicate the graph <pre><code># \u274c Too many branches\n# Keep it simple and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#next-steps","title":"Next Steps","text":"<ul> <li>State Management \u2192 Learn about TypedDict and state updates</li> <li>Nodes &amp; Edges \u2192 Build your graph components</li> <li>Conditional Routing \u2192 Add branching logic</li> <li>Compilation &amp; Execution \u2192 Run your workflows</li> <li>Visualization \u2192 Visualize your graphs</li> </ul>"},{"location":"building-blocks/langgraph/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official LangGraph Docs - Comprehensive guide</li> <li>LangGraph GitHub - Source code and examples</li> <li>LangGraph Tutorials - Step-by-step guides</li> </ul> <p>Ready to dive deeper? Start with State Management \u2192</p>"},{"location":"building-blocks/langgraph/state/","title":"LangGraph State Management","text":"<p>TL;DR: State in LangGraph is a TypedDict that flows through your workflow, carrying data between nodes.</p>"},{"location":"building-blocks/langgraph/state/#what-is-state","title":"What is State?","text":"<p>In LangGraph, state is a dictionary that: - Flows through your workflow - Is read by nodes - Is updated by nodes - Maintains type safety with <code>TypedDict</code></p> <p>Think of it as shared memory for your agent workflow.</p>"},{"location":"building-blocks/langgraph/state/#defining-state","title":"Defining State","text":"<p>Use Python's <code>TypedDict</code> to define your state schema:</p> <pre><code>from typing import TypedDict, Optional\n\nclass MyState(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]\n</code></pre> <p>Benefits: - \u2705 IDE autocomplete - \u2705 Type checking - \u2705 Clear documentation - \u2705 Runtime validation</p>"},{"location":"building-blocks/langgraph/state/#state-flow","title":"State Flow","text":"<p>State flows through nodes in your workflow:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    input: str\n    intermediate: str\n    output: str\n\n# Node 1: Reads 'input', writes 'intermediate'\ndef node1(state: State) -&gt; dict:\n    result = process(state[\"input\"])\n    return {\"intermediate\": result}\n\n# Node 2: Reads 'intermediate', writes 'output'\ndef node2(state: State) -&gt; dict:\n    result = finalize(state[\"intermediate\"])\n    return {\"output\": result}\n\n# Build workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"node1\", node1)\nworkflow.add_node(\"node2\", node2)\nworkflow.add_edge(ma.START, \"node1\")\nworkflow.add_edge(\"node1\", \"node2\")\nworkflow.add_edge(\"node2\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"input\": \"Hello\"})\n# State flows: {\"input\": \"Hello\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\", \"output\": \"...\"}\nprint(result[\"output\"])\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-updates","title":"State Updates","text":""},{"location":"building-blocks/langgraph/state/#immutable-updates","title":"Immutable Updates","text":"<p>Nodes return updates, not full state:</p> <pre><code># \u274c DON'T: Mutate state directly\ndef bad_node(state):\n    state[\"answer\"] = \"Paris\"  # Mutates input!\n    return state\n\n# \u2705 DO: Return updates\ndef good_node(state):\n    return {\"answer\": \"Paris\"}  # Returns update\n</code></pre> <p>LangGraph merges your update into the state automatically:</p> <pre><code># Before node\nstate = {\"question\": \"What is the capital of France?\"}\n\n# Node returns\nupdate = {\"answer\": \"Paris\"}\n\n# After node (automatic merge)\nstate = {\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\"\n}\n</code></pre>"},{"location":"building-blocks/langgraph/state/#optional-vs-required-fields","title":"Optional vs Required Fields","text":"<p>Use <code>Optional</code> for fields that may not exist initially:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    # Required fields (must be in initial input)\n    question: str\n\n    # Optional fields (nodes will populate)\n    answer: Optional[str]\n    reasoning: Optional[str]\n    confidence: Optional[float]\n</code></pre>"},{"location":"building-blocks/langgraph/state/#complex-state-types","title":"Complex State Types","text":""},{"location":"building-blocks/langgraph/state/#lists","title":"Lists","text":"<pre><code>from typing import List\n\nclass State(TypedDict):\n    messages: List[str]\n    findings: List[dict]\n</code></pre> <p>Appending to lists:</p> <pre><code>def add_message(state: State) -&gt; dict:\n    # Option 1: Replace entire list\n    new_messages = state[\"messages\"] + [\"New message\"]\n    return {\"messages\": new_messages}\n\n    # Option 2: Use Annotated for automatic appending (advanced)\n    # See LangGraph docs for details\n</code></pre>"},{"location":"building-blocks/langgraph/state/#nested-dicts","title":"Nested Dicts","text":"<pre><code>class State(TypedDict):\n    user: dict  # {\"name\": str, \"email\": str}\n    config: dict\n</code></pre>"},{"location":"building-blocks/langgraph/state/#custom-classes","title":"Custom Classes","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass User:\n    name: str\n    email: str\n\nclass State(TypedDict):\n    user: User\n    active: bool\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-in-mahsm","title":"State in mahsm","text":""},{"location":"building-blocks/langgraph/state/#with-dspy_node","title":"With @dspy_node","text":"<p><code>@dspy_node</code> automatically extracts and updates state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\n\nclass QAState(TypedDict):\n    question: str\n    reasoning: Optional[str]\n    answer: Optional[str]\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# In workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QA())\n\n# When QA runs:\n# 1. Extracts 'question' from state\n# 2. Calls forward(question=state[\"question\"])\n# 3. Merges {\"reasoning\": \"...\", \"answer\": \"...\"} into state\n</code></pre>"},{"location":"building-blocks/langgraph/state/#with-regular-functions","title":"With Regular Functions","text":"<pre><code>def my_node(state: QAState) -&gt; dict:\n    \"\"\"Regular function node.\"\"\"\n    question = state[\"question\"]\n    # Process...\n    return {\"answer\": \"Paris\"}\n\nworkflow.add_node(\"my_node\", my_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-initialization","title":"State Initialization","text":""},{"location":"building-blocks/langgraph/state/#basic-initialization","title":"Basic Initialization","text":"<pre><code># Invoke with initial state\nresult = graph.invoke({\n    \"question\": \"What is DSPy?\",\n    \"confidence\": 0.0\n})\n</code></pre>"},{"location":"building-blocks/langgraph/state/#from-user-input","title":"From User Input","text":"<pre><code>def create_initial_state(user_input: str) -&gt; dict:\n    \"\"\"Create initial state from user input.\"\"\"\n    return {\n        \"question\": user_input,\n        \"iteration\": 0,\n        \"history\": []\n    }\n\nstate = create_initial_state(\"What is LangGraph?\")\nresult = graph.invoke(state)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-persistence","title":"State Persistence","text":"<p>State is immutable within a single execution:</p> <pre><code># Single execution\nresult = graph.invoke({\"question\": \"Hello\"})\n# State flows through workflow and is returned\n\n# New execution (fresh state)\nresult2 = graph.invoke({\"question\": \"Goodbye\"})\n# Independent from result\n</code></pre> <p>For persistent state across executions, use LangGraph's checkpointing (advanced):</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\n# Compile with memory\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\n# Run with thread ID\nconfig = {\"configurable\": {\"thread_id\": \"user_123\"}}\nresult1 = graph.invoke({\"question\": \"Hello\"}, config=config)\nresult2 = graph.invoke({\"question\": \"Continue...\"}, config=config)\n# result2 has access to result1's state!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/state/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict <pre><code># \u2705 Type-safe\nclass State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Make node outputs Optional <pre><code># \u2705 Clear that these are populated later\nanswer: Optional[str]\n</code></pre></p> </li> <li> <p>Return only updates <pre><code># \u2705 Clean updates\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> <li> <p>Use descriptive field names <pre><code># \u2705 Clear purpose\nuser_question: str\ngenerated_answer: str\nquality_score: float\n</code></pre></p> </li> <li> <p>Match DSPy signatures to state keys <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 Signature matches\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Never do this\ndef node(state):\n    state[\"answer\"] = \"Paris\"\n    return state\n</code></pre></p> </li> <li> <p>Use vague names <pre><code># \u274c What is this?\nresult: str\ndata: dict\n</code></pre></p> </li> <li> <p>Make everything required <pre><code># \u274c Will error if not in initial input\nclass State(TypedDict):\n    question: str\n    answer: str  # Should be Optional[str]\n</code></pre></p> </li> <li> <p>Return full state unnecessarily <pre><code># \u274c Redundant\ndef node(state):\n    return {**state, \"answer\": \"Paris\"}\n\n# \u2705 Just return update\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#debugging-state","title":"Debugging State","text":""},{"location":"building-blocks/langgraph/state/#print-state-in-nodes","title":"Print State in Nodes","text":"<pre><code>def debug_node(state):\n    print(f\"Current state: {state}\")\n    return {}\n\nworkflow.add_node(\"debug\", debug_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#trace-state-flow","title":"Trace State Flow","text":"<pre><code># Run with verbose output\nresult = graph.invoke({\"question\": \"Hello\"})\nprint(f\"Final state: {result}\")\n</code></pre>"},{"location":"building-blocks/langgraph/state/#use-langfuse","title":"Use Langfuse","text":"<p>With <code>ma.tracing.init()</code>, state is automatically logged:</p> <pre><code>ma.tracing.init()\nresult = graph.invoke({\"question\": \"Hello\"})\n# Check Langfuse UI to see state at each node!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#advanced-state-reducers","title":"Advanced: State Reducers","text":"<p>For complex state updates (like appending to lists), use reducers:</p> <pre><code>from typing import Annotated\nfrom langgraph.graph import add\n\nclass State(TypedDict):\n    # Normal field\n    question: str\n\n    # Auto-appending list (uses 'add' reducer)\n    messages: Annotated[List[str], add]\n\n# Now nodes can just return new messages\ndef add_message(state):\n    return {\"messages\": [\"New message\"]}\n# LangGraph automatically appends to existing messages!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-schema-evolution","title":"State Schema Evolution","text":"<p>As your workflow grows, extend your state:</p> <pre><code># v1\nclass StateV1(TypedDict):\n    question: str\n    answer: Optional[str]\n\n# v2 (add new fields)\nclass StateV2(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]  # New field\n    sources: Optional[List[str]]  # New field\n</code></pre> <p>Existing nodes continue working (they ignore new fields).</p>"},{"location":"building-blocks/langgraph/state/#example-multi-step-research-state","title":"Example: Multi-Step Research State","text":"<pre><code>from typing import TypedDict, Optional, List\n\nclass ResearchState(TypedDict):\n    # Input\n    question: str\n\n    # Intermediate\n    search_queries: Optional[List[str]]\n    raw_findings: Optional[List[dict]]\n\n    # Output\n    synthesized_answer: Optional[str]\n    sources: Optional[List[str]]\n    confidence: Optional[float]\n\n    # Metadata\n    iteration: int\n    total_tokens: int\n\n# Use in workflow\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.Predict(\"question -&gt; search_queries: list of queries\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n# Workflow automatically manages all state fields!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#next-steps","title":"Next Steps","text":"<ul> <li>Nodes &amp; Edges \u2192 Learn how nodes interact with state</li> <li>Conditional Routing \u2192 Route based on state</li> <li>Your First Agent \u2192 Build a stateful agent</li> </ul>"},{"location":"building-blocks/langgraph/state/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph State Docs - Official guide</li> <li>TypedDict Documentation - Python docs</li> </ul> <p>Next: Learn about Nodes &amp; Edges \u2192</p>"},{"location":"concepts/declarative-design/","title":"Declarative by Design","text":"<p>The central philosophy of <code>mahsm</code> is its declarative approach. Instead of manually writing imperative \"glue code\" to connect different libraries, you declare the components of your system, and <code>mahsm</code> handles the integration and boilerplate.</p> <p>This \"convention over configuration\" approach is designed to let you focus entirely on your agent's business logic, not the plumbing.</p>"},{"location":"concepts/declarative-design/#the-mahsm-approach","title":"The <code>mahsm</code> Approach","text":"<ul> <li>You declare your agent's reasoning by writing standard <code>dspy.Module</code> classes. The powerful <code>@ma.dspy_node</code> decorator instantly makes them compatible with the orchestration layer.</li> <li>You declare your workflow's structure by adding your nodes to a <code>ma.graph.StateGraph</code> and defining the edges between them.</li> <li>You declare your evaluation criteria by configuring the <code>ma.testing.PytestHarness</code> to run your graph against a dataset.</li> </ul>"},{"location":"concepts/declarative-design/#the-benefits","title":"The Benefits","text":"<p>This philosophy drastically reduces boilerplate, improves code readability, and embeds best practices for observability and testing directly into the development process. The result is a workflow that is faster, more robust, and produces systems that are understandable by default.</p>"},{"location":"concepts/four-pillars/","title":"The Four Pillars of <code>mahsm</code>","text":"<p><code>mahsm</code> achieves its power by deeply integrating four essential, best-in-class libraries into a single, seamless experience. All functionality is exposed through the unified <code>import mahsm as ma</code> API, giving you a consistent and clean developer experience.</p>"},{"location":"concepts/four-pillars/#1-dspy-the-reasoning-engine-madspy","title":"1. DSPy: The Reasoning Engine (<code>ma.dspy</code>)","text":"<ul> <li>What it is: A framework from Stanford NLP for programming\u2014not just prompting\u2014language models. It separates program flow from parameters (prompts and model weights) and uses optimizers to tune them for maximum performance.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> treats DSPy modules as the fundamental building blocks of agent intelligence. The core innovation is the <code>@ma.dspy_node</code> decorator. This tool instantly transforms any <code>dspy.Module</code> into a fully compliant LangGraph node, automatically handling the complex mapping of data from the shared graph <code>State</code> to the module's inputs and back.</li> </ul>"},{"location":"concepts/four-pillars/#2-langgraph-the-orchestration-scaffolding-magraph","title":"2. LangGraph: The Orchestration Scaffolding (<code>ma.graph</code>)","text":"<ul> <li>What it is: A library for building stateful, multi-agent applications by representing them as cyclical graphs. It provides the primitives of <code>State</code>, <code>Nodes</code>, and <code>Edges</code> to create complex, long-running agentic workflows.</li> <li>How <code>mahsm</code> Fuses it: LangGraph provides the skeleton, and <code>mahsm</code> provides the intelligent organs. By making DSPy modules the primary type of \"thinking\" node, <code>mahsm</code> supercharges LangGraph development. You define your application's <code>State</code> and use <code>ma.graph.StateGraph</code> to wire together your <code>@ma.dspy_node</code> agents.</li> </ul>"},{"location":"concepts/four-pillars/#3-langfuse-the-unified-observability-layer","title":"3. LangFuse: The Unified Observability Layer","text":"<ul> <li>What it is: A comprehensive open-source platform for LLM observability, providing detailed tracing, debugging, and analytics for AI applications.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> makes deep, hierarchical tracing an automatic, zero-effort feature. The single <code>ma.init()</code> function simultaneously instruments both LangGraph and DSPy. When you run your graph, <code>mahsm</code> creates a single, unified trace in LangFuse that captures both the high-level graph flow and the low-level DSPy execution details (prompts, tool calls, etc.), solving the massive pain point of achieving end-to-end observability.</li> </ul>"},{"location":"concepts/four-pillars/#4-evalprotocol-the-quality-control-testing-framework-matesting","title":"4. EvalProtocol: The Quality Control &amp; Testing Framework (<code>ma.testing</code>)","text":"<ul> <li>What it is: A standardized, <code>pytest</code>-based framework for evaluating the performance of AI systems using LLM-as-a-judge and other metrics.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> bridges the gap between your built application and your test suite. The <code>ma.testing.PytestHarness</code> class radically simplifies setup by automatically generating the boilerplate processors required by <code>eval-protocol</code>. The harness can even pull evaluation datasets directly from your production LangFuse traces, enabling a tight, continuous loop of deploying, observing, and evaluating your system's real-world performance.</li> </ul>"},{"location":"concepts/workflow/","title":"The <code>mahsm</code> Development Workflow","text":"<p>Developing with <code>mahsm</code> follows a simple, iterative, and powerful three-step loop: Build, Trace, and Evaluate. This cycle is designed to be fast and data-driven, ensuring you are creating high-quality, robust systems.</p> <p> </p>"},{"location":"concepts/workflow/#1-build","title":"1. Build","text":"<p>This is the core development step where you write idiomatic <code>mahsm</code> code. - Define State: Create a <code>TypedDict</code> that represents the shared state of your application. - Create Nodes: Write <code>dspy.Module</code> classes to encapsulate the reasoning logic for your agents. Decorate them with <code>@ma.dspy_node</code>. - Wire Graph: Add your nodes to a <code>ma.graph.StateGraph</code> and define the edges to control the flow of execution. - Compile: Call <code>.compile()</code> on your graph to create the runnable application.</p>"},{"location":"concepts/workflow/#2-trace","title":"2. Trace","text":"<p>Once built, you run your application.     *   With <code>ma.init()</code> called at the start of your script, every execution is automatically and deeply traced in LangFuse.     *   You use the LangFuse UI to inspect the full decision-making process of your agent, debug issues, understand latency, and analyze token usage.     *   You can tag interesting traces to save them as examples for regression testing or for creating evaluation datasets.</p>"},{"location":"concepts/workflow/#3-evaluate","title":"3. Evaluate","text":"<p>Finally, you verify the quality of your agent's output.     *   Write a Test File: Create a standard <code>pytest</code> file.     *   Configure the Harness: Use the <code>ma.testing.PytestHarness</code> to connect your compiled <code>mahsm</code> graph to the evaluation protocol.     *   Run Eval: Use datasets (potentially generated from your production traces in LangFuse) to run an evaluation.     *   Analyze &amp; Iterate: Use the evaluation leaderboards and results to identify weaknesses in your agent's logic, then go back to the BUILD step to improve it.</p>"},{"location":"design/ma.tuning-spec/","title":"Ma.tuning spec","text":"<p># mahsm.tuning \u2014 A single-module abstraction for SFT/DPO/RL/LoRA</p> <p>Status: Draft (for initial PR)</p> <p>Owners: @SohumKothavade</p> <p>Last updated: 2025-11-01</p> <p>## Executive summary</p> <p>Goal: Add one concise, universal module \u2014 <code>mahsm.tuning</code> \u2014 that lets any mahsm agent participate in post\u2011training (SFT, DPO, RL) with adapter publication (LoRA or full weights), while keeping the runtime fast and simple. We adopt training\u2013agent disaggregation: the agent runtime emits learning events; a remote trainer consumes events and returns artefacts via an OpenAI\u2011compatible endpoint. This follows the pattern demonstrated by Microsoft Agent Lightning (trainer/agent disaggregation with OpenAI\u2011style serving) and Arbor (DSPy\u2011centric RL server) [1][2][3].</p> <p>Design anchors:  - Stable abstraction is the event/trajectory schema, not any single algorithm [1][3][4].  - One \u201cstage\u201d contract for all methods: collect \u2192 curate \u2192 optimize \u2192 apply.  - Trainer adapters (TRL, Agent Lightning/veRL, Arbor) implement a tiny <code>fit(dataset, config) -&gt; artefact</code> API.  - Publishing is pluggable (LoRA adapter, new model version, policy swap).</p> <p>Non\u2011goals (initial PR): implement full PPO/GRPO in\u2011process, build a bespoke trace store, or replace LangFuse.</p> <p>## Why this now</p> <ul> <li>State of the art converges on: (a) decoupled runtime vs trainer, (b) unified MDP\u2011like trace schema, (c) OpenAI\u2011style serving of the optimized policy [1][3][4].</li> <li>Existing trainers (TRL for SFT/DPO; veRL/LightningRL for RL; Arbor for DSPy programs) can be consumed through thin adapters rather than re\u2011implemented [2][4][5][6].</li> </ul> <p>## Architecture (three planes)</p> <pre><code>graph LR\n  subgraph Runtime plane (mahsm graphs)\n    A[Agent graph (LangGraph + DSPy)] -- emits --&gt; E((Learning events))\n    A -- queries --&gt; Svc(OpenAI\u2011ish Inference Endpoint)\n  end\n\n  subgraph Data/trace plane\n    E\n    Store[(Trace store e.g., LangFuse)]\n    E -- normalized schema --&gt; Store\n  end\n\n  subgraph Learning plane (remote trainer)\n    T[Trainer adapters: TRL / Agent Lightning(veRL) / Arbor]\n    T -- read --&gt; Store\n    T -- artefacts --&gt; Pub[(Artefacts: LoRA, weights)]\n    Pub -- served as --&gt; Svc\n  end\n</code></pre> <p>Consequence: the agent stays responsive; training scales independently; algorithms are hot\u2011swappable.</p> <p>## Minimal surface area (single module)</p> <p>Add <code>mahsm/tuning.py</code> with four concepts and a small public API. No new top\u2011level packages.</p> <p>### 1) Event schema (stable core)</p> <p>A minimal MDP-ish schema sufficient for SFT/DPO/RL collection.</p> <pre><code># sketch only \u2014 implemented as dataclasses or TypedDicts\nclass Episode: id: str; task: str|None; metadata: dict\nclass Step: idx: int; input: dict; output: dict; tool_calls: list[dict]; lat_ms: int|None\nclass Reward: value: float; source: str  # e.g., \"user\", \"auto-metric\", \"rm\"\nclass Event: episode_id: str; step: Step|None; reward: Reward|None; tags: list[str]; ts: float\n</code></pre> <p>Emission policy:  - Node start, node end, tool call(s), final answer, score/reward events.  - API: <code>ma.tuning.emit(event)</code> and convenience wrappers for common node hooks.</p> <p>Mapping to LangFuse:  - Provide a serializer: <code>Event -&gt; LangFuse trace/observation</code> and back, keeping all downstream datasets uniform [7][8].</p> <p>### 2) Dataset transforms</p> <p>Three canonical transforms from events or traces:  - <code>to_sft(events|trace_query, *, template=...) -&gt; Iterable[SFTExample]</code>  - <code>to_preferences(events|trace_query, *, pair_by=..., filters=...) -&gt; Iterable[DPOExample]</code>  - <code>to_trajectories(events|stream, *, window=episode|n_steps, filters=...) -&gt; Iterable[Trajectory]</code></p> <p>Each supports simple, composable filters: top\u2011k by reward, dedupe identical prompts, drop unsafe, tool\u2011success\u2011only, etc.</p> <p>### 3) Trainer adapters</p> <p>Unify all algorithms behind a single protocol.</p> <pre><code>class Artefact:\n    kind: Literal[\"lora\", \"full_weights\", \"prompt\", \"policy\"]\n    uri: str  # where to fetch it\n\nclass TrainerAdapter(Protocol):\n    def fit(self, dataset: Iterable, config: dict) -&gt; Artefact: ...\n\n# Built\u2011ins (thin wrappers)\nTRL_SFT, TRL_DPO, VERL_RL, Arbor_RL\n</code></pre> <ul> <li>TRL: calls HF TRL\u2019s SFT/DPO trainers [5][6].</li> <li>VERL/Lightning: consumes trajectories, supports disaggregated rollout; expects OpenAI\u2011ish serving in front [4].</li> <li>Arbor: remote DSPy optimizer server; feeds DSPy programs/episodes [2].</li> </ul> <p>### 4) Publishing (apply)</p> <p>Minimal application targets:  - <code>apply_lora(artefact)</code> \u2014 attach LoRA to the active LM in DSPy (<code>dspy.settings.configure(lm=...)</code>).  - <code>apply_model(artefact)</code> \u2014 switch to a new base model/endpoint.  - <code>apply_policy(artefact)</code> \u2014 hot\u2011swap a graph policy (advanced/future).</p> <p>The apply functions use Mahsm\u2019s existing DSPy/graph wrappers; no trainer coupling.</p> <p>## One universal \u201cstage\u201d abstraction</p> <p>A stage composes: source \u2192 curate \u2192 optimize \u2192 apply.</p> <pre><code>@dataclass\nclass Source:\n    uri: str  # e.g., live://graph/&lt;id&gt;, trace://langfuse/&lt;project&gt;/&lt;task&gt;, dataset://hf/&lt;name&gt;, replay://buffer\n\n@dataclass\nclass Stage:\n    name: str\n    source: Source                      # where experience comes from\n    curate: Callable[..., Iterable]     # to_sft / to_preferences / to_trajectories\n    optimize: TrainerAdapter            # SFT / DPO / RL (via adapter)\n    apply: Callable[[Artefact], None]   # adapter://lora, model://version, policy://swap\n\n@dataclass\nclass Plan:\n    stages: list[Stage]\n    guard: dict | None = None  # simple acceptance criteria, e.g., min metrics\n\ndef run(plan: Plan):\n    for s in plan.stages:\n        ds = s.curate(resolve(s.source))\n        artefact = s.optimize.fit(ds, config={})\n        s.apply(artefact)\n</code></pre> <p>This keeps all methods uniform. SFT is just <code>trace -&gt; to_sft -&gt; TRL_SFT -&gt; apply_lora</code>. Online RL is <code>live -&gt; to_trajectories -&gt; VERL_RL -&gt; apply_lora</code>.</p> <p>## Public API sketch (what users write)</p> <pre><code>import mahsm as ma\nfrom mahsm import tuning as mt\n\n# 1) Wrap existing graph (no changes to nodes)\ngraph = build_graph_somewhere()\n\n# 2) Start event collection (LangFuse is already set up in ma.tracing)\ncollector = mt.collect_from(graph, project=\"proj1\", task=\"codegen\")\n\n# 3) Define a staged plan\nplan = mt.Plan(\n    stages=[\n        mt.Stage(\n            name=\"bootstrap-sft\",\n            source=mt.Source(\"trace://langfuse/proj1/codegen\"),\n            curate=mt.to_sft,\n            optimize=mt.TRL_SFT(model=\"Qwen/Qwen2.5-7B\", lora=True),\n            apply=mt.apply_lora,\n        ),\n        mt.Stage(\n            name=\"align-dpo\",\n            source=mt.Source(\"trace://langfuse/proj1/codegen\"),\n            curate=mt.to_preferences,\n            optimize=mt.TRL_DPO(model=\"Qwen/Qwen2.5-7B\", lora=True),\n            apply=mt.apply_lora,\n        ),\n        mt.Stage(\n            name=\"online-rl\",\n            source=mt.Source(\"live://graph/codegen\"),\n            curate=mt.to_trajectories,\n            optimize=mt.VERL_RL(endpoint=\"http://trainer:8080\"),\n            apply=mt.apply_lora,\n        ),\n    ],\n    guard={\"qa.accuracy\": 0.9},\n)\n\nmt.run(plan)\n</code></pre> <p>## Integration points in mahsm today</p> <ul> <li>Graph + nodes: <code>mahsm.core.dspy_node</code> already unifies node IO; tuning can wrap these with event emit hooks.</li> <li>Tracing: <code>mahsm.tracing</code> (LangFuse) provides spans; tuning serializes its <code>Event</code> schema to the same backend [7].</li> <li>Testing/evals: <code>mahsm.testing.PytestHarness</code> can be the guard runner, logging back via LangFuse.</li> </ul> <p>Minimal code changes to existing modules: none. <code>mahsm.tuning</code> imports <code>mahsm.graph</code>, <code>mahsm.dspy</code>, and optional <code>mahsm.tracing</code>.</p> <p>## MVP scope (first PR)</p> <p>1) Ship <code>mahsm/tuning.py</code> with:     - Event datatypes + <code>emit()</code> + LangFuse serializers.     - <code>to_sft</code>, <code>to_preferences</code>, <code>to_trajectories</code> with basic filters.     - Adapters: <code>TRL_SFT</code>, <code>TRL_DPO</code> (local, single\u2011GPU happy path; LoRA via PEFT). Config dict passthrough.     - <code>apply_lora</code> (DSPy LM adaptor swap) and <code>apply_model</code> (endpoint swap).     - <code>Stage</code>, <code>Plan</code>, and <code>run()</code>.</p> <p>2) Example in <code>docs/getting-started/</code>: \u201cTune your existing mahsm agent in 20 lines\u201d.</p> <p>3) Optional: a tiny <code>LangFuse -&gt; SFT examples</code> cookbook.</p> <p>Out of scope for MVP: online RL training loop, remote rollout orchestration. Those come via adapters next.</p> <p>## Phase 2 (follow\u2011up PRs)</p> <ul> <li>Adapter: <code>VERL_RL</code> (consumes trajectories, talks to a remote trainer; OpenAI\u2011compatible serving) [4].</li> <li>Adapter: <code>Arbor_RL</code> (connect to Arbor server; DSPy program optimization) [2].</li> <li>Reward model and auto\u2011metrics wiring (generative RM prototypes; or use existing evaluator outputs) [4].</li> <li>Policy hot\u2011swap for LangGraph (advanced publisher).</li> </ul> <p>## Design decisions and rationale</p> <ul> <li>Event schema first: Algorithms churn; trajectories (episode \u2192 steps \u2192 rewards \u2192 metadata) are the durable contract [1][3][4].</li> <li>Single \u201cstage\u201d abstraction: reduces all tuning to four verbs; easiest conceptual on\u2011ramp.</li> <li>Adapters over implementations: leverage TRL/veRL/Arbor and inherit their improvements [2][4][5].</li> <li>Disaggregation by default: keeps mahsm runtime responsive; trainers scale independently [1][3][4].</li> </ul> <p>## Open questions for confirmation</p> <p>1) Default trace store: standardize on LangFuse for now (yes/no)? If no, ship a neutral JSONL store as fallback.  2) First adapters to land: TRL SFT+DPO (MVP), then VERL RL, then Arbor \u2014 agree?  3) Publish targets: prioritize LoRA for size/mobility; defer full\u2011weights? Any preferred PEFT backend/models?  4) Guard API: reuse <code>mahsm.testing</code> evaluators, or keep a lightweight metric callback in <code>tuning.run()</code>?</p> <p>## References</p> <p>[1] Agent Lightning: Train ANY AI Agents with Reinforcement Learning (arXiv, 2025) \u2014 https://arxiv.org/abs/2508.03680</p> <p>[2] Ziems/arbor: A framework for optimizing DSPy programs with RL \u2014 https://github.com/ziems/arbor</p> <p>[3] Agent Lightning project &amp; docs \u2014 https://github.com/microsoft/agent-lightning and https://microsoft.github.io/agent-lightning/latest/</p> <p>[4] veRL/VERL (Volcano Engine RL for LLMs) \u2014 https://github.com/volcengine/verl and release notes \u2014 https://github.com/volcengine/verl/releases</p> <p>[5] Hugging Face TRL (SFT/DPO/RL) \u2014 https://github.com/huggingface/trl</p> <p>[6] TRL documentation: SFT Trainer and DPO Trainer \u2014 https://huggingface.co/docs/trl/en/sft_trainer and https://huggingface.co/docs/trl/en/dpo_trainer</p> <p>[7] LangFuse data/API: query traces via SDKs \u2014 https://langfuse.com/docs/api-and-data-platform/features/query-via-sdk</p> <p>[8] LangFuse export options \u2014 https://langfuse.com/docs/api-and-data-platform/features/export-from-ui</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Getting <code>mahsm</code> installed is quick and easy. We recommend using a virtual environment to manage your project's dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>uv</code> (or <code>pip</code>) package installer</li> </ul>"},{"location":"getting-started/installation/#installing-mahsm","title":"Installing <code>mahsm</code>","text":"<p>To install the core <code>mahsm</code> library, run the following command:</p> <pre><code>uv pip install mahsm\n</code></pre> <p>This will install mahsm and its core dependencies, including DSPy, LangGraph, LangFuse, and EvalProtocol.</p>"},{"location":"getting-started/installation/#setting-up-observability-langfuse","title":"Setting Up Observability (LangFuse)","text":"<p>One of the core features of mahsm is its deep integration with LangFuse for observability. To enable it, you need to set the following environment variables: <pre><code>export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nexport LANGFUSE_SECRET_KEY=\"sk-lf-...\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\" # Or your self-hosted instance\n</code></pre></p> <p>You can find your keys in your LangFuse project settings.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart: Building Your First mahsm Agent","text":"<p>Let's build a simple research agent in 60 seconds to see how mahsm works. This example demonstrates the declarative nature of the framework.</p>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# 1. Configure DSPy\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# 2. Initialize tracing (do this once at the start of your app)\nma.tracing.init()\n\n# 3. Define the shared state\nclass AgentState(TypedDict):\n    question: str\n    research_result: Optional[str]\n\n# 4. Create a reasoning node with @ma.dspy_node\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"question -&gt; research_result\")\n\n    def forward(self, question):\n        return self.predictor(question=question)\n\n# 5. Build and compile the graph\nworkflow = ma.graph.StateGraph(AgentState)\nworkflow.add_node(\"researcher\", Researcher())\nworkflow.add_edge(ma.START, \"researcher\")\nworkflow.add_edge(\"researcher\", ma.END)\ngraph = workflow.compile()\n\n# 6. Run your agent\nresult = graph.invoke({\"question\": \"What is the future of multi-agent AI systems?\"})\nprint(result['research_result'])\n# \u2705 Automatically traced in Langfuse!\n</code></pre> <p>That's it! You've built a fully observable and testable agent with minimal boilerplate.</p>"},{"location":"getting-started/quickstart/#breaking-it-down","title":"Breaking It Down","text":""},{"location":"getting-started/quickstart/#1-configure-dspy","title":"1. Configure DSPy","text":"<pre><code>import dspy\nimport os\n\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n</code></pre> <p>What's happening: - Configure the language model that DSPy will use - <code>dspy.LM()</code> supports OpenAI, Anthropic, local models, and more - Use environment variables for API keys (never hardcode!)</p>"},{"location":"getting-started/quickstart/#2-initialize-tracing","title":"2. Initialize Tracing","text":"<pre><code>ma.tracing.init()\n</code></pre> <p>What's happening: - One line enables automatic tracing for all LLM calls - Traces are sent to Langfuse for observability - Make sure you have <code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, and <code>LANGFUSE_BASE_URL</code> in your environment</p>"},{"location":"getting-started/quickstart/#3-define-state","title":"3. Define State","text":"<pre><code>from typing import TypedDict, Optional\n\nclass AgentState(TypedDict):\n    question: str\n    research_result: Optional[str]\n</code></pre> <p>What's happening: - State is a TypedDict that flows through your workflow - <code>question</code> is required (input) - <code>research_result</code> is optional (populated by nodes) - Type-safe and IDE-friendly</p>"},{"location":"getting-started/quickstart/#4-create-a-dspy-node","title":"4. Create a DSPy Node","text":"<pre><code>@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"question -&gt; research_result\")\n\n    def forward(self, question):\n        return self.predictor(question=question)\n</code></pre> <p>What's happening: - <code>@ma.dspy_node</code> makes your DSPy module work with LangGraph - <code>ChainOfThought</code> adds reasoning before answering - Signature <code>\"question -&gt; research_result\"</code> matches state keys - <code>forward()</code> method contains your logic</p>"},{"location":"getting-started/quickstart/#5-build-the-workflow","title":"5. Build the Workflow","text":"<pre><code>workflow = ma.graph.StateGraph(AgentState)\nworkflow.add_node(\"researcher\", Researcher())\nworkflow.add_edge(ma.START, \"researcher\")\nworkflow.add_edge(\"researcher\", ma.END)\ngraph = workflow.compile()\n</code></pre> <p>What's happening: - <code>StateGraph(AgentState)</code> creates a workflow with your state schema - <code>add_node()</code> adds your Researcher to the graph - <code>add_edge()</code> defines the flow: START \u2192 researcher \u2192 END - <code>compile()</code> turns the workflow into an executable graph</p>"},{"location":"getting-started/quickstart/#6-run-your-agent","title":"6. Run Your Agent","text":"<pre><code>result = graph.invoke({\"question\": \"What is the future of multi-agent AI systems?\"})\nprint(result['research_result'])\n</code></pre> <p>What's happening: - <code>invoke()</code> runs the workflow with initial state - State flows through nodes, getting updated along the way - Returns the final state with all populated fields - All LLM calls are automatically traced to Langfuse!</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts \u2192 Understand the mahsm philosophy</li> <li>DSPy Overview \u2192 Learn about DSPy modules</li> <li>LangGraph Overview \u2192 Learn about workflows</li> <li>Your First Agent \u2192 Build a complete multi-step agent</li> </ul> <p>Ready to build more? Explore the Building Blocks! \ud83d\ude80</p>"}]}