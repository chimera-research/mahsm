{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mahsm Documentation","text":"<p>Build production-grade AI systems with declarative simplicity.</p> <p>mahsm is a Python framework that combines the best tools for building, tracing, and evaluating LLM-powered applications\u2014wrapped in a simple, declarative API.</p>"},{"location":"#what-is-mahsm","title":"What is mahsm?","text":"<p>mahsm integrates four powerful frameworks into a unified development experience:</p> <ul> <li>DSPy \u2192 Prompt engineering through programming</li> <li>LangGraph \u2192 Stateful, cyclical agent workflows  </li> <li>Langfuse \u2192 Production-grade observability</li> <li>EvalProtocol \u2192 Systematic evaluation &amp; testing</li> </ul> <p>Instead of learning four different APIs, you learn one: mahsm's declarative interface.</p>"},{"location":"#why-mahsm","title":"Why mahsm?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Building production LLM applications requires: 1. Smart prompting (DSPy's modules &amp; optimizers) 2. Complex workflows (LangGraph's state machines) 3. Deep observability (Langfuse's tracing) 4. Rigorous testing (EvalProtocol's evaluations)</p> <p>Each framework has its own API, patterns, and integration challenges.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>mahsm provides:</p> <pre><code>import mahsm as ma\nimport dspy\nimport os\n\n# 1. Configure once\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()  # Automatic tracing for everything\n\n# 2. Define agents declaratively\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.researcher = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.researcher(question=question)\n\n# 3. Build workflows visually\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"research\", Researcher())\nworkflow.add_edge(ma.START, \"research\")\ngraph = workflow.compile()\n\n# 4. Run &amp; automatically trace\nresult = graph.invoke({\"question\": \"...\"})\n# \u2705 All LLM calls traced to Langfuse\n# \u2705 Full execution graph visible\n# \u2705 Costs &amp; latencies tracked\n\n# 5. Evaluate systematically\n@ma.testing.evaluation_test(...)\nasync def test_quality(row):\n    return await ma.testing.aha_judge(row, rubric=\"...\")\n# \u2705 Results synced to Langfuse\n# \u2705 Model comparisons automated\n</code></pre> <p>Result: You write less code, iterate faster, and ship with confidence.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#declarative-api","title":"\ud83c\udfaf Declarative API","text":"<p>Define what you want, not how to build it:</p> <pre><code># Instead of manually chaining prompts...\n@ma.dspy_node\nclass MyAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n</code></pre>"},{"location":"#automatic-tracing","title":"\ud83d\udd04 Automatic Tracing","text":"<p>One line enables observability for all frameworks:</p> <pre><code>ma.tracing.init()\n# \u2705 DSPy modules traced\n# \u2705 LangGraph nodes traced\n# \u2705 Custom @observe functions traced\n</code></pre>"},{"location":"#unified-testing","title":"\ud83d\udcca Unified Testing","text":"<p>Test across models, prompts, and configurations:</p> <pre><code>@ma.testing.evaluation_test(\n    completion_params=[\n        {\"model\": \"openai/gpt-4o-mini\"},\n        {\"model\": \"openai/gpt-4o\"},\n    ]\n)\nasync def test_agent(row):\n    # Runs on both models, compares results\n    pass\n</code></pre>"},{"location":"#production-ready","title":"\ud83d\ude80 Production-Ready","text":"<ul> <li>Type-safe state management (TypedDict)</li> <li>Structured logging with Langfuse</li> <li>Automated evaluation pipelines</li> <li>Cost &amp; latency tracking</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mahsm\n</code></pre>"},{"location":"#your-first-agent-60-seconds","title":"Your First Agent (60 seconds)","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# Define state\nclass State(TypedDict):\n    question: str\n    answer: str\n\n# Define agent\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Build graph\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"question\": \"What is DSPy?\"})\nprint(result[\"answer\"])\n# Output visible in Langfuse UI automatically!\n</code></pre> <p>Next: Follow the Quick Start Guide for a complete walkthrough.</p>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#new-to-llm-development","title":"\ud83c\udf93 New to LLM Development?","text":"<p>Start here to learn the fundamentals:</p> <ol> <li>Installation - Set up your environment</li> <li>Core Concepts - Understanding the mahsm philosophy</li> <li>Your First Agent - Build a complete agent step-by-step</li> </ol>"},{"location":"#want-to-understand-the-building-blocks","title":"\ud83d\udd27 Want to Understand the Building Blocks?","text":"<p>Deep dive into each framework:</p> <ul> <li>DSPy Basics - Signatures, modules, optimizers</li> <li>LangGraph Basics - State, nodes, edges, routing</li> <li>Langfuse Basics - Tracing, observability, scoring</li> <li>EvalProtocol Basics - Testing, evaluation, metrics</li> </ul>"},{"location":"#ready-to-build","title":"\ud83d\ude80 Ready to Build?","text":"<p>Check out complete examples:</p> <ul> <li>Research Agent - Multi-step reasoning pipeline</li> <li>Multi-Agent System - Coordinated agent teams</li> <li>Evaluation Pipeline - Comprehensive testing setup</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>mahsm is built on four pillars:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Your Application                \u2502\n\u2502   (Agents, Workflows, Evaluations)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 mahsm API\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              mahsm Core                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  @dspy   \u2502 .tracing \u2502   .testing   \u2502    \u2502\n\u2502  \u2502  _node   \u2502  .init() \u2502 .evaluation  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  DSPy   \u2502 \u2502Langfuse \u2502 \u2502EvalProtocol \u2502\n   \u2502 Modules \u2502 \u2502 Tracing \u2502 \u2502    Tests    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502        LangGraph Workflows          \u2502\n   \u2502   (StateGraph, compile, invoke)     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Points: - DSPy powers intelligent prompting - LangGraph orchestrates execution - Langfuse traces everything automatically - EvalProtocol validates quality</p> <p>mahsm's <code>@dspy_node</code> decorator bridges DSPy modules and LangGraph nodes, while <code>ma.tracing.init()</code> instruments the entire stack.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udcd6 Documentation: You're reading it! Explore the sidebar \u2192</li> <li>\ud83d\udcac GitHub Discussions: Ask questions</li> <li>\ud83d\udc1b Issues: Report bugs</li> <li>\u2b50 Star the repo: Show your support</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Installation Guide \u2192 Set up mahsm</li> <li>Quick Start \u2192 Build your first agent  </li> <li>DSPy Overview \u2192 Learn prompt engineering</li> <li>LangGraph Overview \u2192 Learn workflows</li> </ul> <p>Ready to build? Let's go! \ud83d\ude80</p>"},{"location":"building-blocks/dspy/modules/","title":"DSPy Modules","text":"<p>TL;DR: Modules are reusable components that combine signatures with prompting strategies like chain-of-thought or ReAct.</p>"},{"location":"building-blocks/dspy/modules/#what-are-dspy-modules","title":"What are DSPy Modules?","text":"<p>DSPy Modules are the building blocks of your LLM pipelines. Each module: - Takes a signature (input \u2192 output specification) - Applies a prompting strategy (e.g., chain-of-thought, few-shot) - Returns structured outputs</p> <p>Think of modules as smart function wrappers that automatically generate and execute prompts.</p>"},{"location":"building-blocks/dspy/modules/#built-in-modules","title":"Built-in Modules","text":""},{"location":"building-blocks/dspy/modules/#1-dspypredict","title":"1. dspy.Predict","text":"<p>The simplest module\u2014direct prediction without reasoning.</p> <pre><code>import dspy\n\npredictor = dspy.Predict(\"question -&gt; answer\")\n\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre> <p>When to use: - Simple, straightforward tasks - When you don't need reasoning traces - Fast, low-token operations</p>"},{"location":"building-blocks/dspy/modules/#2-dspychainofthought","title":"2. dspy.ChainOfThought","text":"<p>Adds step-by-step reasoning before the final answer.</p> <pre><code>cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\nresult = cot(question=\"If a train travels 60 mph for 2.5 hours, how far does it go?\")\nprint(result.reasoning)  # \"Let me think step by step...\"\nprint(result.answer)      # \"150 miles\"\n</code></pre> <p>How it works: - Automatically adds a <code>reasoning</code> field to outputs - Prompts the model to \"think step by step\" - Better for complex reasoning tasks</p> <p>When to use: - Mathematical problems - Multi-step reasoning - When you want to see the model's thought process</p> <p>Example with mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass MathState(TypedDict):\n    problem: str\n    reasoning: str\n    solution: str\n\n@ma.dspy_node\nclass MathSolver(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.solver = dspy.ChainOfThought(\"problem -&gt; reasoning, solution\")\n\n    def forward(self, problem):\n        return self.solver(problem=problem)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(MathState)\nworkflow.add_node(\"solve\", MathSolver())\n# Both reasoning and solution are written to state!\n</code></pre>"},{"location":"building-blocks/dspy/modules/#3-dspyreact","title":"3. dspy.ReAct","text":"<p>Implements the ReAct pattern (Reasoning + Acting) for tool-using agents.</p> <pre><code>from dspy import ReAct\n\n# Define tools\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Your search implementation\n    return f\"Results for: {query}\"\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return str(eval(expression))\n\n# Create ReAct module\nreact = dspy.ReAct(\"question -&gt; answer\", tools=[search_web, calculate])\n\nresult = react(question=\"What is the population of Tokyo times 2?\")\nprint(result.answer)\n# Agent will:\n# 1. search_web(\"population of Tokyo\")\n# 2. calculate(\"37400000 * 2\")\n# 3. Return final answer\n</code></pre> <p>How it works: - Model alternates between Reasoning and Acting (tool calls) - Automatically generates tool calls based on the question - Continues until it has enough information</p> <p>When to use: - When your agent needs external tools (search, calculator, APIs) - Multi-step tasks requiring information gathering - Agentic workflows</p> <p>Example with mahsm:</p> <pre><code>@ma.dspy_node\nclass ResearchAgent(ma.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\", tools=tools)\n\n    def forward(self, question):\n        return self.react(question=question)\n\n# Define tools\ndef search_papers(query: str) -&gt; str:\n    \"\"\"Search academic papers.\"\"\"\n    return \"Paper results...\"\n\n# Use in workflow\nagent = ResearchAgent(tools=[search_papers])\nworkflow.add_node(\"research\", agent)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#4-dspyprogramofthought","title":"4. dspy.ProgramOfThought","text":"<p>Combines natural language reasoning with code execution.</p> <pre><code>pot = dspy.ProgramOfThought(\"problem -&gt; answer\")\n\nresult = pot(problem=\"Calculate the compound interest on $1000 at 5% for 3 years\")\n# Generates Python code, executes it, returns answer\nprint(result.answer)  # \"$1157.63\"\n</code></pre> <p>When to use: - Mathematical computations - Tasks requiring precise calculations - When you want guaranteed accuracy for arithmetic</p>"},{"location":"building-blocks/dspy/modules/#5-dspymultichaincomparison","title":"5. dspy.MultiChainComparison","text":"<p>Generates multiple reasoning chains and selects the best one.</p> <pre><code>mcc = dspy.MultiChainComparison(\"question -&gt; answer\", M=3)\n\nresult = mcc(question=\"What are the benefits of renewable energy?\")\n# Generates 3 different reasoning chains, picks the best\nprint(result.answer)\n</code></pre> <p>When to use: - High-stakes decisions - When you want diverse perspectives - Quality over speed</p>"},{"location":"building-blocks/dspy/modules/#custom-modules","title":"Custom Modules","text":"<p>Create your own modules by subclassing <code>dspy.Module</code>:</p> <pre><code>import dspy\n\nclass CustomPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize sub-modules\n        self.classifier = dspy.Predict(\"text -&gt; category\")\n        self.summarizer = dspy.ChainOfThought(\"text, category -&gt; summary\")\n\n    def forward(self, text):\n        # Step 1: Classify\n        category_result = self.classifier(text=text)\n        category = category_result.category\n\n        # Step 2: Summarize based on category\n        if category == \"technical\":\n            # Use chain-of-thought for complex content\n            return self.summarizer(text=text, category=category)\n        else:\n            # Simple summary for non-technical\n            return {\"summary\": text[:100]}\n</code></pre> <p>Key points: 1. Always call <code>super().__init__()</code> 2. Initialize sub-modules in <code>__init__</code> 3. Implement <code>forward()</code> method 4. Return a dict or DSPy prediction</p>"},{"location":"building-blocks/dspy/modules/#module-composition","title":"Module Composition","text":"<p>Combine modules to build complex pipelines:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, results -&gt; answer, sources\")\n        self.verify = dspy.Predict(\"answer, sources -&gt; confidence: float 0-1\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        results = self.search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        synthesis = self.synthesize(question=question, results=results)\n\n        # Step 4: Verify confidence\n        verification = self.verify(\n            answer=synthesis.answer,\n            sources=synthesis.sources\n        )\n\n        return {\n            \"answer\": synthesis.answer,\n            \"sources\": synthesis.sources,\n            \"confidence\": verification.confidence\n        }\n\n    def search_api(self, query):\n        # Your search implementation\n        return f\"Results for {query}\"\n</code></pre>"},{"location":"building-blocks/dspy/modules/#modules-in-mahsm","title":"Modules in mahsm","text":"<p>The <code>@dspy_node</code> decorator makes DSPy modules work seamlessly with LangGraph:</p>"},{"location":"building-blocks/dspy/modules/#basic-usage","title":"Basic Usage","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    question: str\n    answer: str\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Add to workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\n</code></pre>"},{"location":"building-blocks/dspy/modules/#multi-module-pipeline","title":"Multi-Module Pipeline","text":"<pre><code>class PipelineState(TypedDict):\n    question: str\n    search_query: str\n    results: str\n    answer: str\n\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.ChainOfThought(\"question -&gt; search_query\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n@ma.dspy_node\nclass Synthesizer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.synth = dspy.ChainOfThought(\"question, results -&gt; answer\")\n\n    def forward(self, question, results):\n        return self.synth(question=question, results=results)\n\n# Build workflow\nworkflow = ma.graph.StateGraph(PipelineState)\nworkflow.add_node(\"generate_query\", QueryGenerator())\nworkflow.add_node(\"search\", search_function)  # Regular function\nworkflow.add_node(\"synthesize\", Synthesizer())\n\nworkflow.add_edge(ma.START, \"generate_query\")\nworkflow.add_edge(\"generate_query\", \"search\")\nworkflow.add_edge(\"search\", \"synthesize\")\nworkflow.add_edge(\"synthesize\", ma.END)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#module-configuration","title":"Module Configuration","text":""},{"location":"building-blocks/dspy/modules/#model-selection","title":"Model Selection","text":"<p>Configure the model used by all modules:</p> <pre><code>import dspy\nimport os\n\n# Option 1: OpenAI\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 2: Anthropic\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 3: Local model\nlm = dspy.LM('ollama/llama3.1', api_base='http://localhost:11434')\ndspy.configure(lm=lm)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#per-module-configuration","title":"Per-Module Configuration","text":"<pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Use different models for different tasks\n        self.fast_predictor = dspy.Predict(\"input -&gt; output\")\n        self.complex_reasoner = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        # Configure different models per call\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o-mini')):\n            quick = self.fast_predictor(input=input)\n\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o')):\n            detailed = self.complex_reasoner(input=input)\n\n        return {\"quick\": quick.output, \"detailed\": detailed.output}\n</code></pre>"},{"location":"building-blocks/dspy/modules/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/modules/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use ChainOfThought for complex tasks <pre><code># \u2705 Better reasoning\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Compose modules for complex pipelines <pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"...\")\n        self.step2 = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Match module outputs to state fields <pre><code>class State(TypedDict):\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use ReAct for tool-using agents <pre><code>dspy.ReAct(\"question -&gt; answer\", tools=[...])\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mix Predict and ChainOfThought unnecessarily <pre><code># \u274c Inconsistent reasoning\nself.mod1 = dspy.Predict(\"q -&gt; a\")\nself.mod2 = dspy.ChainOfThought(\"q -&gt; a\")\n# Pick one strategy per pipeline\n</code></pre></p> </li> <li> <p>Forget to initialize parent class <pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        # \u274c Missing super().__init__()\n        self.predictor = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Create deeply nested modules <pre><code># \u274c Too complex\nclass A(dspy.Module):\n    def __init__(self):\n        self.b = B()\n\nclass B(dspy.Module):\n    def __init__(self):\n        self.c = C()\n# Keep it flat and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#comparison-table","title":"Comparison Table","text":"Module Use Case Reasoning Tool Use Speed Predict Simple tasks \u274c None \u274c No \u26a1\u26a1\u26a1 Fast ChainOfThought Complex reasoning \u2705 Yes \u274c No \u26a1\u26a1 Medium ReAct Tool-using agents \u2705 Yes \u2705 Yes \u26a1 Slow ProgramOfThought Math/code tasks \u2705 Yes (code) \u2705 Code exec \u26a1 Slow MultiChainComparison High-quality outputs \u2705 Multiple \u274c No \ud83d\udc0c Very slow"},{"location":"building-blocks/dspy/modules/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Optimizers \u2192 Automatically improve your modules</li> <li>Best Practices \u2192 Production tips</li> <li>Your First Agent \u2192 Build a complete agent</li> </ul>"},{"location":"building-blocks/dspy/modules/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Modules Docs - Official guide</li> <li>DSPy Examples - Real-world module usage</li> </ul> <p>Next: Learn about DSPy Optimizers \u2192</p>"},{"location":"building-blocks/dspy/optimizers/","title":"DSPy Optimizers","text":"<p>TL;DR: Optimizers (teleprompts) automatically improve your DSPy modules by learning from examples\u2014no manual prompt engineering needed.</p>"},{"location":"building-blocks/dspy/optimizers/#what-are-dspy-optimizers","title":"What are DSPy Optimizers?","text":"<p>DSPy Optimizers (also called teleprompts) are algorithms that automatically improve your modules by: - Learning from training examples - Generating better prompts - Adding few-shot demonstrations - Tuning instructions</p> <p>Instead of manually tweaking prompts, you define success criteria and let the optimizer find the best approach.</p>"},{"location":"building-blocks/dspy/optimizers/#why-use-optimizers","title":"Why Use Optimizers?","text":""},{"location":"building-blocks/dspy/optimizers/#manual-prompting-traditional","title":"Manual Prompting (Traditional)","text":"<pre><code># \u274c Manual iteration\nprompt = \"Answer the question: {question}\"\n# Try it... doesn't work well\nprompt = \"Think step by step and answer: {question}\"\n# Try again... better but not perfect\nprompt = \"You are an expert. Think carefully and answer: {question}\"\n# Keep iterating...\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#automatic-optimization-dspy","title":"Automatic Optimization (DSPy)","text":"<pre><code># \u2705 Define success metric\ndef accuracy(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# \u2705 Let optimizer find the best approach\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_module = optimizer.compile(my_module, trainset=examples)\n# Done! Module is automatically improved\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/optimizers/#1-metrics","title":"1. Metrics","text":"<p>A metric function measures success:</p> <pre><code>def my_metric(example, prediction, trace=None):\n    \"\"\"\n    Args:\n        example: Ground truth from trainset\n        prediction: Module's output\n        trace: Optional execution trace\n\n    Returns:\n        float or bool: Score (higher is better)\n    \"\"\"\n    # Simple exact match\n    return example.answer == prediction.answer\n\n# Or more nuanced\ndef f1_metric(example, prediction):\n    # Calculate F1 score\n    precision = calculate_precision(example, prediction)\n    recall = calculate_recall(example, prediction)\n    return 2 * (precision * recall) / (precision + recall)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-training-set","title":"2. Training Set","text":"<p>Examples with inputs and expected outputs:</p> <pre><code>import dspy\n\ntrainset = [\n    dspy.Example(\n        question=\"What is 2+2?\",\n        answer=\"4\"\n    ).with_inputs(\"question\"),  # Mark what's an input\n\n    dspy.Example(\n        question=\"What is the capital of France?\",\n        answer=\"Paris\"\n    ).with_inputs(\"question\"),\n]\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#3-compilation","title":"3. Compilation","text":"<p>The optimization process:</p> <pre><code>optimizer = BootstrapFewShot(metric=accuracy)\noptimized = optimizer.compile(\n    student=my_module,      # Module to optimize\n    trainset=trainset,      # Training examples\n    teacher=None            # Optional better model\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#built-in-optimizers","title":"Built-in Optimizers","text":""},{"location":"building-blocks/dspy/optimizers/#1-bootstrapfewshot","title":"1. BootstrapFewShot","text":"<p>Learns few-shot examples from your training data.</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Create optimizer\noptimizer = BootstrapFewShot(\n    metric=accuracy,\n    max_bootstrapped_demos=4,  # Number of examples to add\n    max_labeled_demos=4         # Max examples per prompt\n)\n\n# Optimize module\noptimized_qa = optimizer.compile(\n    student=qa_module,\n    trainset=train_examples\n)\n\n# optimized_qa now includes learned few-shot examples!\n</code></pre> <p>How it works: 1. Runs your module on training examples 2. Keeps successful predictions as demonstrations 3. Adds them to future prompts automatically</p> <p>When to use: - You have labeled training data - Few-shot learning helps your task - You want quick improvements</p> <p>Example:</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"),\n    dspy.Example(question=\"What is 3*3?\", answer=\"9\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use optimized version\nresult = optimized_qa(question=\"What is 5+5?\")\nprint(result.answer)  # More likely to be correct!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-mipro-multi-prompt-instruction-proposal-optimizer","title":"2. MIPRO (Multi-prompt Instruction Proposal Optimizer)","text":"<p>Advanced optimizer that tunes instructions and demonstrations.</p> <pre><code>from dspy.teleprompt import MIPRO\n\noptimizer = MIPRO(\n    metric=accuracy,\n    num_candidates=10,  # Number of prompt variations to try\n    init_temperature=1.0\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples,  # Validation set for selection\n    num_trials=20\n)\n</code></pre> <p>How it works: 1. Generates multiple prompt instruction variations 2. Tests each on training data 3. Selects best based on validation performance</p> <p>When to use: - You have both training and validation sets - You want the best possible performance - You can afford longer optimization time</p>"},{"location":"building-blocks/dspy/optimizers/#3-bootstrapfewshotwithrandomsearch","title":"3. BootstrapFewShotWithRandomSearch","text":"<p>Combines few-shot learning with random search over hyperparameters.</p> <pre><code>from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\noptimizer = BootstrapFewShotWithRandomSearch(\n    metric=accuracy,\n    max_bootstrapped_demos=4,\n    num_candidate_programs=10,  # Number of variations to try\n    num_threads=4               # Parallel evaluation\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#4-bayesiansignatureoptimizer","title":"4. BayesianSignatureOptimizer","text":"<p>Uses Bayesian optimization to find best prompts.</p> <pre><code>from dspy.teleprompt import BayesianSignatureOptimizer\n\noptimizer = BayesianSignatureOptimizer(\n    metric=accuracy,\n    n=20  # Number of optimization steps\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#advanced-teacher-student-optimization","title":"Advanced: Teacher-Student Optimization","text":"<p>Use a stronger model (teacher) to generate labels for a weaker model (student):</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Configure teacher (expensive, high-quality model)\nteacher_lm = dspy.LM('openai/gpt-4o', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Configure student (cheap, fast model)\nstudent_lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Create modules\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Teacher uses GPT-4o\nwith dspy.settings.context(lm=teacher_lm):\n    teacher = QA()\n\n# Student uses GPT-4o-mini\nwith dspy.settings.context(lm=student_lm):\n    student = QA()\n\n# Bootstrap student from teacher\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_student = optimizer.compile(\n    student=student,\n    teacher=teacher,  # Use teacher to generate examples\n    trainset=trainset\n)\n\n# optimized_student achieves near-teacher quality at student cost!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#optimization-in-mahsm","title":"Optimization in mahsm","text":"<p>Optimize mahsm modules just like regular DSPy modules:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define state\nclass QAState(TypedDict):\n    question: str\n    answer: str\n\n# Define module\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is DSPy?\", answer=\"A framework...\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=lambda e, p: e.answer in p.answer)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", optimized_qa)  # Use optimized version!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/optimizers/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Start with BootstrapFewShot <pre><code># \u2705 Simple and effective\noptimizer = BootstrapFewShot(metric=accuracy)\n</code></pre></p> </li> <li> <p>Use meaningful metrics <pre><code># \u2705 Task-specific\ndef f1_score(example, prediction):\n    return calculate_f1(example, prediction)\n</code></pre></p> </li> <li> <p>Use validation sets for selection <pre><code># \u2705 Prevents overfitting\noptimizer.compile(student, trainset=train, valset=val)\n</code></pre></p> </li> <li> <p>Start small, then scale <pre><code># \u2705 Iterate on 10 examples first\ntrainset_small = trainset[:10]\noptimized = optimizer.compile(module, trainset=trainset_small)\n# Then use full dataset\n</code></pre></p> </li> <li> <p>Save optimized modules <pre><code># \u2705 Don't re-optimize every time\noptimized.save(\"optimized_qa.json\")\nloaded = QA()\nloaded.load(\"optimized_qa.json\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Optimize without evaluation <pre><code># \u274c How do you know it's better?\noptimized = optimizer.compile(module, trainset=data)\n# \u2705 Always evaluate\nscore = evaluate(optimized, testset)\n</code></pre></p> </li> <li> <p>Use tiny training sets <pre><code># \u274c 2 examples won't help\ntrainset = [example1, example2]\n# \u2705 Use at least 20-50 examples\n</code></pre></p> </li> <li> <p>Over-optimize on training data <pre><code># \u274c Overfitting risk\nmax_bootstrapped_demos=100  # Too many!\n# \u2705 Use 3-5 demonstrations\nmax_bootstrapped_demos=4\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#evaluation-after-optimization","title":"Evaluation After Optimization","text":"<p>Always evaluate on a held-out test set:</p> <pre><code>from dspy.evaluate import Evaluate\n\n# Create evaluator\nevaluator = Evaluate(\n    devset=testset,\n    metric=accuracy,\n    num_threads=4,\n    display_progress=True\n)\n\n# Compare before and after\nbaseline_score = evaluator(original_module)\noptimized_score = evaluator(optimized_module)\n\nprint(f\"Baseline: {baseline_score}%\")\nprint(f\"Optimized: {optimized_score}%\")\nprint(f\"Improvement: {optimized_score - baseline_score}%\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#complete-optimization-pipeline","title":"Complete Optimization Pipeline","text":"<pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\nfrom dspy.evaluate import Evaluate\n\n# 1. Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 2. Create datasets\nfull_data = load_data()\ntrain, val, test = split_data(full_data, [0.7, 0.15, 0.15])\n\n# 3. Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# 4. Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match, max_bootstrapped_demos=4)\noptimized_qa = optimizer.compile(student=qa, trainset=train, valset=val)\n\n# 5. Evaluate\nevaluator = Evaluate(devset=test, metric=exact_match)\nbaseline_score = evaluator(qa)\noptimized_score = evaluator(optimized_qa)\n\nprint(f\"Baseline: {baseline_score:.1f}%\")\nprint(f\"Optimized: {optimized_score:.1f}%\")\n\n# 6. Save\nif optimized_score &gt; baseline_score:\n    optimized_qa.save(\"best_qa_model.json\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#comparison-table","title":"Comparison Table","text":"Optimizer Speed Quality Best For BootstrapFewShot \u26a1\u26a1\u26a1 Fast \u2b50\u2b50 Good Quick improvements MIPRO \u26a1 Slow \u2b50\u2b50\u2b50 Best Maximum quality BootstrapFewShotWithRandomSearch \u26a1\u26a1 Medium \u2b50\u2b50\u2b50 Better Balanced BayesianSignatureOptimizer \u26a1 Slow \u2b50\u2b50\u2b50 Better Complex tasks"},{"location":"building-blocks/dspy/optimizers/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices \u2192 Production DSPy tips</li> <li>Optimization Workflow Guide \u2192 Complete tutorial</li> <li>LangGraph Overview \u2192 Learn about workflows</li> </ul>"},{"location":"building-blocks/dspy/optimizers/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Optimizers Docs - Official guide</li> <li>DSPy Optimization Paper - Research paper</li> </ul> <p>Next: Explore Best Practices \u2192</p>"},{"location":"building-blocks/dspy/overview/","title":"DSPy Overview","text":"<p>TL;DR: DSPy turns prompt engineering into programming\u2014define what you want, not how to prompt for it.</p>"},{"location":"building-blocks/dspy/overview/#what-is-dspy","title":"What is DSPy?","text":"<p>DSPy (Declarative Self-improving Language Programs in Python) is a framework for building LLM applications through programming, not manual prompting.</p> <p>Instead of writing and tweaking prompts like this:</p> <pre><code># \u274c Traditional prompting\nprompt = \"\"\"\nYou are a helpful assistant. Given a question, provide a detailed answer.\n\nQuestion: {question}\nThink step by step and provide your reasoning.\n\nAnswer:\n\"\"\"\nresponse = llm.complete(prompt.format(question=\"What is DSPy?\"))\n</code></pre> <p>You write code like this:</p> <pre><code># \u2705 DSPy approach\nimport dspy\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.cot(question=question)\n\nqa = QA()\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)\n</code></pre> <p>Key Insight: You declare the structure (chain-of-thought reasoning), and DSPy generates the actual prompts automatically.</p>"},{"location":"building-blocks/dspy/overview/#why-dspy","title":"Why DSPy?","text":""},{"location":"building-blocks/dspy/overview/#1-composability","title":"1. Composability","text":"<p>Build complex pipelines from simple components:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, context -&gt; answer\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        context = search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        return self.synthesize(question=question, context=context)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-automatic-optimization","title":"2. Automatic Optimization","text":"<p>DSPy can automatically improve your prompts:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Define success metric\ndef validate_answer(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# Optimize the pipeline\noptimizer = BootstrapFewShot(metric=validate_answer)\noptimized_qa = optimizer.compile(QA(), trainset=examples)\n\n# optimized_qa now has better prompts learned from examples!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-model-agnostic","title":"3. Model Agnostic","text":"<p>Switch between models without changing code:</p> <pre><code># Use GPT-4o-mini\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Later, switch to Claude\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n# Your code stays the same!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/overview/#1-signatures","title":"1. Signatures","text":"<p>Signatures define input \u2192 output specifications:</p> <pre><code># Simple signature\n\"question -&gt; answer\"\n\n# Multi-input signature\n\"question, context -&gt; answer\"\n\n# With hints\n\"question -&gt; answer: a detailed, technical response\"\n</code></pre> <p>Learn more about Signatures \u2192</p>"},{"location":"building-blocks/dspy/overview/#2-modules","title":"2. Modules","text":"<p>Modules are reusable components that use signatures:</p> <pre><code># Built-in modules\ndspy.Predict(\"question -&gt; answer\")          # Basic prediction\ndspy.ChainOfThought(\"question -&gt; answer\")   # With reasoning\ndspy.ReAct(\"question -&gt; answer\")            # Tool-using agent\n\n# Custom modules\nclass MyAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        return self.predictor(input=input)\n</code></pre> <p>Learn more about Modules \u2192</p>"},{"location":"building-blocks/dspy/overview/#3-optimizers-teleprompts","title":"3. Optimizers (Teleprompts)","text":"<p>Optimizers automatically improve your modules:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot, MIPRO\n\n# Few-shot learning\noptimizer = BootstrapFewShot(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=data)\n\n# Advanced optimization\noptimizer = MIPRO(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=train, valset=val)\n</code></pre> <p>Learn more about Optimizers \u2192</p>"},{"location":"building-blocks/dspy/overview/#dspy-in-mahsm","title":"DSPy in mahsm","text":"<p>mahsm makes DSPy even easier by integrating it with LangGraph workflows:</p>"},{"location":"building-blocks/dspy/overview/#the-dspy_node-decorator","title":"The <code>@dspy_node</code> Decorator","text":"<p>Convert any DSPy module into a LangGraph node:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n\n# Use it in a workflow\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"researcher\", Researcher())  # \u2705 Works seamlessly\n</code></pre> <p>How it works: 1. <code>@dspy_node</code> wraps your DSPy module 2. Automatically extracts inputs from state 3. Merges outputs back into state 4. Handles Langfuse tracing</p> <p>Learn more about @dspy_node \u2192</p>"},{"location":"building-blocks/dspy/overview/#quick-example-building-a-qa-agent","title":"Quick Example: Building a Q&amp;A Agent","text":"<p>Let's build a complete Q&amp;A agent using DSPy + mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# 1. Configure DSPy\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 2. Define state\nclass QAState(TypedDict):\n    question: str\n    reasoning: str\n    answer: str\n\n# 3. Create DSPy module\n@ma.dspy_node\nclass QAAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 4. Build LangGraph workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QAAgent())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\"question\": \"What are the benefits of using DSPy?\"})\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Reasoning: {result['reasoning']}\")\n# \u2705 Automatically traced in Langfuse!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#when-to-use-dspy","title":"When to Use DSPy","text":""},{"location":"building-blocks/dspy/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Complex reasoning tasks requiring chain-of-thought</li> <li>Multi-step pipelines with intermediate outputs</li> <li>Optimizable systems where you can measure success</li> <li>Model-agnostic applications that need portability</li> </ul>"},{"location":"building-blocks/dspy/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot prompts (just use the LLM API directly)</li> <li>When you need exact prompt control (DSPy generates prompts)</li> <li>Streaming responses with partial updates (DSPy is batch-oriented)</li> </ul>"},{"location":"building-blocks/dspy/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/overview/#1-sequential-processing","title":"1. Sequential Processing","text":"<pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"input -&gt; intermediate\")\n        self.step2 = dspy.ChainOfThought(\"intermediate -&gt; output\")\n\n    def forward(self, input):\n        intermediate = self.step1(input=input)\n        return self.step2(intermediate=intermediate.intermediate)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-conditional-logic","title":"2. Conditional Logic","text":"<pre><code>class ConditionalAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.classifier = dspy.Predict(\"question -&gt; category\")\n        self.tech_expert = dspy.ChainOfThought(\"question -&gt; answer\")\n        self.general_expert = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        category = self.classifier(question=question).category\n\n        if \"technical\" in category.lower():\n            return self.tech_expert(question=question)\n        else:\n            return self.general_expert(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-tool-use-with-react","title":"3. Tool Use with ReAct","text":"<pre><code>class ToolUser(dspy.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\")\n        self.react.tools = tools\n\n    def forward(self, question):\n        return self.react(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Signatures \u2192 Learn how to define inputs and outputs</li> <li>DSPy Modules \u2192 Explore built-in modules like ChainOfThought, ReAct</li> <li>DSPy Optimizers \u2192 Automatically improve your prompts</li> <li>Best Practices \u2192 Tips for production DSPy code</li> </ul>"},{"location":"building-blocks/dspy/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official DSPy Docs - Comprehensive DSPy documentation</li> <li>DSPy GitHub - Source code and examples</li> <li>DSPy Paper - Research paper explaining DSPy</li> </ul> <p>Ready to dive deeper? Start with Signatures \u2192</p>"},{"location":"building-blocks/dspy/signatures/","title":"DSPy Signatures","text":"<p>TL;DR: Signatures are type specifications that tell DSPy what inputs your module needs and what outputs it should produce.</p>"},{"location":"building-blocks/dspy/signatures/#what-is-a-signature","title":"What is a Signature?","text":"<p>A signature in DSPy is like a function type hint\u2014it specifies: - What inputs the module receives - What outputs it should produce - Optional descriptions for each field</p> <p>Think of it as a contract between your code and the LLM.</p>"},{"location":"building-blocks/dspy/signatures/#basic-syntax","title":"Basic Syntax","text":""},{"location":"building-blocks/dspy/signatures/#string-signatures","title":"String Signatures","text":"<p>The simplest way to define a signature:</p> <pre><code>import dspy\n\n# Single input \u2192 single output\n\"question -&gt; answer\"\n\n# Multiple inputs \u2192 single output\n\"question, context -&gt; answer\"\n\n# Multiple inputs \u2192 multiple outputs\n\"question, context -&gt; answer, confidence\"\n</code></pre> <p>Format: <code>input1, input2 -&gt; output1, output2</code></p>"},{"location":"building-blocks/dspy/signatures/#example","title":"Example","text":"<pre><code># Create a predictor with a signature\nqa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n# Use it\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)  # Access output by name\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#adding-descriptions","title":"Adding Descriptions","text":"<p>You can add hints to guide the LLM:</p> <pre><code># Add output description\n\"question -&gt; answer: a concise, factual response\"\n\n# Add input descriptions\n\"question: a technical question -&gt; answer: a detailed explanation\"\n\n# Multiple fields with descriptions\n\"question: user query, context: relevant docs -&gt; answer: synthesized response, sources: list of citations\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#example-with-descriptions","title":"Example with Descriptions","text":"<pre><code>predictor = dspy.ChainOfThought(\n    \"question: a user's question about AI -&gt; answer: a detailed, technical explanation\"\n)\n\nresult = predictor(question=\"How does attention work in transformers?\")\nprint(result.answer)\n# Output will be more detailed and technical due to the hint\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#class-based-signatures","title":"Class-Based Signatures","text":"<p>For complex signatures, use classes:</p> <pre><code>import dspy\n\nclass QASignature(dspy.Signature):\n    \"\"\"Answer questions with detailed explanations.\"\"\"\n\n    question = dspy.InputField(desc=\"The user's question\")\n    context = dspy.InputField(desc=\"Relevant background information\")\n    answer = dspy.OutputField(desc=\"A comprehensive answer\")\n    confidence = dspy.OutputField(desc=\"Confidence score (0-100)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#using-class-signatures","title":"Using Class Signatures","text":"<pre><code># Pass the class (not an instance!)\npredictor = dspy.ChainOfThought(QASignature)\n\nresult = predictor(\n    question=\"What is DSPy?\",\n    context=\"DSPy is a framework for prompt programming...\"\n)\nprint(result.answer)\nprint(result.confidence)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#input-and-output-fields","title":"Input and Output Fields","text":""},{"location":"building-blocks/dspy/signatures/#inputfield","title":"InputField","text":"<p>Defines what the module receives:</p> <pre><code>import dspy\n\nclass MySignature(dspy.Signature):\n    # Basic input\n    query = dspy.InputField()\n\n    # With description\n    context = dspy.InputField(desc=\"Background information\")\n\n    # With format hint\n    examples = dspy.InputField(desc=\"Few-shot examples\", format=list)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#outputfield","title":"OutputField","text":"<p>Defines what the module produces:</p> <pre><code>class MySignature(dspy.Signature):\n    # Basic output\n    answer = dspy.OutputField()\n\n    # With description\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n\n    # With prefix (shown before the output in the prompt)\n    summary = dspy.OutputField(prefix=\"SUMMARY:\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/signatures/#1-simple-qa","title":"1. Simple Q&amp;A","text":"<pre><code>\"question -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#2-context-aware-qa","title":"2. Context-Aware Q&amp;A","text":"<pre><code>\"question, context -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#3-multi-output","title":"3. Multi-Output","text":"<pre><code>\"document -&gt; summary, key_points, sentiment\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#4-complex-reasoning","title":"4. Complex Reasoning","text":"<pre><code>class ReasoningSignature(dspy.Signature):\n    \"\"\"Solve complex problems with step-by-step reasoning.\"\"\"\n\n    problem = dspy.InputField(desc=\"The problem to solve\")\n    constraints = dspy.InputField(desc=\"Any constraints or requirements\")\n\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n    solution = dspy.OutputField(desc=\"The final solution\")\n    confidence = dspy.OutputField(desc=\"Confidence level (low/medium/high)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#5-classification","title":"5. Classification","text":"<pre><code>class ClassificationSignature(dspy.Signature):\n    \"\"\"Classify text into categories.\"\"\"\n\n    text = dspy.InputField(desc=\"The text to classify\")\n    categories = dspy.InputField(desc=\"Valid categories (comma-separated)\")\n\n    category = dspy.OutputField(desc=\"The chosen category\")\n    reason = dspy.OutputField(desc=\"Brief explanation for the choice\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#signatures-in-mahsm","title":"Signatures in mahsm","text":"<p>When using <code>@dspy_node</code>, signatures determine how inputs are extracted from state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\n# 1. Define state\nclass ResearchState(TypedDict):\n    question: str\n    context: str\n    answer: str\n    reasoning: str\n\n# 2. Create module with signature\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        # Signature matches state fields\n        self.research = dspy.ChainOfThought(\"question, context -&gt; answer, reasoning\")\n\n    def forward(self, question, context):\n        return self.research(question=question, context=context)\n\n# 3. Use in workflow\nworkflow = ma.graph.StateGraph(ResearchState)\nworkflow.add_node(\"researcher\", Researcher())\n\n# When the node runs:\n# - \"question\" and \"context\" are extracted from state\n# - \"answer\" and \"reasoning\" are written back to state\n</code></pre> <p>Key Point: Match your signature field names to your state keys for seamless integration!</p>"},{"location":"building-blocks/dspy/signatures/#advanced-dynamic-signatures","title":"Advanced: Dynamic Signatures","text":"<p>Create signatures programmatically:</p> <pre><code>def create_signature(input_fields, output_fields):\n    inputs = \", \".join(input_fields)\n    outputs = \", \".join(output_fields)\n    return f\"{inputs} -&gt; {outputs}\"\n\n# Example: Dynamic fields\nsig = create_signature([\"question\", \"context\"], [\"answer\", \"score\"])\n# Result: \"question, context -&gt; answer, score\"\n\npredictor = dspy.Predict(sig)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/signatures/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use descriptive field names <pre><code>\"user_question -&gt; detailed_answer\"  # \u2705 Clear\n</code></pre></p> </li> <li> <p>Add descriptions for ambiguous fields <pre><code>\"query: the user's search query -&gt; results: list of relevant items\"\n</code></pre></p> </li> <li> <p>Match state keys in mahsm <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use multi-output for intermediate reasoning <pre><code>\"question -&gt; reasoning, answer\"  # \u2705 Captures thought process\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Use vague names <pre><code>\"input -&gt; output\"  # \u274c Not descriptive\n</code></pre></p> </li> <li> <p>Mix concerns in one field <pre><code>\"query -&gt; answer_and_confidence\"  # \u274c Split into two outputs\n</code></pre></p> </li> <li> <p>Over-complicate <pre><code># \u274c Too many fields\n\"q, c1, c2, c3, c4 -&gt; a, r1, r2, r3, conf, meta\"\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#troubleshooting","title":"Troubleshooting","text":""},{"location":"building-blocks/dspy/signatures/#issue-llm-not-returning-expected-output","title":"Issue: LLM not returning expected output","text":"<p>Solution: Add more specific descriptions</p> <pre><code># Before (vague)\n\"text -&gt; category\"\n\n# After (specific)\n\"text: a customer review -&gt; category: one of [positive, negative, neutral]\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-output-format-is-inconsistent","title":"Issue: Output format is inconsistent","text":"<p>Solution: Use structured output hints</p> <pre><code>class StructuredSignature(dspy.Signature):\n    query = dspy.InputField()\n    answer = dspy.OutputField(desc=\"Answer in JSON format with keys: summary, details\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-state-keys-dont-match-signature","title":"Issue: State keys don't match signature","text":"<p>Solution: Ensure field names align</p> <pre><code># State has \"user_query\"\nclass State(TypedDict):\n    user_query: str\n\n# \u274c Signature uses \"question\"\ndspy.ChainOfThought(\"question -&gt; answer\")  # Won't find \"question\" in state!\n\n# \u2705 Match the key\ndspy.ChainOfThought(\"user_query -&gt; answer\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Modules \u2192 Learn about Predict, ChainOfThought, ReAct</li> <li>Your First Agent \u2192 Build a complete agent</li> <li>API Reference: @dspy_node \u2192 mahsm integration details</li> </ul>"},{"location":"building-blocks/dspy/signatures/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Signatures Documentation - Official guide</li> <li>DSPy Examples - Real-world signature usage</li> </ul> <p>Next: Explore DSPy Modules \u2192</p>"},{"location":"building-blocks/langgraph/overview/","title":"LangGraph Overview","text":"<p>TL;DR: LangGraph builds stateful, cyclical workflows for LLM agents\u2014think state machines for AI.</p>"},{"location":"building-blocks/langgraph/overview/#what-is-langgraph","title":"What is LangGraph?","text":"<p>LangGraph is a framework for building stateful, multi-step workflows with LLMs. Unlike simple chains (input \u2192 LLM \u2192 output), LangGraph enables:</p> <ul> <li>Cycles: Agents can loop, retry, and refine</li> <li>State: Persistent memory across steps</li> <li>Branching: Conditional routing based on outputs</li> <li>Parallelism: Run multiple nodes concurrently</li> </ul> <p>Think of it as a state machine where each node is an AI agent or tool.</p>"},{"location":"building-blocks/langgraph/overview/#why-langgraph","title":"Why LangGraph?","text":""},{"location":"building-blocks/langgraph/overview/#the-problem-with-chains","title":"The Problem with Chains","text":"<p>Traditional LLM chains are linear:</p> <pre><code># \u274c Linear chain - can't loop or branch\nquery \u2192 retrieve_docs \u2192 generate_answer \u2192 done\n</code></pre> <p>Real agents need to: - Loop until a condition is met - Branch based on intermediate results - Maintain state across steps</p>"},{"location":"building-blocks/langgraph/overview/#the-langgraph-solution","title":"The LangGraph Solution","text":"<pre><code># \u2705 Cyclic workflow with branching\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  generate   \u2502\n       \u2502   query     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   search    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502 synthesize  \u2502      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502   check     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502  quality    \u2502 if poor, retry\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502 if good\n              \u25bc\n            END\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/langgraph/overview/#1-state","title":"1. State","text":"<p>State is a <code>TypedDict</code> that flows through your workflow:</p> <pre><code>from typing import TypedDict, Optional\n\nclass ResearchState(TypedDict):\n    question: str\n    search_query: Optional[str]\n    findings: Optional[str]\n    answer: Optional[str]\n</code></pre> <p>Learn more about State \u2192</p>"},{"location":"building-blocks/langgraph/overview/#2-nodes","title":"2. Nodes","text":"<p>Nodes are functions or agents that process state:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n</code></pre> <p>Learn more about Nodes &amp; Edges \u2192</p>"},{"location":"building-blocks/langgraph/overview/#3-edges","title":"3. Edges","text":"<p>Edges connect nodes:</p> <pre><code># Simple edge\nworkflow.add_edge(\"node_a\", \"node_b\")\n\n# Conditional edge\nworkflow.add_conditional_edges(\n    \"checker\",\n    lambda state: \"retry\" if state[\"quality\"] &lt; 0.7 else END\n)\n</code></pre> <p>Learn more about Conditional Routing \u2192</p>"},{"location":"building-blocks/langgraph/overview/#4-graph-compilation","title":"4. Graph Compilation","text":"<p>Compile the workflow into an executable graph:</p> <pre><code>workflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"agent\", my_agent)\nworkflow.add_edge(ma.START, \"agent\")\nworkflow.add_edge(\"agent\", ma.END)\n\ngraph = workflow.compile()  # \u2705 Ready to run\n</code></pre> <p>Learn more about Compilation \u2192</p>"},{"location":"building-blocks/langgraph/overview/#quick-example","title":"Quick Example","text":"<p>Let's build a self-correcting Q&amp;A agent:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 1. Define state\nclass QAState(TypedDict):\n    question: str\n    answer: Optional[str]\n    quality_score: Optional[float]\n    iteration: int\n\n# 2. Define nodes\n@ma.dspy_node\nclass Answerer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityChecker(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.checker = dspy.Predict(\"question, answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, question, answer):\n        return self.checker(question=question, answer=answer)\n\ndef increment_iteration(state: QAState) -&gt; QAState:\n    \"\"\"Increment iteration counter.\"\"\"\n    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n\n# 3. Define routing\ndef should_retry(state: QAState):\n    \"\"\"Retry if quality is low and we haven't exceeded max iterations.\"\"\"\n    if state.get(\"iteration\", 0) &gt;= 3:\n        return ma.END  # Give up after 3 tries\n\n    quality = float(state.get(\"quality_score\", 0))\n    if quality &lt; 0.7:\n        return \"answer\"  # Retry\n    return ma.END  # Good enough!\n\n# 4. Build graph\nworkflow = ma.graph.StateGraph(QAState)\n\nworkflow.add_node(\"answer\", Answerer())\nworkflow.add_node(\"check\", QualityChecker())\nworkflow.add_node(\"increment\", increment_iteration)\n\nworkflow.add_edge(ma.START, \"increment\")\nworkflow.add_edge(\"increment\", \"answer\")\nworkflow.add_edge(\"answer\", \"check\")\nworkflow.add_conditional_edges(\"check\", should_retry)\n\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\n    \"question\": \"Explain quantum entanglement simply.\",\n    \"iteration\": 0\n})\n\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Quality: {result['quality_score']}\")\nprint(f\"Iterations: {result['iteration']}\")\n# \u2705 Agent retries until quality threshold is met!\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#langgraph-in-mahsm","title":"LangGraph in mahsm","text":"<p>mahsm enhances LangGraph with:</p>"},{"location":"building-blocks/langgraph/overview/#1-simplified-node-creation","title":"1. Simplified Node Creation","text":"<pre><code># Without mahsm\ndef my_node(state):\n    # Manual state extraction\n    question = state[\"question\"]\n    # Call LLM\n    response = llm.complete(question)\n    # Manual state update\n    return {\"answer\": response}\n\n# With mahsm\n@ma.dspy_node\nclass MyNode(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n# \u2705 State extraction/merging handled automatically\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-automatic-tracing","title":"2. Automatic Tracing","text":"<pre><code>ma.tracing.init()  # One line\n# \u2705 All LangGraph nodes traced to Langfuse\n# \u2705 All DSPy calls traced\n# \u2705 Custom functions with @observe traced\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-type-safe-state","title":"3. Type-Safe State","text":"<pre><code>class MyState(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 IDE autocomplete\n# \u2705 Static type checking\n# \u2705 Runtime validation\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#when-to-use-langgraph","title":"When to Use LangGraph","text":""},{"location":"building-blocks/langgraph/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Multi-step agents that need memory</li> <li>Cyclical workflows (retry, refine, iterate)</li> <li>Conditional branching based on outputs</li> <li>Complex orchestration of multiple agents</li> <li>Human-in-the-loop systems</li> </ul>"},{"location":"building-blocks/langgraph/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot completions (use DSPy directly)</li> <li>Purely stateless operations (no need for state management)</li> <li>Real-time streaming (LangGraph is batch-oriented)</li> </ul>"},{"location":"building-blocks/langgraph/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/langgraph/overview/#1-linear-pipeline","title":"1. Linear Pipeline","text":"<pre><code>START \u2192 agent1 \u2192 agent2 \u2192 agent3 \u2192 END\n</code></pre> <pre><code>workflow.add_edge(ma.START, \"agent1\")\nworkflow.add_edge(\"agent1\", \"agent2\")\nworkflow.add_edge(\"agent2\", \"agent3\")\nworkflow.add_edge(\"agent3\", ma.END)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-conditional-branching","title":"2. Conditional Branching","text":"<pre><code>                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      START \u2500\u2500\u2500\u2500\u2500\u25ba\u2502 router  \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                 \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 path_a \u2502       \u2502 path_b \u2502\n         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n                    END\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"router\",\n    lambda state: \"path_a\" if condition(state) else \"path_b\"\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-retry-loop","title":"3. Retry Loop","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502              \u2502\n        \u25bc              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  try  \u2502\u2500\u2500\u2500\u25ba\u2502   check   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u25bc\n                    END (if success)\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"check\",\n    lambda state: \"try\" if not success(state) else ma.END\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#4-multi-agent-collaboration","title":"4. Multi-Agent Collaboration","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u250c\u2500\u2500\u25ba\u2502 researcher\u2502\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n    \u2502                     \u25bc\n    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2502coordinator\u2502\u25c4\u2500\u2500\u2502synthesizer\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/overview/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict for state <pre><code>class State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Keep nodes focused <pre><code># \u2705 Single responsibility\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    # Only generates queries\n    pass\n</code></pre></p> </li> <li> <p>Handle missing state gracefully <pre><code>def my_router(state):\n    value = state.get(\"key\", default_value)\n    # ...\n</code></pre></p> </li> <li> <p>Use conditional edges for routing <pre><code>workflow.add_conditional_edges(\"checker\", route_function)\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Don't do this\ndef node(state):\n    state[\"key\"] = \"value\"  # Mutates input!\n    return state\n\n# \u2705 Do this\ndef node(state):\n    return {\"key\": \"value\"}  # Returns update\n</code></pre></p> </li> <li> <p>Create infinite loops without exit conditions <pre><code># \u274c No way to exit\nworkflow.add_conditional_edges(\"node\", lambda s: \"node\")\n\n# \u2705 Add exit condition\ndef router(state):\n    if state[\"count\"] &gt; 10:\n        return ma.END\n    return \"node\"\n</code></pre></p> </li> <li> <p>Over-complicate the graph <pre><code># \u274c Too many branches\n# Keep it simple and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#next-steps","title":"Next Steps","text":"<ul> <li>State Management \u2192 Learn about TypedDict and state updates</li> <li>Nodes &amp; Edges \u2192 Build your graph components</li> <li>Conditional Routing \u2192 Add branching logic</li> <li>Compilation &amp; Execution \u2192 Run your workflows</li> <li>Visualization \u2192 Visualize your graphs</li> </ul>"},{"location":"building-blocks/langgraph/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official LangGraph Docs - Comprehensive guide</li> <li>LangGraph GitHub - Source code and examples</li> <li>LangGraph Tutorials - Step-by-step guides</li> </ul> <p>Ready to dive deeper? Start with State Management \u2192</p>"},{"location":"building-blocks/langgraph/state/","title":"LangGraph State Management","text":"<p>TL;DR: State in LangGraph is a TypedDict that flows through your workflow, carrying data between nodes.</p>"},{"location":"building-blocks/langgraph/state/#what-is-state","title":"What is State?","text":"<p>In LangGraph, state is a dictionary that: - Flows through your workflow - Is read by nodes - Is updated by nodes - Maintains type safety with <code>TypedDict</code></p> <p>Think of it as shared memory for your agent workflow.</p>"},{"location":"building-blocks/langgraph/state/#defining-state","title":"Defining State","text":"<p>Use Python's <code>TypedDict</code> to define your state schema:</p> <pre><code>from typing import TypedDict, Optional\n\nclass MyState(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]\n</code></pre> <p>Benefits: - \u2705 IDE autocomplete - \u2705 Type checking - \u2705 Clear documentation - \u2705 Runtime validation</p>"},{"location":"building-blocks/langgraph/state/#state-flow","title":"State Flow","text":"<p>State flows through nodes in your workflow:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    input: str\n    intermediate: str\n    output: str\n\n# Node 1: Reads 'input', writes 'intermediate'\ndef node1(state: State) -&gt; dict:\n    result = process(state[\"input\"])\n    return {\"intermediate\": result}\n\n# Node 2: Reads 'intermediate', writes 'output'\ndef node2(state: State) -&gt; dict:\n    result = finalize(state[\"intermediate\"])\n    return {\"output\": result}\n\n# Build workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"node1\", node1)\nworkflow.add_node(\"node2\", node2)\nworkflow.add_edge(ma.START, \"node1\")\nworkflow.add_edge(\"node1\", \"node2\")\nworkflow.add_edge(\"node2\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"input\": \"Hello\"})\n# State flows: {\"input\": \"Hello\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\", \"output\": \"...\"}\nprint(result[\"output\"])\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-updates","title":"State Updates","text":""},{"location":"building-blocks/langgraph/state/#immutable-updates","title":"Immutable Updates","text":"<p>Nodes return updates, not full state:</p> <pre><code># \u274c DON'T: Mutate state directly\ndef bad_node(state):\n    state[\"answer\"] = \"Paris\"  # Mutates input!\n    return state\n\n# \u2705 DO: Return updates\ndef good_node(state):\n    return {\"answer\": \"Paris\"}  # Returns update\n</code></pre> <p>LangGraph merges your update into the state automatically:</p> <pre><code># Before node\nstate = {\"question\": \"What is the capital of France?\"}\n\n# Node returns\nupdate = {\"answer\": \"Paris\"}\n\n# After node (automatic merge)\nstate = {\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\"\n}\n</code></pre>"},{"location":"building-blocks/langgraph/state/#optional-vs-required-fields","title":"Optional vs Required Fields","text":"<p>Use <code>Optional</code> for fields that may not exist initially:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    # Required fields (must be in initial input)\n    question: str\n\n    # Optional fields (nodes will populate)\n    answer: Optional[str]\n    reasoning: Optional[str]\n    confidence: Optional[float]\n</code></pre>"},{"location":"building-blocks/langgraph/state/#complex-state-types","title":"Complex State Types","text":""},{"location":"building-blocks/langgraph/state/#lists","title":"Lists","text":"<pre><code>from typing import List\n\nclass State(TypedDict):\n    messages: List[str]\n    findings: List[dict]\n</code></pre> <p>Appending to lists:</p> <pre><code>def add_message(state: State) -&gt; dict:\n    # Option 1: Replace entire list\n    new_messages = state[\"messages\"] + [\"New message\"]\n    return {\"messages\": new_messages}\n\n    # Option 2: Use Annotated for automatic appending (advanced)\n    # See LangGraph docs for details\n</code></pre>"},{"location":"building-blocks/langgraph/state/#nested-dicts","title":"Nested Dicts","text":"<pre><code>class State(TypedDict):\n    user: dict  # {\"name\": str, \"email\": str}\n    config: dict\n</code></pre>"},{"location":"building-blocks/langgraph/state/#custom-classes","title":"Custom Classes","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass User:\n    name: str\n    email: str\n\nclass State(TypedDict):\n    user: User\n    active: bool\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-in-mahsm","title":"State in mahsm","text":""},{"location":"building-blocks/langgraph/state/#with-dspy_node","title":"With @dspy_node","text":"<p><code>@dspy_node</code> automatically extracts and updates state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\n\nclass QAState(TypedDict):\n    question: str\n    reasoning: Optional[str]\n    answer: Optional[str]\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# In workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QA())\n\n# When QA runs:\n# 1. Extracts 'question' from state\n# 2. Calls forward(question=state[\"question\"])\n# 3. Merges {\"reasoning\": \"...\", \"answer\": \"...\"} into state\n</code></pre>"},{"location":"building-blocks/langgraph/state/#with-regular-functions","title":"With Regular Functions","text":"<pre><code>def my_node(state: QAState) -&gt; dict:\n    \"\"\"Regular function node.\"\"\"\n    question = state[\"question\"]\n    # Process...\n    return {\"answer\": \"Paris\"}\n\nworkflow.add_node(\"my_node\", my_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-initialization","title":"State Initialization","text":""},{"location":"building-blocks/langgraph/state/#basic-initialization","title":"Basic Initialization","text":"<pre><code># Invoke with initial state\nresult = graph.invoke({\n    \"question\": \"What is DSPy?\",\n    \"confidence\": 0.0\n})\n</code></pre>"},{"location":"building-blocks/langgraph/state/#from-user-input","title":"From User Input","text":"<pre><code>def create_initial_state(user_input: str) -&gt; dict:\n    \"\"\"Create initial state from user input.\"\"\"\n    return {\n        \"question\": user_input,\n        \"iteration\": 0,\n        \"history\": []\n    }\n\nstate = create_initial_state(\"What is LangGraph?\")\nresult = graph.invoke(state)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-persistence","title":"State Persistence","text":"<p>State is immutable within a single execution:</p> <pre><code># Single execution\nresult = graph.invoke({\"question\": \"Hello\"})\n# State flows through workflow and is returned\n\n# New execution (fresh state)\nresult2 = graph.invoke({\"question\": \"Goodbye\"})\n# Independent from result\n</code></pre> <p>For persistent state across executions, use LangGraph's checkpointing (advanced):</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\n# Compile with memory\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\n# Run with thread ID\nconfig = {\"configurable\": {\"thread_id\": \"user_123\"}}\nresult1 = graph.invoke({\"question\": \"Hello\"}, config=config)\nresult2 = graph.invoke({\"question\": \"Continue...\"}, config=config)\n# result2 has access to result1's state!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/state/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict <pre><code># \u2705 Type-safe\nclass State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Make node outputs Optional <pre><code># \u2705 Clear that these are populated later\nanswer: Optional[str]\n</code></pre></p> </li> <li> <p>Return only updates <pre><code># \u2705 Clean updates\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> <li> <p>Use descriptive field names <pre><code># \u2705 Clear purpose\nuser_question: str\ngenerated_answer: str\nquality_score: float\n</code></pre></p> </li> <li> <p>Match DSPy signatures to state keys <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 Signature matches\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Never do this\ndef node(state):\n    state[\"answer\"] = \"Paris\"\n    return state\n</code></pre></p> </li> <li> <p>Use vague names <pre><code># \u274c What is this?\nresult: str\ndata: dict\n</code></pre></p> </li> <li> <p>Make everything required <pre><code># \u274c Will error if not in initial input\nclass State(TypedDict):\n    question: str\n    answer: str  # Should be Optional[str]\n</code></pre></p> </li> <li> <p>Return full state unnecessarily <pre><code># \u274c Redundant\ndef node(state):\n    return {**state, \"answer\": \"Paris\"}\n\n# \u2705 Just return update\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#debugging-state","title":"Debugging State","text":""},{"location":"building-blocks/langgraph/state/#print-state-in-nodes","title":"Print State in Nodes","text":"<pre><code>def debug_node(state):\n    print(f\"Current state: {state}\")\n    return {}\n\nworkflow.add_node(\"debug\", debug_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#trace-state-flow","title":"Trace State Flow","text":"<pre><code># Run with verbose output\nresult = graph.invoke({\"question\": \"Hello\"})\nprint(f\"Final state: {result}\")\n</code></pre>"},{"location":"building-blocks/langgraph/state/#use-langfuse","title":"Use Langfuse","text":"<p>With <code>ma.tracing.init()</code>, state is automatically logged:</p> <pre><code>ma.tracing.init()\nresult = graph.invoke({\"question\": \"Hello\"})\n# Check Langfuse UI to see state at each node!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#advanced-state-reducers","title":"Advanced: State Reducers","text":"<p>For complex state updates (like appending to lists), use reducers:</p> <pre><code>from typing import Annotated\nfrom langgraph.graph import add\n\nclass State(TypedDict):\n    # Normal field\n    question: str\n\n    # Auto-appending list (uses 'add' reducer)\n    messages: Annotated[List[str], add]\n\n# Now nodes can just return new messages\ndef add_message(state):\n    return {\"messages\": [\"New message\"]}\n# LangGraph automatically appends to existing messages!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-schema-evolution","title":"State Schema Evolution","text":"<p>As your workflow grows, extend your state:</p> <pre><code># v1\nclass StateV1(TypedDict):\n    question: str\n    answer: Optional[str]\n\n# v2 (add new fields)\nclass StateV2(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]  # New field\n    sources: Optional[List[str]]  # New field\n</code></pre> <p>Existing nodes continue working (they ignore new fields).</p>"},{"location":"building-blocks/langgraph/state/#example-multi-step-research-state","title":"Example: Multi-Step Research State","text":"<pre><code>from typing import TypedDict, Optional, List\n\nclass ResearchState(TypedDict):\n    # Input\n    question: str\n\n    # Intermediate\n    search_queries: Optional[List[str]]\n    raw_findings: Optional[List[dict]]\n\n    # Output\n    synthesized_answer: Optional[str]\n    sources: Optional[List[str]]\n    confidence: Optional[float]\n\n    # Metadata\n    iteration: int\n    total_tokens: int\n\n# Use in workflow\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.Predict(\"question -&gt; search_queries: list of queries\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n# Workflow automatically manages all state fields!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#next-steps","title":"Next Steps","text":"<ul> <li>Nodes &amp; Edges \u2192 Learn how nodes interact with state</li> <li>Conditional Routing \u2192 Route based on state</li> <li>Your First Agent \u2192 Build a stateful agent</li> </ul>"},{"location":"building-blocks/langgraph/state/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph State Docs - Official guide</li> <li>TypedDict Documentation - Python docs</li> </ul> <p>Next: Learn about Nodes &amp; Edges \u2192</p>"},{"location":"concepts/declarative-design/","title":"Declarative by Design","text":"<p>The central philosophy of <code>mahsm</code> is its declarative approach. Instead of manually writing imperative \"glue code\" to connect different libraries, you declare the components of your system, and <code>mahsm</code> handles the integration and boilerplate.</p> <p>This \"convention over configuration\" approach is designed to let you focus entirely on your agent's business logic, not the plumbing.</p>"},{"location":"concepts/declarative-design/#the-mahsm-approach","title":"The <code>mahsm</code> Approach","text":"<ul> <li>You declare your agent's reasoning by writing standard <code>dspy.Module</code> classes. The powerful <code>@ma.dspy_node</code> decorator instantly makes them compatible with the orchestration layer.</li> <li>You declare your workflow's structure by adding your nodes to a <code>ma.graph.StateGraph</code> and defining the edges between them.</li> <li>You declare your evaluation criteria by configuring the <code>ma.testing.PytestHarness</code> to run your graph against a dataset.</li> </ul>"},{"location":"concepts/declarative-design/#the-benefits","title":"The Benefits","text":"<p>This philosophy drastically reduces boilerplate, improves code readability, and embeds best practices for observability and testing directly into the development process. The result is a workflow that is faster, more robust, and produces systems that are understandable by default.</p>"},{"location":"concepts/four-pillars/","title":"The Four Pillars of <code>mahsm</code>","text":"<p><code>mahsm</code> achieves its power by deeply integrating four essential, best-in-class libraries into a single, seamless experience. All functionality is exposed through the unified <code>import mahsm as ma</code> API, giving you a consistent and clean developer experience.</p>"},{"location":"concepts/four-pillars/#1-dspy-the-reasoning-engine-madspy","title":"1. DSPy: The Reasoning Engine (<code>ma.dspy</code>)","text":"<ul> <li>What it is: A framework from Stanford NLP for programming\u2014not just prompting\u2014language models. It separates program flow from parameters (prompts and model weights) and uses optimizers to tune them for maximum performance.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> treats DSPy modules as the fundamental building blocks of agent intelligence. The core innovation is the <code>@ma.dspy_node</code> decorator. This tool instantly transforms any <code>dspy.Module</code> into a fully compliant LangGraph node, automatically handling the complex mapping of data from the shared graph <code>State</code> to the module's inputs and back.</li> </ul>"},{"location":"concepts/four-pillars/#2-langgraph-the-orchestration-scaffolding-magraph","title":"2. LangGraph: The Orchestration Scaffolding (<code>ma.graph</code>)","text":"<ul> <li>What it is: A library for building stateful, multi-agent applications by representing them as cyclical graphs. It provides the primitives of <code>State</code>, <code>Nodes</code>, and <code>Edges</code> to create complex, long-running agentic workflows.</li> <li>How <code>mahsm</code> Fuses it: LangGraph provides the skeleton, and <code>mahsm</code> provides the intelligent organs. By making DSPy modules the primary type of \"thinking\" node, <code>mahsm</code> supercharges LangGraph development. You define your application's <code>State</code> and use <code>ma.graph.StateGraph</code> to wire together your <code>@ma.dspy_node</code> agents.</li> </ul>"},{"location":"concepts/four-pillars/#3-langfuse-the-unified-observability-layer","title":"3. LangFuse: The Unified Observability Layer","text":"<ul> <li>What it is: A comprehensive open-source platform for LLM observability, providing detailed tracing, debugging, and analytics for AI applications.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> makes deep, hierarchical tracing an automatic, zero-effort feature. The single <code>ma.init()</code> function simultaneously instruments both LangGraph and DSPy. When you run your graph, <code>mahsm</code> creates a single, unified trace in LangFuse that captures both the high-level graph flow and the low-level DSPy execution details (prompts, tool calls, etc.), solving the massive pain point of achieving end-to-end observability.</li> </ul>"},{"location":"concepts/four-pillars/#4-evalprotocol-the-quality-control-testing-framework-matesting","title":"4. EvalProtocol: The Quality Control &amp; Testing Framework (<code>ma.testing</code>)","text":"<ul> <li>What it is: A standardized, <code>pytest</code>-based framework for evaluating the performance of AI systems using LLM-as-a-judge and other metrics.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> bridges the gap between your built application and your test suite. The <code>ma.testing.PytestHarness</code> class radically simplifies setup by automatically generating the boilerplate processors required by <code>eval-protocol</code>. The harness can even pull evaluation datasets directly from your production LangFuse traces, enabling a tight, continuous loop of deploying, observing, and evaluating your system's real-world performance.</li> </ul>"},{"location":"concepts/workflow/","title":"The <code>mahsm</code> Development Workflow","text":"<p>Developing with <code>mahsm</code> follows a simple, iterative, and powerful three-step loop: Build, Trace, and Evaluate. This cycle is designed to be fast and data-driven, ensuring you are creating high-quality, robust systems.</p> <p> </p>"},{"location":"concepts/workflow/#1-build","title":"1. Build","text":"<p>This is the core development step where you write idiomatic <code>mahsm</code> code. - Define State: Create a <code>TypedDict</code> that represents the shared state of your application. - Create Nodes: Write <code>dspy.Module</code> classes to encapsulate the reasoning logic for your agents. Decorate them with <code>@ma.dspy_node</code>. - Wire Graph: Add your nodes to a <code>ma.graph.StateGraph</code> and define the edges to control the flow of execution. - Compile: Call <code>.compile()</code> on your graph to create the runnable application.</p>"},{"location":"concepts/workflow/#2-trace","title":"2. Trace","text":"<p>Once built, you run your application.     *   With <code>ma.init()</code> called at the start of your script, every execution is automatically and deeply traced in LangFuse.     *   You use the LangFuse UI to inspect the full decision-making process of your agent, debug issues, understand latency, and analyze token usage.     *   You can tag interesting traces to save them as examples for regression testing or for creating evaluation datasets.</p>"},{"location":"concepts/workflow/#3-evaluate","title":"3. Evaluate","text":"<p>Finally, you verify the quality of your agent's output.     *   Write a Test File: Create a standard <code>pytest</code> file.     *   Configure the Harness: Use the <code>ma.testing.PytestHarness</code> to connect your compiled <code>mahsm</code> graph to the evaluation protocol.     *   Run Eval: Use datasets (potentially generated from your production traces in LangFuse) to run an evaluation.     *   Analyze &amp; Iterate: Use the evaluation leaderboards and results to identify weaknesses in your agent's logic, then go back to the BUILD step to improve it.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Getting <code>mahsm</code> installed is quick and easy. We recommend using a virtual environment to manage your project's dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>uv</code> (or <code>pip</code>) package installer</li> </ul>"},{"location":"getting-started/installation/#installing-mahsm","title":"Installing <code>mahsm</code>","text":"<p>To install the core <code>mahsm</code> library, run the following command:</p> <pre><code>uv pip install mahsm\n</code></pre> <p>This will install mahsm and its core dependencies, including DSPy, LangGraph, LangFuse, and EvalProtocol.</p>"},{"location":"getting-started/installation/#setting-up-observability-langfuse","title":"Setting Up Observability (LangFuse)","text":"<p>One of the core features of mahsm is its deep integration with LangFuse for observability. To enable it, you need to set the following environment variables: <pre><code>export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nexport LANGFUSE_SECRET_KEY=\"sk-lf-...\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\" # Or your self-hosted instance\n</code></pre></p> <p>You can find your keys in your LangFuse project settings.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart: Building Your First <code>mahsm</code> Agent","text":"<p>Let's build a simple research agent to see how the core components of <code>mahsm</code> work together. This example demonstrates the declarative nature of the framework.</p>"},{"location":"getting-started/quickstart/#1-the-single-import","title":"1. The Single Import","text":"<p>All of <code>mahsm</code>'s fused functionality is available through the top-level <code>ma</code> import.</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\n</code></pre>"},{"location":"getting-started/quickstart/#2-define-the-shared-state","title":"2. Define the Shared State","text":"<p>The State is a TypedDict that defines the data structure that flows through your graph. Every node can read from and write to this state.</p> <pre><code>class AgentState(TypedDict):\n    input_query: str\n    research_result: Optional[str]\n</code></pre>"},{"location":"getting-started/quickstart/#3-create-a-reasoning-node-with-madspy_node","title":"3. Create a Reasoning Node with @ma.dspy_node","text":"<p>Here, we define the \"brain\" of our agent using a <code>dspy.Module</code>. The <code>@ma.dspy_node</code> decorator is the magic that makes this module compatible with the LangGraph orchestrator, automatically handling data mapping from the AgentState.</p> <pre><code>@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.signature = \"input_query -&gt; research_result\"\n        self.predictor = ma.dspy.ChainOfThought(self.signature)\n\n    def forward(self, input_query):\n        return self.predictor(input_query=input_query)\n</code></pre>"},{"location":"getting-started/quickstart/#4-build-and-compile-the-graph","title":"4. Build and Compile the Graph","text":"<p>Finally, we use <code>ma.graph.StateGraph</code> to define the workflow. We add our <code>Researcher</code> node and define the edges that control the flow of execution.</p> <pre><code># Initialize the graph with our state definition\nworkflow = ma.graph.StateGraph(AgentState)\n\n# Add the DSPy-powered node\nworkflow.add_node(\"researcher\", Researcher())\n\n# Define the workflow structure\nworkflow.add_edge(ma.START, \"researcher\")\nworkflow.add_edge(\"researcher\", ma.END)\n\n# Compile the graph into a runnable application\ngraph = workflow.compile()\n</code></pre>"},{"location":"getting-started/quickstart/#5-run-and-trace","title":"5. Run and Trace","text":"<p>To run your agent, simply invoke the graph. If you've called ma.init() beforehand, the entire execution will be traced in LangFuse automatically.</p> <pre><code># Initialize tracing (do this once at the start of your app)\nma.init()\n\n# Run the graph\ninputs = {\"input_query\": \"What is the future of multi-agent AI systems?\"}\nresult = graph.invoke(inputs)\n\nprint(result['research_result'])\n</code></pre> <p>That's it! You've built a fully observable and testable agent with minimal boilerplate, focusing only on the essential logic.</p>"}]}