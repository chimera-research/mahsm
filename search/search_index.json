{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mahsm Documentation","text":"<p>Build production-grade AI systems with declarative simplicity.</p> <p>mahsm is a Python framework that combines the best tools for building, tracing, and evaluating LLM-powered applications\u2014wrapped in a simple, declarative API.</p>"},{"location":"#what-is-mahsm","title":"What is mahsm?","text":"<p>mahsm integrates four powerful frameworks into a unified development experience:</p> <ul> <li>DSPy \u2192 Prompt engineering through programming</li> <li>LangGraph \u2192 Stateful, cyclical agent workflows  </li> <li>Langfuse \u2192 Production-grade observability</li> <li>EvalProtocol \u2192 Systematic evaluation &amp; testing</li> </ul> <p>Instead of learning four different APIs, you learn one: mahsm's declarative interface.</p>"},{"location":"#why-mahsm","title":"Why mahsm?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Building production LLM applications requires: 1. Smart prompting (DSPy's modules &amp; optimizers) 2. Complex workflows (LangGraph's state machines) 3. Deep observability (Langfuse's tracing) 4. Rigorous testing (EvalProtocol's evaluations)</p> <p>Each framework has its own API, patterns, and integration challenges.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>mahsm provides:</p> <pre><code>import mahsm as ma\nimport dspy\nimport os\n\n# 1. Configure once\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()  # Automatic tracing for everything\n\n# 2. Define agents declaratively\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.researcher = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.researcher(question=question)\n\n# 3. Build workflows visually\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"research\", Researcher())\nworkflow.add_edge(ma.START, \"research\")\ngraph = workflow.compile()\n\n# 4. Run &amp; automatically trace\nresult = graph.invoke({\"question\": \"...\"})\n# \u2705 All LLM calls traced to Langfuse\n# \u2705 Full execution graph visible\n# \u2705 Costs &amp; latencies tracked\n\n# 5. Evaluate systematically\n@ma.testing.evaluation_test(...)\nasync def test_quality(row):\n    return await ma.testing.aha_judge(row, rubric=\"...\")\n# \u2705 Results synced to Langfuse\n# \u2705 Model comparisons automated\n</code></pre> <p>Result: You write less code, iterate faster, and ship with confidence.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#declarative-api","title":"\ud83c\udfaf Declarative API","text":"<p>Define what you want, not how to build it:</p> <pre><code># Instead of manually chaining prompts...\n@ma.dspy_node\nclass MyAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n</code></pre>"},{"location":"#automatic-tracing","title":"\ud83d\udd04 Automatic Tracing","text":"<p>One line enables observability for all frameworks:</p> <pre><code>ma.tracing.init()\n# \u2705 DSPy modules traced\n# \u2705 LangGraph nodes traced\n# \u2705 Custom @observe functions traced\n</code></pre>"},{"location":"#unified-testing","title":"\ud83d\udcca Unified Testing","text":"<p>Test across models, prompts, and configurations:</p> <pre><code>@ma.testing.evaluation_test(\n    completion_params=[\n        {\"model\": \"openai/gpt-4o-mini\"},\n        {\"model\": \"openai/gpt-4o\"},\n    ]\n)\nasync def test_agent(row):\n    # Runs on both models, compares results\n    pass\n</code></pre>"},{"location":"#production-ready","title":"\ud83d\ude80 Production-Ready","text":"<ul> <li>Type-safe state management (TypedDict)</li> <li>Structured logging with Langfuse</li> <li>Automated evaluation pipelines</li> <li>Cost &amp; latency tracking</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mahsm\n</code></pre>"},{"location":"#your-first-agent-60-seconds","title":"Your First Agent (60 seconds)","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# Define state\nclass State(TypedDict):\n    question: str\n    answer: str\n\n# Define agent\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Build graph\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"question\": \"What is DSPy?\"})\nprint(result[\"answer\"])\n# Output visible in Langfuse UI automatically!\n</code></pre> <p>Next: Follow the Quick Start Guide for a complete walkthrough.</p>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#new-to-llm-development","title":"\ud83c\udf93 New to LLM Development?","text":"<p>Start here to learn the fundamentals:</p> <ol> <li>Installation - Set up your environment</li> <li>Core Concepts - Understanding the mahsm philosophy</li> <li>Your First Agent - Build a complete agent step-by-step</li> </ol>"},{"location":"#want-to-understand-the-building-blocks","title":"\ud83d\udd27 Want to Understand the Building Blocks?","text":"<p>Deep dive into each framework:</p> <ul> <li>DSPy Basics - Signatures, modules, optimizers</li> <li>LangGraph Basics - State, nodes, edges, routing</li> <li>Langfuse Basics - Tracing, observability, scoring</li> <li>EvalProtocol Basics - Testing, evaluation, metrics</li> </ul>"},{"location":"#ready-to-build","title":"\ud83d\ude80 Ready to Build?","text":"<p>Check out complete examples:</p> <ul> <li>Research Agent - Multi-step reasoning pipeline</li> <li>Multi-Agent System - Coordinated agent teams</li> <li>Evaluation Pipeline - Comprehensive testing setup</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>mahsm is built on four pillars:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Your Application                \u2502\n\u2502   (Agents, Workflows, Evaluations)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 mahsm API\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              mahsm Core                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  @dspy   \u2502 .tracing \u2502   .testing   \u2502    \u2502\n\u2502  \u2502  _node   \u2502  .init() \u2502 .evaluation  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  DSPy   \u2502 \u2502Langfuse \u2502 \u2502EvalProtocol \u2502\n   \u2502 Modules \u2502 \u2502 Tracing \u2502 \u2502    Tests    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502            \u2502\n   \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502        LangGraph Workflows          \u2502\n   \u2502   (StateGraph, compile, invoke)     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Points: - DSPy powers intelligent prompting - LangGraph orchestrates execution - Langfuse traces everything automatically - EvalProtocol validates quality</p> <p>mahsm's <code>@dspy_node</code> decorator bridges DSPy modules and LangGraph nodes, while <code>ma.tracing.init()</code> instruments the entire stack.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udcd6 Documentation: You're reading it! Explore the sidebar \u2192</li> <li>\ud83d\udcac GitHub Discussions: Ask questions</li> <li>\ud83d\udc1b Issues: Report bugs</li> <li>\u2b50 Star the repo: Show your support</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Installation Guide \u2192 Set up mahsm</li> <li>Quick Start \u2192 Build your first agent  </li> <li>DSPy Overview \u2192 Learn prompt engineering</li> <li>LangGraph Overview \u2192 Learn workflows</li> </ul> <p>Ready to build? Let's go! \ud83d\ude80</p>"},{"location":"building-blocks/dspy/best-practices/","title":"DSPy Best Practices","text":"<p>TL;DR: Production-ready patterns for building robust, maintainable DSPy applications with mahsm.</p>"},{"location":"building-blocks/dspy/best-practices/#module-design","title":"Module Design","text":""},{"location":"building-blocks/dspy/best-practices/#keep-modules-focused","title":"\u2705 Keep Modules Focused","text":"<p>Each module should have a single, clear responsibility:</p> <pre><code># \u2705 Good: Focused modules\nclass QueryGenerator(dspy.Module):\n    \"\"\"Only generates search queries.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.ChainOfThought(\"question -&gt; search_query\")\n\nclass ResultSynthesizer(dspy.Module):\n    \"\"\"Only synthesizes results.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.synth = dspy.ChainOfThought(\"question, results -&gt; answer\")\n\n# \u274c Bad: Does too much\nclass MegaModule(dspy.Module):\n    \"\"\"Tries to do everything.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.do_everything = dspy.ChainOfThought(\"anything -&gt; everything\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-descriptive-signatures","title":"\u2705 Use Descriptive Signatures","text":"<p>Make your signatures self-documenting:</p> <pre><code># \u2705 Good: Clear and descriptive\n\"user_question, search_results -&gt; synthesized_answer, confidence_score\"\n\n# \u274c Bad: Vague\n\"input -&gt; output\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#match-signatures-to-state","title":"\u2705 Match Signatures to State","text":"<p>When using <code>@ma.dspy_node</code>, align signature fields with state keys:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]\n\n# \u2705 Signature matches state exactly\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer, confidence\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"building-blocks/dspy/best-practices/#validate-outputs","title":"\u2705 Validate Outputs","text":"<p>Always validate LLM outputs before using them:</p> <pre><code>import dspy\n\nclass SafeQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        result = self.qa(question=question)\n\n        # Validate output\n        if not result.answer or len(result.answer) &lt; 10:\n            # Fallback or retry logic\n            return {\"answer\": \"I don't have enough information to answer that.\"}\n\n        return {\"answer\": result.answer}\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#handle-api-failures","title":"\u2705 Handle API Failures","text":"<p>Wrap LLM calls with error handling:</p> <pre><code>import dspy\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nclass RobustQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10)\n    )\n    def forward(self, question):\n        try:\n            result = self.qa(question=question)\n            return {\"answer\": result.answer}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            # Log to monitoring system\n            raise\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"building-blocks/dspy/best-practices/#use-appropriate-module-types","title":"\u2705 Use Appropriate Module Types","text":"<p>Choose the right module for the task:</p> <pre><code># \u2705 Simple tasks \u2192 Predict (fast)\nclassifier = dspy.Predict(\"text -&gt; category\")\n\n# \u2705 Complex reasoning \u2192 ChainOfThought (better quality)\nreasoner = dspy.ChainOfThought(\"problem -&gt; solution\")\n\n# \u2705 Tool use \u2192 ReAct (agentic)\nagent = dspy.ReAct(\"question -&gt; answer\", tools=tools)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#cache-expensive-operations","title":"\u2705 Cache Expensive Operations","text":"<p>Cache module compilations and optimizations:</p> <pre><code>import dspy\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_optimized_qa():\n    \"\"\"Cache the optimized module.\"\"\"\n    qa = QA()\n\n    # Check if we have a saved version\n    try:\n        qa.load(\"optimized_qa.json\")\n        return qa\n    except FileNotFoundError:\n        # Optimize and save\n        optimizer = BootstrapFewShot(metric=accuracy)\n        optimized = optimizer.compile(qa, trainset=trainset)\n        optimized.save(\"optimized_qa.json\")\n        return optimized\n\n# Use cached version\nqa_module = get_optimized_qa()\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#batch-when-possible","title":"\u2705 Batch When Possible","text":"<p>Process multiple inputs together:</p> <pre><code>class BatchQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward_batch(self, questions):\n        \"\"\"Process multiple questions efficiently.\"\"\"\n        # DSPy handles batching internally\n        results = [self.qa(question=q) for q in questions]\n        return [r.answer for r in results]\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#testing","title":"Testing","text":""},{"location":"building-blocks/dspy/best-practices/#unit-test-your-modules","title":"\u2705 Unit Test Your Modules","text":"<p>Test modules independently:</p> <pre><code>import unittest\nimport dspy\n\nclass TestQA(unittest.TestCase):\n    def setUp(self):\n        # Use a mock LM for testing\n        lm = dspy.LM('openai/gpt-4o-mini', api_key=\"test\")\n        dspy.configure(lm=lm)\n        self.qa = QA()\n\n    def test_qa_returns_answer(self):\n        result = self.qa(question=\"What is 2+2?\")\n        self.assertIn(\"answer\", result)\n        self.assertIsInstance(result[\"answer\"], str)\n\n    def test_qa_handles_empty_question(self):\n        result = self.qa(question=\"\")\n        # Should handle gracefully\n        self.assertIsNotNone(result[\"answer\"])\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#test-with-real-examples","title":"\u2705 Test with Real Examples","text":"<p>Create a test dataset:</p> <pre><code>import dspy\n\ntest_cases = [\n    dspy.Example(\n        question=\"What is the capital of France?\",\n        expected_answer=\"Paris\"\n    ).with_inputs(\"question\"),\n\n    dspy.Example(\n        question=\"What is 2+2?\",\n        expected_answer=\"4\"\n    ).with_inputs(\"question\"),\n]\n\ndef test_accuracy():\n    qa = QA()\n    correct = 0\n\n    for case in test_cases:\n        result = qa(question=case.question)\n        if case.expected_answer.lower() in result[\"answer\"].lower():\n            correct += 1\n\n    accuracy = correct / len(test_cases)\n    assert accuracy &gt;= 0.8, f\"Accuracy too low: {accuracy}\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#configuration-management","title":"Configuration Management","text":""},{"location":"building-blocks/dspy/best-practices/#use-environment-variables","title":"\u2705 Use Environment Variables","text":"<p>Never hardcode API keys or configuration:</p> <pre><code>import os\nimport dspy\n\n# \u2705 Good: Environment variables\nlm = dspy.LM(\n    'openai/gpt-4o-mini',\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    max_tokens=int(os.getenv(\"MAX_TOKENS\", \"2000\")),\n    temperature=float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n)\ndspy.configure(lm=lm)\n\n# \u274c Bad: Hardcoded\n# lm = dspy.LM('openai/gpt-4o-mini', api_key=\"sk-...\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#create-configuration-classes","title":"\u2705 Create Configuration Classes","text":"<p>Organize configuration:</p> <pre><code>from dataclasses import dataclass\nimport os\n\n@dataclass\nclass DSPyConfig:\n    model: str = \"openai/gpt-4o-mini\"\n    api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    max_tokens: int = 2000\n    temperature: float = 0.7\n\n    @classmethod\n    def from_env(cls):\n        return cls(\n            model=os.getenv(\"DSPY_MODEL\", \"openai/gpt-4o-mini\"),\n            api_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n            max_tokens=int(os.getenv(\"MAX_TOKENS\", \"2000\")),\n            temperature=float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n        )\n\n# Use it\nconfig = DSPyConfig.from_env()\nlm = dspy.LM(config.model, api_key=config.api_key, max_tokens=config.max_tokens)\ndspy.configure(lm=lm)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"building-blocks/dspy/best-practices/#always-enable-tracing","title":"\u2705 Always Enable Tracing","text":"<p>Use mahsm's tracing in production:</p> <pre><code>import mahsm as ma\n\n# Enable at application startup\nma.tracing.init()\n\n# All DSPy calls are now traced to Langfuse!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#add-custom-metrics","title":"\u2705 Add Custom Metrics","text":"<p>Track custom metrics with Langfuse:</p> <pre><code>from langfuse.decorators import observe\n\n@observe(name=\"qa_pipeline\")\ndef run_qa(question: str):\n    result = qa_module(question=question)\n\n    # Log custom metrics\n    from langfuse import Langfuse\n    client = Langfuse()\n    client.score(\n        name=\"answer_length\",\n        value=len(result[\"answer\"])\n    )\n\n    return result\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#log-important-events","title":"\u2705 Log Important Events","text":"<p>Use structured logging:</p> <pre><code>import logging\nimport dspy\n\nlogger = logging.getLogger(__name__)\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        logger.info(f\"Processing question: {question[:50]}...\")\n\n        try:\n            result = self.qa(question=question)\n            logger.info(f\"Generated answer of length {len(result.answer)}\")\n            return {\"answer\": result.answer}\n        except Exception as e:\n            logger.error(f\"Failed to answer: {e}\", exc_info=True)\n            raise\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"building-blocks/dspy/best-practices/#use-chainofthought-for-complex-tasks","title":"\u2705 Use ChainOfThought for Complex Tasks","text":"<p>Enable reasoning for better outputs:</p> <pre><code># \u2705 Good for complex tasks\nreasoner = dspy.ChainOfThought(\"problem -&gt; solution\")\n\n# \u274c Bad for complex tasks (no reasoning)\npredictor = dspy.Predict(\"problem -&gt; solution\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#add-context-to-signatures","title":"\u2705 Add Context to Signatures","text":"<p>Guide the model with descriptions:</p> <pre><code># \u2705 Good: Descriptive\nsignature = \"question: a user's technical question -&gt; answer: a detailed, accurate response with examples\"\n\n# \u274c Bad: Vague\nsignature = \"question -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-few-shot-examples","title":"\u2705 Use Few-Shot Examples","text":"<p>Optimize with BootstrapFewShot:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\nqa = QA()\noptimizer = BootstrapFewShot(metric=accuracy, max_bootstrapped_demos=4)\noptimized_qa = optimizer.compile(qa, trainset=examples)\n\n# optimized_qa now includes learned examples\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"building-blocks/dspy/best-practices/#separate-concerns","title":"\u2705 Separate Concerns","text":"<p>Organize code into modules:</p> <pre><code>my_project/\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 dspy_config.py       # Configuration\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 qa.py                # QA module\n\u2502   \u251c\u2500\u2500 summarizer.py        # Summarizer module\n\u2502   \u2514\u2500\u2500 classifier.py        # Classifier module\n\u251c\u2500\u2500 workflows/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 research_pipeline.py # LangGraph workflows\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 metrics.py           # Evaluation metrics\n\u2514\u2500\u2500 main.py                  # Application entry point\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#use-type-hints","title":"\u2705 Use Type Hints","text":"<p>Make code maintainable with types:</p> <pre><code>from typing import Dict, Any, Optional\nimport dspy\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Answer a question.\n\n        Args:\n            question: The user's question\n\n        Returns:\n            Dictionary with 'answer' key\n        \"\"\"\n        result = self.qa(question=question)\n        return {\"answer\": result.answer}\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#deployment","title":"Deployment","text":""},{"location":"building-blocks/dspy/best-practices/#use-versioned-artifacts","title":"\u2705 Use Versioned Artifacts","text":"<p>Save and version optimized modules:</p> <pre><code>import dspy\nfrom datetime import datetime\n\ndef save_optimized_module(module, version: str = None):\n    \"\"\"Save module with timestamp.\"\"\"\n    if version is None:\n        version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    filename = f\"optimized_qa_{version}.json\"\n    module.save(filename)\n    print(f\"Saved to {filename}\")\n    return filename\n\n# Usage\noptimized_qa = optimizer.compile(qa, trainset=train)\nsave_optimized_module(optimized_qa, version=\"v1.0.0\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#implement-health-checks","title":"\u2705 Implement Health Checks","text":"<p>Monitor service health:</p> <pre><code>from fastapi import FastAPI, HTTPException\nimport dspy\n\napp = FastAPI()\n\n@app.get(\"/health\")\ndef health_check():\n    \"\"\"Check if DSPy is configured correctly.\"\"\"\n    try:\n        # Test configuration\n        lm = dspy.settings.lm\n        if lm is None:\n            raise Exception(\"DSPy not configured\")\n\n        return {\"status\": \"healthy\", \"model\": str(lm)}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#handle-rate-limits","title":"\u2705 Handle Rate Limits","text":"<p>Implement backoff strategies:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import RateLimitError\n\nclass RateLimitedQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    @retry(\n        retry=retry_if_exception_type(RateLimitError),\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=2, min=4, max=60)\n    )\n    def forward(self, question):\n        return self.qa(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"building-blocks/dspy/best-practices/#forgetting-superinit","title":"\u274c Forgetting super().init()","text":"<pre><code># \u274c Will break\nclass MyModule(dspy.Module):\n    def __init__(self):\n        # Missing super().__init__()!\n        self.predictor = dspy.Predict(\"input -&gt; output\")\n\n# \u2705 Correct\nclass MyModule(dspy.Module):\n    def __init__(self):\n        super().__init__()  # Always call this!\n        self.predictor = dspy.Predict(\"input -&gt; output\")\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#not-configuring-dspy","title":"\u274c Not Configuring DSPy","text":"<pre><code># \u274c Will error\npredictor = dspy.Predict(\"question -&gt; answer\")\nresult = predictor(question=\"Hello\")  # Error: DSPy not configured!\n\n# \u2705 Configure first\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\npredictor = dspy.Predict(\"question -&gt; answer\")\nresult = predictor(question=\"Hello\")  # Works!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#ignoring-optimization","title":"\u274c Ignoring Optimization","text":"<pre><code># \u274c Using unoptimized modules in production\nqa = QA()\n# Likely suboptimal performance\n\n# \u2705 Optimize first\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_qa = optimizer.compile(qa, trainset=train)\n# Much better performance!\n</code></pre>"},{"location":"building-blocks/dspy/best-practices/#checklist-for-production","title":"Checklist for Production","text":"<p>Before deploying to production:</p> <ul> <li>[ ] All modules have <code>super().__init__()</code> calls</li> <li>[ ] DSPy is configured with proper LM</li> <li>[ ] Tracing is enabled (<code>ma.tracing.init()</code>)</li> <li>[ ] API keys are in environment variables</li> <li>[ ] Modules are optimized with real data</li> <li>[ ] Error handling is implemented</li> <li>[ ] Logging is configured</li> <li>[ ] Unit tests pass</li> <li>[ ] Integration tests pass</li> <li>[ ] Performance is acceptable (latency, cost)</li> <li>[ ] Rate limiting/backoff is handled</li> <li>[ ] Health checks are implemented</li> <li>[ ] Monitoring/alerts are set up</li> </ul>"},{"location":"building-blocks/dspy/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>LangGraph Overview \u2192 Build stateful workflows</li> <li>Production Deployment Guide \u2192 Deploy your app</li> <li>Examples \u2192 See complete applications</li> </ul>"},{"location":"building-blocks/dspy/best-practices/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Documentation - Official docs</li> <li>Langfuse Documentation - Tracing &amp; monitoring</li> <li>DSPy GitHub - Source code &amp; examples</li> </ul> <p>Ready for production? Check out Langfuse \u2192</p>"},{"location":"building-blocks/dspy/modules/","title":"DSPy Modules","text":"<p>TL;DR: Modules are reusable components that combine signatures with prompting strategies like chain-of-thought or ReAct.</p>"},{"location":"building-blocks/dspy/modules/#what-are-dspy-modules","title":"What are DSPy Modules?","text":"<p>DSPy Modules are the building blocks of your LLM pipelines. Each module: - Takes a signature (input \u2192 output specification) - Applies a prompting strategy (e.g., chain-of-thought, few-shot) - Returns structured outputs</p> <p>Think of modules as smart function wrappers that automatically generate and execute prompts.</p>"},{"location":"building-blocks/dspy/modules/#built-in-modules","title":"Built-in Modules","text":""},{"location":"building-blocks/dspy/modules/#1-dspypredict","title":"1. dspy.Predict","text":"<p>The simplest module\u2014direct prediction without reasoning.</p> <pre><code>import dspy\n\npredictor = dspy.Predict(\"question -&gt; answer\")\n\nresult = predictor(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre> <p>When to use: - Simple, straightforward tasks - When you don't need reasoning traces - Fast, low-token operations</p>"},{"location":"building-blocks/dspy/modules/#2-dspychainofthought","title":"2. dspy.ChainOfThought","text":"<p>Adds step-by-step reasoning before the final answer.</p> <pre><code>cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\nresult = cot(question=\"If a train travels 60 mph for 2.5 hours, how far does it go?\")\nprint(result.reasoning)  # \"Let me think step by step...\"\nprint(result.answer)      # \"150 miles\"\n</code></pre> <p>How it works: - Automatically adds a <code>reasoning</code> field to outputs - Prompts the model to \"think step by step\" - Better for complex reasoning tasks</p> <p>When to use: - Mathematical problems - Multi-step reasoning - When you want to see the model's thought process</p> <p>Example with mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass MathState(TypedDict):\n    problem: str\n    reasoning: str\n    solution: str\n\n@ma.dspy_node\nclass MathSolver(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.solver = dspy.ChainOfThought(\"problem -&gt; reasoning, solution\")\n\n    def forward(self, problem):\n        return self.solver(problem=problem)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(MathState)\nworkflow.add_node(\"solve\", MathSolver())\n# Both reasoning and solution are written to state!\n</code></pre>"},{"location":"building-blocks/dspy/modules/#3-dspyreact","title":"3. dspy.ReAct","text":"<p>Implements the ReAct pattern (Reasoning + Acting) for tool-using agents.</p> <pre><code>from dspy import ReAct\n\n# Define tools\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Your search implementation\n    return f\"Results for: {query}\"\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    return str(eval(expression))\n\n# Create ReAct module\nreact = dspy.ReAct(\"question -&gt; answer\", tools=[search_web, calculate])\n\nresult = react(question=\"What is the population of Tokyo times 2?\")\nprint(result.answer)\n# Agent will:\n# 1. search_web(\"population of Tokyo\")\n# 2. calculate(\"37400000 * 2\")\n# 3. Return final answer\n</code></pre> <p>How it works: - Model alternates between Reasoning and Acting (tool calls) - Automatically generates tool calls based on the question - Continues until it has enough information</p> <p>When to use: - When your agent needs external tools (search, calculator, APIs) - Multi-step tasks requiring information gathering - Agentic workflows</p> <p>Example with mahsm:</p> <pre><code>@ma.dspy_node\nclass ResearchAgent(ma.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\", tools=tools)\n\n    def forward(self, question):\n        return self.react(question=question)\n\n# Define tools\ndef search_papers(query: str) -&gt; str:\n    \"\"\"Search academic papers.\"\"\"\n    return \"Paper results...\"\n\n# Use in workflow\nagent = ResearchAgent(tools=[search_papers])\nworkflow.add_node(\"research\", agent)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#4-dspyprogramofthought","title":"4. dspy.ProgramOfThought","text":"<p>Combines natural language reasoning with code execution.</p> <pre><code>pot = dspy.ProgramOfThought(\"problem -&gt; answer\")\n\nresult = pot(problem=\"Calculate the compound interest on $1000 at 5% for 3 years\")\n# Generates Python code, executes it, returns answer\nprint(result.answer)  # \"$1157.63\"\n</code></pre> <p>When to use: - Mathematical computations - Tasks requiring precise calculations - When you want guaranteed accuracy for arithmetic</p>"},{"location":"building-blocks/dspy/modules/#5-dspymultichaincomparison","title":"5. dspy.MultiChainComparison","text":"<p>Generates multiple reasoning chains and selects the best one.</p> <pre><code>mcc = dspy.MultiChainComparison(\"question -&gt; answer\", M=3)\n\nresult = mcc(question=\"What are the benefits of renewable energy?\")\n# Generates 3 different reasoning chains, picks the best\nprint(result.answer)\n</code></pre> <p>When to use: - High-stakes decisions - When you want diverse perspectives - Quality over speed</p>"},{"location":"building-blocks/dspy/modules/#custom-modules","title":"Custom Modules","text":"<p>Create your own modules by subclassing <code>dspy.Module</code>:</p> <pre><code>import dspy\n\nclass CustomPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize sub-modules\n        self.classifier = dspy.Predict(\"text -&gt; category\")\n        self.summarizer = dspy.ChainOfThought(\"text, category -&gt; summary\")\n\n    def forward(self, text):\n        # Step 1: Classify\n        category_result = self.classifier(text=text)\n        category = category_result.category\n\n        # Step 2: Summarize based on category\n        if category == \"technical\":\n            # Use chain-of-thought for complex content\n            return self.summarizer(text=text, category=category)\n        else:\n            # Simple summary for non-technical\n            return {\"summary\": text[:100]}\n</code></pre> <p>Key points: 1. Always call <code>super().__init__()</code> 2. Initialize sub-modules in <code>__init__</code> 3. Implement <code>forward()</code> method 4. Return a dict or DSPy prediction</p>"},{"location":"building-blocks/dspy/modules/#module-composition","title":"Module Composition","text":"<p>Combine modules to build complex pipelines:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, results -&gt; answer, sources\")\n        self.verify = dspy.Predict(\"answer, sources -&gt; confidence: float 0-1\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        results = self.search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        synthesis = self.synthesize(question=question, results=results)\n\n        # Step 4: Verify confidence\n        verification = self.verify(\n            answer=synthesis.answer,\n            sources=synthesis.sources\n        )\n\n        return {\n            \"answer\": synthesis.answer,\n            \"sources\": synthesis.sources,\n            \"confidence\": verification.confidence\n        }\n\n    def search_api(self, query):\n        # Your search implementation\n        return f\"Results for {query}\"\n</code></pre>"},{"location":"building-blocks/dspy/modules/#modules-in-mahsm","title":"Modules in mahsm","text":"<p>The <code>@dspy_node</code> decorator makes DSPy modules work seamlessly with LangGraph:</p>"},{"location":"building-blocks/dspy/modules/#basic-usage","title":"Basic Usage","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    question: str\n    answer: str\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Add to workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", QA())\n</code></pre>"},{"location":"building-blocks/dspy/modules/#multi-module-pipeline","title":"Multi-Module Pipeline","text":"<pre><code>class PipelineState(TypedDict):\n    question: str\n    search_query: str\n    results: str\n    answer: str\n\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.ChainOfThought(\"question -&gt; search_query\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n@ma.dspy_node\nclass Synthesizer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.synth = dspy.ChainOfThought(\"question, results -&gt; answer\")\n\n    def forward(self, question, results):\n        return self.synth(question=question, results=results)\n\n# Build workflow\nworkflow = ma.graph.StateGraph(PipelineState)\nworkflow.add_node(\"generate_query\", QueryGenerator())\nworkflow.add_node(\"search\", search_function)  # Regular function\nworkflow.add_node(\"synthesize\", Synthesizer())\n\nworkflow.add_edge(ma.START, \"generate_query\")\nworkflow.add_edge(\"generate_query\", \"search\")\nworkflow.add_edge(\"search\", \"synthesize\")\nworkflow.add_edge(\"synthesize\", ma.END)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#module-configuration","title":"Module Configuration","text":""},{"location":"building-blocks/dspy/modules/#model-selection","title":"Model Selection","text":"<p>Configure the model used by all modules:</p> <pre><code>import dspy\nimport os\n\n# Option 1: OpenAI\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 2: Anthropic\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Option 3: Local model\nlm = dspy.LM('ollama/llama3.1', api_base='http://localhost:11434')\ndspy.configure(lm=lm)\n</code></pre>"},{"location":"building-blocks/dspy/modules/#per-module-configuration","title":"Per-Module Configuration","text":"<pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        # Use different models for different tasks\n        self.fast_predictor = dspy.Predict(\"input -&gt; output\")\n        self.complex_reasoner = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        # Configure different models per call\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o-mini')):\n            quick = self.fast_predictor(input=input)\n\n        with dspy.settings.context(lm=dspy.LM('openai/gpt-4o')):\n            detailed = self.complex_reasoner(input=input)\n\n        return {\"quick\": quick.output, \"detailed\": detailed.output}\n</code></pre>"},{"location":"building-blocks/dspy/modules/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/modules/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use ChainOfThought for complex tasks <pre><code># \u2705 Better reasoning\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Compose modules for complex pipelines <pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"...\")\n        self.step2 = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Match module outputs to state fields <pre><code>class State(TypedDict):\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use ReAct for tool-using agents <pre><code>dspy.ReAct(\"question -&gt; answer\", tools=[...])\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mix Predict and ChainOfThought unnecessarily <pre><code># \u274c Inconsistent reasoning\nself.mod1 = dspy.Predict(\"q -&gt; a\")\nself.mod2 = dspy.ChainOfThought(\"q -&gt; a\")\n# Pick one strategy per pipeline\n</code></pre></p> </li> <li> <p>Forget to initialize parent class <pre><code>class MyModule(dspy.Module):\n    def __init__(self):\n        # \u274c Missing super().__init__()\n        self.predictor = dspy.Predict(\"...\")\n</code></pre></p> </li> <li> <p>Create deeply nested modules <pre><code># \u274c Too complex\nclass A(dspy.Module):\n    def __init__(self):\n        self.b = B()\n\nclass B(dspy.Module):\n    def __init__(self):\n        self.c = C()\n# Keep it flat and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/modules/#comparison-table","title":"Comparison Table","text":"Module Use Case Reasoning Tool Use Speed Predict Simple tasks \u274c None \u274c No \u26a1\u26a1\u26a1 Fast ChainOfThought Complex reasoning \u2705 Yes \u274c No \u26a1\u26a1 Medium ReAct Tool-using agents \u2705 Yes \u2705 Yes \u26a1 Slow ProgramOfThought Math/code tasks \u2705 Yes (code) \u2705 Code exec \u26a1 Slow MultiChainComparison High-quality outputs \u2705 Multiple \u274c No \ud83d\udc0c Very slow"},{"location":"building-blocks/dspy/modules/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Optimizers \u2192 Automatically improve your modules</li> <li>Best Practices \u2192 Production tips</li> <li>Your First Agent \u2192 Build a complete agent</li> </ul>"},{"location":"building-blocks/dspy/modules/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Modules Docs - Official guide</li> <li>DSPy Examples - Real-world module usage</li> </ul> <p>Next: Learn about DSPy Optimizers \u2192</p>"},{"location":"building-blocks/dspy/optimizers/","title":"DSPy Optimizers","text":"<p>TL;DR: Optimizers (teleprompts) automatically improve your DSPy modules by learning from examples\u2014no manual prompt engineering needed.</p>"},{"location":"building-blocks/dspy/optimizers/#what-are-dspy-optimizers","title":"What are DSPy Optimizers?","text":"<p>DSPy Optimizers (also called teleprompts) are algorithms that automatically improve your modules by: - Learning from training examples - Generating better prompts - Adding few-shot demonstrations - Tuning instructions</p> <p>Instead of manually tweaking prompts, you define success criteria and let the optimizer find the best approach.</p>"},{"location":"building-blocks/dspy/optimizers/#why-use-optimizers","title":"Why Use Optimizers?","text":""},{"location":"building-blocks/dspy/optimizers/#manual-prompting-traditional","title":"Manual Prompting (Traditional)","text":"<pre><code># \u274c Manual iteration\nprompt = \"Answer the question: {question}\"\n# Try it... doesn't work well\nprompt = \"Think step by step and answer: {question}\"\n# Try again... better but not perfect\nprompt = \"You are an expert. Think carefully and answer: {question}\"\n# Keep iterating...\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#automatic-optimization-dspy","title":"Automatic Optimization (DSPy)","text":"<pre><code># \u2705 Define success metric\ndef accuracy(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# \u2705 Let optimizer find the best approach\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_module = optimizer.compile(my_module, trainset=examples)\n# Done! Module is automatically improved\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/optimizers/#1-metrics","title":"1. Metrics","text":"<p>A metric function measures success:</p> <pre><code>def my_metric(example, prediction, trace=None):\n    \"\"\"\n    Args:\n        example: Ground truth from trainset\n        prediction: Module's output\n        trace: Optional execution trace\n\n    Returns:\n        float or bool: Score (higher is better)\n    \"\"\"\n    # Simple exact match\n    return example.answer == prediction.answer\n\n# Or more nuanced\ndef f1_metric(example, prediction):\n    # Calculate F1 score\n    precision = calculate_precision(example, prediction)\n    recall = calculate_recall(example, prediction)\n    return 2 * (precision * recall) / (precision + recall)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-training-set","title":"2. Training Set","text":"<p>Examples with inputs and expected outputs:</p> <pre><code>import dspy\n\ntrainset = [\n    dspy.Example(\n        question=\"What is 2+2?\",\n        answer=\"4\"\n    ).with_inputs(\"question\"),  # Mark what's an input\n\n    dspy.Example(\n        question=\"What is the capital of France?\",\n        answer=\"Paris\"\n    ).with_inputs(\"question\"),\n]\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#3-compilation","title":"3. Compilation","text":"<p>The optimization process:</p> <pre><code>optimizer = BootstrapFewShot(metric=accuracy)\noptimized = optimizer.compile(\n    student=my_module,      # Module to optimize\n    trainset=trainset,      # Training examples\n    teacher=None            # Optional better model\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#built-in-optimizers","title":"Built-in Optimizers","text":""},{"location":"building-blocks/dspy/optimizers/#1-bootstrapfewshot","title":"1. BootstrapFewShot","text":"<p>Learns few-shot examples from your training data.</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Create optimizer\noptimizer = BootstrapFewShot(\n    metric=accuracy,\n    max_bootstrapped_demos=4,  # Number of examples to add\n    max_labeled_demos=4         # Max examples per prompt\n)\n\n# Optimize module\noptimized_qa = optimizer.compile(\n    student=qa_module,\n    trainset=train_examples\n)\n\n# optimized_qa now includes learned few-shot examples!\n</code></pre> <p>How it works: 1. Runs your module on training examples 2. Keeps successful predictions as demonstrations 3. Adds them to future prompts automatically</p> <p>When to use: - You have labeled training data - Few-shot learning helps your task - You want quick improvements</p> <p>Example:</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"),\n    dspy.Example(question=\"What is 3*3?\", answer=\"9\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use optimized version\nresult = optimized_qa(question=\"What is 5+5?\")\nprint(result.answer)  # More likely to be correct!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#2-mipro-multi-prompt-instruction-proposal-optimizer","title":"2. MIPRO (Multi-prompt Instruction Proposal Optimizer)","text":"<p>Advanced optimizer that tunes instructions and demonstrations.</p> <pre><code>from dspy.teleprompt import MIPRO\n\noptimizer = MIPRO(\n    metric=accuracy,\n    num_candidates=10,  # Number of prompt variations to try\n    init_temperature=1.0\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples,  # Validation set for selection\n    num_trials=20\n)\n</code></pre> <p>How it works: 1. Generates multiple prompt instruction variations 2. Tests each on training data 3. Selects best based on validation performance</p> <p>When to use: - You have both training and validation sets - You want the best possible performance - You can afford longer optimization time</p>"},{"location":"building-blocks/dspy/optimizers/#3-bootstrapfewshotwithrandomsearch","title":"3. BootstrapFewShotWithRandomSearch","text":"<p>Combines few-shot learning with random search over hyperparameters.</p> <pre><code>from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\noptimizer = BootstrapFewShotWithRandomSearch(\n    metric=accuracy,\n    max_bootstrapped_demos=4,\n    num_candidate_programs=10,  # Number of variations to try\n    num_threads=4               # Parallel evaluation\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#4-bayesiansignatureoptimizer","title":"4. BayesianSignatureOptimizer","text":"<p>Uses Bayesian optimization to find best prompts.</p> <pre><code>from dspy.teleprompt import BayesianSignatureOptimizer\n\noptimizer = BayesianSignatureOptimizer(\n    metric=accuracy,\n    n=20  # Number of optimization steps\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples\n)\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#advanced-teacher-student-optimization","title":"Advanced: Teacher-Student Optimization","text":"<p>Use a stronger model (teacher) to generate labels for a weaker model (student):</p> <pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Configure teacher (expensive, high-quality model)\nteacher_lm = dspy.LM('openai/gpt-4o', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Configure student (cheap, fast model)\nstudent_lm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Create modules\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Teacher uses GPT-4o\nwith dspy.settings.context(lm=teacher_lm):\n    teacher = QA()\n\n# Student uses GPT-4o-mini\nwith dspy.settings.context(lm=student_lm):\n    student = QA()\n\n# Bootstrap student from teacher\noptimizer = BootstrapFewShot(metric=accuracy)\noptimized_student = optimizer.compile(\n    student=student,\n    teacher=teacher,  # Use teacher to generate examples\n    trainset=trainset\n)\n\n# optimized_student achieves near-teacher quality at student cost!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#optimization-in-mahsm","title":"Optimization in mahsm","text":"<p>Optimize mahsm modules just like regular DSPy modules:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Define state\nclass QAState(TypedDict):\n    question: str\n    answer: str\n\n# Define module\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# Create training data\ntrainset = [\n    dspy.Example(question=\"What is DSPy?\", answer=\"A framework...\").with_inputs(\"question\"),\n    # ... more examples\n]\n\n# Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=lambda e, p: e.answer in p.answer)\noptimized_qa = optimizer.compile(qa, trainset=trainset)\n\n# Use in workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", optimized_qa)  # Use optimized version!\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/optimizers/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Start with BootstrapFewShot <pre><code># \u2705 Simple and effective\noptimizer = BootstrapFewShot(metric=accuracy)\n</code></pre></p> </li> <li> <p>Use meaningful metrics <pre><code># \u2705 Task-specific\ndef f1_score(example, prediction):\n    return calculate_f1(example, prediction)\n</code></pre></p> </li> <li> <p>Use validation sets for selection <pre><code># \u2705 Prevents overfitting\noptimizer.compile(student, trainset=train, valset=val)\n</code></pre></p> </li> <li> <p>Start small, then scale <pre><code># \u2705 Iterate on 10 examples first\ntrainset_small = trainset[:10]\noptimized = optimizer.compile(module, trainset=trainset_small)\n# Then use full dataset\n</code></pre></p> </li> <li> <p>Save optimized modules <pre><code># \u2705 Don't re-optimize every time\noptimized.save(\"optimized_qa.json\")\nloaded = QA()\nloaded.load(\"optimized_qa.json\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Optimize without evaluation <pre><code># \u274c How do you know it's better?\noptimized = optimizer.compile(module, trainset=data)\n# \u2705 Always evaluate\nscore = evaluate(optimized, testset)\n</code></pre></p> </li> <li> <p>Use tiny training sets <pre><code># \u274c 2 examples won't help\ntrainset = [example1, example2]\n# \u2705 Use at least 20-50 examples\n</code></pre></p> </li> <li> <p>Over-optimize on training data <pre><code># \u274c Overfitting risk\nmax_bootstrapped_demos=100  # Too many!\n# \u2705 Use 3-5 demonstrations\nmax_bootstrapped_demos=4\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/optimizers/#evaluation-after-optimization","title":"Evaluation After Optimization","text":"<p>Always evaluate on a held-out test set:</p> <pre><code>from dspy.evaluate import Evaluate\n\n# Create evaluator\nevaluator = Evaluate(\n    devset=testset,\n    metric=accuracy,\n    num_threads=4,\n    display_progress=True\n)\n\n# Compare before and after\nbaseline_score = evaluator(original_module)\noptimized_score = evaluator(optimized_module)\n\nprint(f\"Baseline: {baseline_score}%\")\nprint(f\"Optimized: {optimized_score}%\")\nprint(f\"Improvement: {optimized_score - baseline_score}%\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#complete-optimization-pipeline","title":"Complete Optimization Pipeline","text":"<pre><code>import dspy\nfrom dspy.teleprompt import BootstrapFewShot\nfrom dspy.evaluate import Evaluate\n\n# 1. Define module\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 2. Create datasets\nfull_data = load_data()\ntrain, val, test = split_data(full_data, [0.7, 0.15, 0.15])\n\n# 3. Define metric\ndef exact_match(example, prediction):\n    return example.answer.lower() == prediction.answer.lower()\n\n# 4. Optimize\nqa = QA()\noptimizer = BootstrapFewShot(metric=exact_match, max_bootstrapped_demos=4)\noptimized_qa = optimizer.compile(student=qa, trainset=train, valset=val)\n\n# 5. Evaluate\nevaluator = Evaluate(devset=test, metric=exact_match)\nbaseline_score = evaluator(qa)\noptimized_score = evaluator(optimized_qa)\n\nprint(f\"Baseline: {baseline_score:.1f}%\")\nprint(f\"Optimized: {optimized_score:.1f}%\")\n\n# 6. Save\nif optimized_score &gt; baseline_score:\n    optimized_qa.save(\"best_qa_model.json\")\n</code></pre>"},{"location":"building-blocks/dspy/optimizers/#comparison-table","title":"Comparison Table","text":"Optimizer Speed Quality Best For BootstrapFewShot \u26a1\u26a1\u26a1 Fast \u2b50\u2b50 Good Quick improvements MIPRO \u26a1 Slow \u2b50\u2b50\u2b50 Best Maximum quality BootstrapFewShotWithRandomSearch \u26a1\u26a1 Medium \u2b50\u2b50\u2b50 Better Balanced BayesianSignatureOptimizer \u26a1 Slow \u2b50\u2b50\u2b50 Better Complex tasks"},{"location":"building-blocks/dspy/optimizers/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices \u2192 Production DSPy tips</li> <li>Optimization Workflow Guide \u2192 Complete tutorial</li> <li>LangGraph Overview \u2192 Learn about workflows</li> </ul>"},{"location":"building-blocks/dspy/optimizers/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Optimizers Docs - Official guide</li> <li>DSPy Optimization Paper - Research paper</li> </ul> <p>Next: Explore Best Practices \u2192</p>"},{"location":"building-blocks/dspy/overview/","title":"DSPy Overview","text":"<p>TL;DR: DSPy turns prompt engineering into programming\u2014define what you want, not how to prompt for it.</p>"},{"location":"building-blocks/dspy/overview/#what-is-dspy","title":"What is DSPy?","text":"<p>DSPy (Declarative Self-improving Language Programs in Python) is a framework for building LLM applications through programming, not manual prompting.</p> <p>Instead of writing and tweaking prompts like this:</p> <pre><code># \u274c Traditional prompting\nprompt = \"\"\"\nYou are a helpful assistant. Given a question, provide a detailed answer.\n\nQuestion: {question}\nThink step by step and provide your reasoning.\n\nAnswer:\n\"\"\"\nresponse = llm.complete(prompt.format(question=\"What is DSPy?\"))\n</code></pre> <p>You write code like this:</p> <pre><code># \u2705 DSPy approach\nimport dspy\n\nclass QA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.cot = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.cot(question=question)\n\nqa = QA()\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)\n</code></pre> <p>Key Insight: You declare the structure (chain-of-thought reasoning), and DSPy generates the actual prompts automatically.</p>"},{"location":"building-blocks/dspy/overview/#why-dspy","title":"Why DSPy?","text":""},{"location":"building-blocks/dspy/overview/#1-composability","title":"1. Composability","text":"<p>Build complex pipelines from simple components:</p> <pre><code>class ResearchPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.generate_query = dspy.ChainOfThought(\"question -&gt; search_query\")\n        self.synthesize = dspy.ChainOfThought(\"question, context -&gt; answer\")\n\n    def forward(self, question):\n        # Step 1: Generate search query\n        query_result = self.generate_query(question=question)\n\n        # Step 2: Search (simulated)\n        context = search_api(query_result.search_query)\n\n        # Step 3: Synthesize answer\n        return self.synthesize(question=question, context=context)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-automatic-optimization","title":"2. Automatic Optimization","text":"<p>DSPy can automatically improve your prompts:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot\n\n# Define success metric\ndef validate_answer(example, prediction):\n    return example.answer.lower() in prediction.answer.lower()\n\n# Optimize the pipeline\noptimizer = BootstrapFewShot(metric=validate_answer)\noptimized_qa = optimizer.compile(QA(), trainset=examples)\n\n# optimized_qa now has better prompts learned from examples!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-model-agnostic","title":"3. Model Agnostic","text":"<p>Switch between models without changing code:</p> <pre><code># Use GPT-4o-mini\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# Later, switch to Claude\nlm = dspy.LM('anthropic/claude-3-5-sonnet-20241022', api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\ndspy.configure(lm=lm)\n# Your code stays the same!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/dspy/overview/#1-signatures","title":"1. Signatures","text":"<p>Signatures define input \u2192 output specifications:</p> <pre><code># Simple signature\n\"question -&gt; answer\"\n\n# Multi-input signature\n\"question, context -&gt; answer\"\n\n# With hints\n\"question -&gt; answer: a detailed, technical response\"\n</code></pre> <p>Learn more about Signatures \u2192</p>"},{"location":"building-blocks/dspy/overview/#2-modules","title":"2. Modules","text":"<p>Modules are reusable components that use signatures:</p> <pre><code># Built-in modules\ndspy.Predict(\"question -&gt; answer\")          # Basic prediction\ndspy.ChainOfThought(\"question -&gt; answer\")   # With reasoning\ndspy.ReAct(\"question -&gt; answer\")            # Tool-using agent\n\n# Custom modules\nclass MyAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"input -&gt; output\")\n\n    def forward(self, input):\n        return self.predictor(input=input)\n</code></pre> <p>Learn more about Modules \u2192</p>"},{"location":"building-blocks/dspy/overview/#3-optimizers-teleprompts","title":"3. Optimizers (Teleprompts)","text":"<p>Optimizers automatically improve your modules:</p> <pre><code>from dspy.teleprompt import BootstrapFewShot, MIPRO\n\n# Few-shot learning\noptimizer = BootstrapFewShot(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=data)\n\n# Advanced optimization\noptimizer = MIPRO(metric=my_metric)\noptimized = optimizer.compile(my_module, trainset=train, valset=val)\n</code></pre> <p>Learn more about Optimizers \u2192</p>"},{"location":"building-blocks/dspy/overview/#dspy-in-mahsm","title":"DSPy in mahsm","text":"<p>mahsm makes DSPy even easier by integrating it with LangGraph workflows:</p>"},{"location":"building-blocks/dspy/overview/#the-dspy_node-decorator","title":"The <code>@dspy_node</code> Decorator","text":"<p>Convert any DSPy module into a LangGraph node:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n\n# Use it in a workflow\nworkflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"researcher\", Researcher())  # \u2705 Works seamlessly\n</code></pre> <p>How it works: 1. <code>@dspy_node</code> wraps your DSPy module 2. Automatically extracts inputs from state 3. Merges outputs back into state 4. Handles Langfuse tracing</p> <p>Learn more about @dspy_node \u2192</p>"},{"location":"building-blocks/dspy/overview/#quick-example-building-a-qa-agent","title":"Quick Example: Building a Q&amp;A Agent","text":"<p>Let's build a complete Q&amp;A agent using DSPy + mahsm:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\nimport dspy\nimport os\n\n# 1. Configure DSPy\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 2. Define state\nclass QAState(TypedDict):\n    question: str\n    reasoning: str\n    answer: str\n\n# 3. Create DSPy module\n@ma.dspy_node\nclass QAAgent(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# 4. Build LangGraph workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QAAgent())\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\"question\": \"What are the benefits of using DSPy?\"})\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Reasoning: {result['reasoning']}\")\n# \u2705 Automatically traced in Langfuse!\n</code></pre>"},{"location":"building-blocks/dspy/overview/#when-to-use-dspy","title":"When to Use DSPy","text":""},{"location":"building-blocks/dspy/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Complex reasoning tasks requiring chain-of-thought</li> <li>Multi-step pipelines with intermediate outputs</li> <li>Optimizable systems where you can measure success</li> <li>Model-agnostic applications that need portability</li> </ul>"},{"location":"building-blocks/dspy/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot prompts (just use the LLM API directly)</li> <li>When you need exact prompt control (DSPy generates prompts)</li> <li>Streaming responses with partial updates (DSPy is batch-oriented)</li> </ul>"},{"location":"building-blocks/dspy/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/overview/#1-sequential-processing","title":"1. Sequential Processing","text":"<pre><code>class Pipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = dspy.ChainOfThought(\"input -&gt; intermediate\")\n        self.step2 = dspy.ChainOfThought(\"intermediate -&gt; output\")\n\n    def forward(self, input):\n        intermediate = self.step1(input=input)\n        return self.step2(intermediate=intermediate.intermediate)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#2-conditional-logic","title":"2. Conditional Logic","text":"<pre><code>class ConditionalAgent(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.classifier = dspy.Predict(\"question -&gt; category\")\n        self.tech_expert = dspy.ChainOfThought(\"question -&gt; answer\")\n        self.general_expert = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        category = self.classifier(question=question).category\n\n        if \"technical\" in category.lower():\n            return self.tech_expert(question=question)\n        else:\n            return self.general_expert(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#3-tool-use-with-react","title":"3. Tool Use with ReAct","text":"<pre><code>class ToolUser(dspy.Module):\n    def __init__(self, tools):\n        super().__init__()\n        self.react = dspy.ReAct(\"question -&gt; answer\")\n        self.react.tools = tools\n\n    def forward(self, question):\n        return self.react(question=question)\n</code></pre>"},{"location":"building-blocks/dspy/overview/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Signatures \u2192 Learn how to define inputs and outputs</li> <li>DSPy Modules \u2192 Explore built-in modules like ChainOfThought, ReAct</li> <li>DSPy Optimizers \u2192 Automatically improve your prompts</li> <li>Best Practices \u2192 Tips for production DSPy code</li> </ul>"},{"location":"building-blocks/dspy/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official DSPy Docs - Comprehensive DSPy documentation</li> <li>DSPy GitHub - Source code and examples</li> <li>DSPy Paper - Research paper explaining DSPy</li> </ul> <p>Ready to dive deeper? Start with Signatures \u2192</p>"},{"location":"building-blocks/dspy/signatures/","title":"DSPy Signatures","text":"<p>TL;DR: Signatures are type specifications that tell DSPy what inputs your module needs and what outputs it should produce.</p>"},{"location":"building-blocks/dspy/signatures/#what-is-a-signature","title":"What is a Signature?","text":"<p>A signature in DSPy is like a function type hint\u2014it specifies: - What inputs the module receives - What outputs it should produce - Optional descriptions for each field</p> <p>Think of it as a contract between your code and the LLM.</p>"},{"location":"building-blocks/dspy/signatures/#basic-syntax","title":"Basic Syntax","text":""},{"location":"building-blocks/dspy/signatures/#string-signatures","title":"String Signatures","text":"<p>The simplest way to define a signature:</p> <pre><code>import dspy\n\n# Single input \u2192 single output\n\"question -&gt; answer\"\n\n# Multiple inputs \u2192 single output\n\"question, context -&gt; answer\"\n\n# Multiple inputs \u2192 multiple outputs\n\"question, context -&gt; answer, confidence\"\n</code></pre> <p>Format: <code>input1, input2 -&gt; output1, output2</code></p>"},{"location":"building-blocks/dspy/signatures/#example","title":"Example","text":"<pre><code># Create a predictor with a signature\nqa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n# Use it\nresult = qa(question=\"What is DSPy?\")\nprint(result.answer)  # Access output by name\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#adding-descriptions","title":"Adding Descriptions","text":"<p>You can add hints to guide the LLM:</p> <pre><code># Add output description\n\"question -&gt; answer: a concise, factual response\"\n\n# Add input descriptions\n\"question: a technical question -&gt; answer: a detailed explanation\"\n\n# Multiple fields with descriptions\n\"question: user query, context: relevant docs -&gt; answer: synthesized response, sources: list of citations\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#example-with-descriptions","title":"Example with Descriptions","text":"<pre><code>predictor = dspy.ChainOfThought(\n    \"question: a user's question about AI -&gt; answer: a detailed, technical explanation\"\n)\n\nresult = predictor(question=\"How does attention work in transformers?\")\nprint(result.answer)\n# Output will be more detailed and technical due to the hint\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#class-based-signatures","title":"Class-Based Signatures","text":"<p>For complex signatures, use classes:</p> <pre><code>import dspy\n\nclass QASignature(dspy.Signature):\n    \"\"\"Answer questions with detailed explanations.\"\"\"\n\n    question = dspy.InputField(desc=\"The user's question\")\n    context = dspy.InputField(desc=\"Relevant background information\")\n    answer = dspy.OutputField(desc=\"A comprehensive answer\")\n    confidence = dspy.OutputField(desc=\"Confidence score (0-100)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#using-class-signatures","title":"Using Class Signatures","text":"<pre><code># Pass the class (not an instance!)\npredictor = dspy.ChainOfThought(QASignature)\n\nresult = predictor(\n    question=\"What is DSPy?\",\n    context=\"DSPy is a framework for prompt programming...\"\n)\nprint(result.answer)\nprint(result.confidence)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#input-and-output-fields","title":"Input and Output Fields","text":""},{"location":"building-blocks/dspy/signatures/#inputfield","title":"InputField","text":"<p>Defines what the module receives:</p> <pre><code>import dspy\n\nclass MySignature(dspy.Signature):\n    # Basic input\n    query = dspy.InputField()\n\n    # With description\n    context = dspy.InputField(desc=\"Background information\")\n\n    # With format hint\n    examples = dspy.InputField(desc=\"Few-shot examples\", format=list)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#outputfield","title":"OutputField","text":"<p>Defines what the module produces:</p> <pre><code>class MySignature(dspy.Signature):\n    # Basic output\n    answer = dspy.OutputField()\n\n    # With description\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n\n    # With prefix (shown before the output in the prompt)\n    summary = dspy.OutputField(prefix=\"SUMMARY:\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/dspy/signatures/#1-simple-qa","title":"1. Simple Q&amp;A","text":"<pre><code>\"question -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#2-context-aware-qa","title":"2. Context-Aware Q&amp;A","text":"<pre><code>\"question, context -&gt; answer\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#3-multi-output","title":"3. Multi-Output","text":"<pre><code>\"document -&gt; summary, key_points, sentiment\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#4-complex-reasoning","title":"4. Complex Reasoning","text":"<pre><code>class ReasoningSignature(dspy.Signature):\n    \"\"\"Solve complex problems with step-by-step reasoning.\"\"\"\n\n    problem = dspy.InputField(desc=\"The problem to solve\")\n    constraints = dspy.InputField(desc=\"Any constraints or requirements\")\n\n    reasoning = dspy.OutputField(desc=\"Step-by-step thought process\")\n    solution = dspy.OutputField(desc=\"The final solution\")\n    confidence = dspy.OutputField(desc=\"Confidence level (low/medium/high)\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#5-classification","title":"5. Classification","text":"<pre><code>class ClassificationSignature(dspy.Signature):\n    \"\"\"Classify text into categories.\"\"\"\n\n    text = dspy.InputField(desc=\"The text to classify\")\n    categories = dspy.InputField(desc=\"Valid categories (comma-separated)\")\n\n    category = dspy.OutputField(desc=\"The chosen category\")\n    reason = dspy.OutputField(desc=\"Brief explanation for the choice\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#signatures-in-mahsm","title":"Signatures in mahsm","text":"<p>When using <code>@dspy_node</code>, signatures determine how inputs are extracted from state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\n# 1. Define state\nclass ResearchState(TypedDict):\n    question: str\n    context: str\n    answer: str\n    reasoning: str\n\n# 2. Create module with signature\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        # Signature matches state fields\n        self.research = dspy.ChainOfThought(\"question, context -&gt; answer, reasoning\")\n\n    def forward(self, question, context):\n        return self.research(question=question, context=context)\n\n# 3. Use in workflow\nworkflow = ma.graph.StateGraph(ResearchState)\nworkflow.add_node(\"researcher\", Researcher())\n\n# When the node runs:\n# - \"question\" and \"context\" are extracted from state\n# - \"answer\" and \"reasoning\" are written back to state\n</code></pre> <p>Key Point: Match your signature field names to your state keys for seamless integration!</p>"},{"location":"building-blocks/dspy/signatures/#advanced-dynamic-signatures","title":"Advanced: Dynamic Signatures","text":"<p>Create signatures programmatically:</p> <pre><code>def create_signature(input_fields, output_fields):\n    inputs = \", \".join(input_fields)\n    outputs = \", \".join(output_fields)\n    return f\"{inputs} -&gt; {outputs}\"\n\n# Example: Dynamic fields\nsig = create_signature([\"question\", \"context\"], [\"answer\", \"score\"])\n# Result: \"question, context -&gt; answer, score\"\n\npredictor = dspy.Predict(sig)\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/dspy/signatures/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use descriptive field names <pre><code>\"user_question -&gt; detailed_answer\"  # \u2705 Clear\n</code></pre></p> </li> <li> <p>Add descriptions for ambiguous fields <pre><code>\"query: the user's search query -&gt; results: list of relevant items\"\n</code></pre></p> </li> <li> <p>Match state keys in mahsm <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# Signature matches state\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> <li> <p>Use multi-output for intermediate reasoning <pre><code>\"question -&gt; reasoning, answer\"  # \u2705 Captures thought process\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Use vague names <pre><code>\"input -&gt; output\"  # \u274c Not descriptive\n</code></pre></p> </li> <li> <p>Mix concerns in one field <pre><code>\"query -&gt; answer_and_confidence\"  # \u274c Split into two outputs\n</code></pre></p> </li> <li> <p>Over-complicate <pre><code># \u274c Too many fields\n\"q, c1, c2, c3, c4 -&gt; a, r1, r2, r3, conf, meta\"\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/dspy/signatures/#troubleshooting","title":"Troubleshooting","text":""},{"location":"building-blocks/dspy/signatures/#issue-llm-not-returning-expected-output","title":"Issue: LLM not returning expected output","text":"<p>Solution: Add more specific descriptions</p> <pre><code># Before (vague)\n\"text -&gt; category\"\n\n# After (specific)\n\"text: a customer review -&gt; category: one of [positive, negative, neutral]\"\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-output-format-is-inconsistent","title":"Issue: Output format is inconsistent","text":"<p>Solution: Use structured output hints</p> <pre><code>class StructuredSignature(dspy.Signature):\n    query = dspy.InputField()\n    answer = dspy.OutputField(desc=\"Answer in JSON format with keys: summary, details\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#issue-state-keys-dont-match-signature","title":"Issue: State keys don't match signature","text":"<p>Solution: Ensure field names align</p> <pre><code># State has \"user_query\"\nclass State(TypedDict):\n    user_query: str\n\n# \u274c Signature uses \"question\"\ndspy.ChainOfThought(\"question -&gt; answer\")  # Won't find \"question\" in state!\n\n# \u2705 Match the key\ndspy.ChainOfThought(\"user_query -&gt; answer\")\n</code></pre>"},{"location":"building-blocks/dspy/signatures/#next-steps","title":"Next Steps","text":"<ul> <li>DSPy Modules \u2192 Learn about Predict, ChainOfThought, ReAct</li> <li>Your First Agent \u2192 Build a complete agent</li> <li>API Reference: @dspy_node \u2192 mahsm integration details</li> </ul>"},{"location":"building-blocks/dspy/signatures/#external-resources","title":"External Resources","text":"<ul> <li>DSPy Signatures Documentation - Official guide</li> <li>DSPy Examples - Real-world signature usage</li> </ul> <p>Next: Explore DSPy Modules \u2192</p>"},{"location":"building-blocks/langgraph/compilation/","title":"LangGraph Compilation &amp; Execution","text":"<p>TL;DR: Compile your workflow into an executable graph, then run it with <code>invoke()</code> or <code>stream()</code>.</p>"},{"location":"building-blocks/langgraph/compilation/#workflow-lifecycle","title":"Workflow Lifecycle","text":"<p>Building and running a LangGraph workflow has three stages:</p> <ol> <li>Build: Create workflow with nodes and edges</li> <li>Compile: Convert to executable graph</li> <li>Execute: Run with input state</li> </ol> <pre><code>import mahsm as ma\n\n# 1. Build\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"step1\", step1_func)\nworkflow.add_edge(ma.START, \"step1\")\nworkflow.add_edge(\"step1\", ma.END)\n\n# 2. Compile\ngraph = workflow.compile()\n\n# 3. Execute\nresult = graph.invoke({\"input\": \"Hello\"})\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#compilation","title":"Compilation","text":""},{"location":"building-blocks/langgraph/compilation/#basic-compilation","title":"Basic Compilation","text":"<p>Call <code>compile()</code> on your workflow:</p> <pre><code>graph = workflow.compile()\n</code></pre> <p>What happens during compilation: - Validates workflow structure - Checks for unreachable nodes - Optimizes execution order - Creates execution engine</p>"},{"location":"building-blocks/langgraph/compilation/#compilation-errors","title":"Compilation Errors","text":"<p>Common issues:</p> <pre><code># \u274c Missing START edge\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"node1\", func1)\n# ERROR: No entry point!\n\n# \u2705 Fixed\nworkflow.add_edge(ma.START, \"node1\")\n\n# \u274c Unreachable node\nworkflow.add_edge(ma.START, \"node1\")\nworkflow.add_edge(\"node1\", ma.END)\nworkflow.add_node(\"orphan\", orphan_func)  # Never reached!\n# WARNING: Node \"orphan\" is unreachable\n\n# \u274c No exit\nworkflow.add_edge(ma.START, \"loop1\")\nworkflow.add_edge(\"loop1\", \"loop2\")\nworkflow.add_edge(\"loop2\", \"loop1\")  # Infinite loop!\n# ERROR: No path to END\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#best-practices","title":"Best Practices","text":"<p>\u2705 Compile once, reuse many times</p> <pre><code># \u2705 Good: Compile once\ngraph = workflow.compile()\nfor input in inputs:\n    result = graph.invoke(input)\n\n# \u274c Bad: Recompiling every time\nfor input in inputs:\n    graph = workflow.compile()  # Wasteful!\n    result = graph.invoke(input)\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#execution-methods","title":"Execution Methods","text":""},{"location":"building-blocks/langgraph/compilation/#invoke-synchronous-execution","title":"invoke() - Synchronous Execution","text":"<p>Run workflow and get final state:</p> <pre><code>result = graph.invoke({\"question\": \"What is Python?\"})\nprint(result)  # Final state\n</code></pre> <p>Characteristics: - Blocks until complete - Returns final state only - Simple and straightforward</p>"},{"location":"building-blocks/langgraph/compilation/#stream-streaming-execution","title":"stream() - Streaming Execution","text":"<p>Stream state updates as they happen:</p> <pre><code>for state_update in graph.stream({\"question\": \"What is Python?\"}):\n    print(f\"Update: {state_update}\")\n</code></pre> <p>Characteristics: - Yields state after each node - Great for progress tracking - See intermediate results</p>"},{"location":"building-blocks/langgraph/compilation/#example-stream-vs-invoke","title":"Example: Stream vs Invoke","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    count: int\n\ndef increment(state):\n    print(f\"  Node: Incrementing {state['count']}\")\n    return {\"count\": state[\"count\"] + 1}\n\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"inc1\", increment)\nworkflow.add_node(\"inc2\", increment)\nworkflow.add_node(\"inc3\", increment)\nworkflow.add_edge(ma.START, \"inc1\")\nworkflow.add_edge(\"inc1\", \"inc2\")\nworkflow.add_edge(\"inc2\", \"inc3\")\nworkflow.add_edge(\"inc3\", ma.END)\n\ngraph = workflow.compile()\n\nprint(\"=== With invoke() ===\")\nresult = graph.invoke({\"count\": 0})\nprint(f\"Final result: {result}\")\n\nprint(\"\\n=== With stream() ===\")\nfor state in graph.stream({\"count\": 0}):\n    print(f\"Intermediate: {state}\")\n</code></pre> <p>Output: <pre><code>=== With invoke() ===\n  Node: Incrementing 0\n  Node: Incrementing 1\n  Node: Incrementing 2\nFinal result: {'count': 3}\n\n=== With stream() ===\n  Node: Incrementing 0\nIntermediate: {'count': 1}\n  Node: Incrementing 1\nIntermediate: {'count': 2}\n  Node: Incrementing 2\nIntermediate: {'count': 3}\n</code></pre></p>"},{"location":"building-blocks/langgraph/compilation/#input-output","title":"Input &amp; Output","text":""},{"location":"building-blocks/langgraph/compilation/#input-format","title":"Input Format","text":"<p>Pass initial state as a dict:</p> <pre><code>result = graph.invoke({\n    \"question\": \"What is Python?\",\n    \"context\": \"Programming\",\n    \"max_length\": 100\n})\n</code></pre> <p>Rules: - Must be a dictionary - Keys must match state TypedDict fields - Optional fields can be omitted</p>"},{"location":"building-blocks/langgraph/compilation/#output-format","title":"Output Format","text":"<p>Returns final state as a dict:</p> <pre><code>result = graph.invoke({\"input\": \"Hello\"})\n# result = {\"input\": \"Hello\", \"output\": \"World\", ...}\n\n# Access fields\nprint(result[\"output\"])\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#partial-inputs","title":"Partial Inputs","text":"<p>You don't need to provide all state fields:</p> <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n    confidence: float\n    metadata: dict\n\n# Only provide required fields\nresult = graph.invoke({\"question\": \"What is AI?\"})\n# Other fields are added by nodes\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#execution-control","title":"Execution Control","text":""},{"location":"building-blocks/langgraph/compilation/#setting-configuration","title":"Setting Configuration","text":"<p>Configure execution behavior:</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\n# Add checkpointing\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\n# Run with config\nresult = graph.invoke(\n    {\"question\": \"Hello\"},\n    config={\"configurable\": {\"thread_id\": \"conversation-1\"}}\n)\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#interrupting-execution","title":"Interrupting Execution","text":"<p>For long-running workflows:</p> <pre><code>import asyncio\n\n# Async version\nasync def run_with_timeout():\n    try:\n        result = await asyncio.wait_for(\n            graph.ainvoke({\"input\": \"data\"}),\n            timeout=30.0\n        )\n        return result\n    except asyncio.TimeoutError:\n        print(\"Workflow timed out!\")\n        return None\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#complete-example","title":"Complete Example","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# State\nclass PipelineState(TypedDict):\n    question: str\n    category: Optional[str]\n    answer: Optional[str]\n    sources: Optional[list]\n\n# Nodes\ndef categorize(state: PipelineState) -&gt; dict:\n    question = state[\"question\"]\n    if \"code\" in question.lower():\n        return {\"category\": \"programming\"}\n    elif \"math\" in question.lower():\n        return {\"category\": \"mathematics\"}\n    else:\n        return {\"category\": \"general\"}\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question, category -&gt; answer\")\n\n    def forward(self, question, category):\n        return self.qa(question=question, category=category)\n\ndef add_sources(state: PipelineState) -&gt; dict:\n    # Mock sources\n    return {\"sources\": [\"Source 1\", \"Source 2\"]}\n\n# Router\ndef route_by_category(state: PipelineState):\n    \"\"\"Skip sources for simple questions.\"\"\"\n    if state[\"category\"] == \"general\":\n        return ma.END\n    return \"add_sources\"\n\n# Build workflow\nworkflow = ma.graph.StateGraph(PipelineState)\nworkflow.add_node(\"categorize\", categorize)\nworkflow.add_node(\"qa\", QA())\nworkflow.add_node(\"add_sources\", add_sources)\n\nworkflow.add_edge(ma.START, \"categorize\")\nworkflow.add_edge(\"categorize\", \"qa\")\nworkflow.add_conditional_edges(\"qa\", route_by_category)\nworkflow.add_edge(\"add_sources\", ma.END)\n\n# Compile once\ngraph = workflow.compile()\n\n# Execute multiple times\nprint(\"=== Example 1: General Question ===\")\nresult1 = graph.invoke({\"question\": \"What is Python?\"})\nprint(f\"Category: {result1['category']}\")\nprint(f\"Answer: {result1['answer']}\")\nprint(f\"Sources: {result1.get('sources', 'None')}\")\n\nprint(\"\\n=== Example 2: Code Question ===\")\nresult2 = graph.invoke({\"question\": \"How do I write a Python loop?\"})\nprint(f\"Category: {result2['category']}\")\nprint(f\"Answer: {result2['answer']}\")\nprint(f\"Sources: {result2.get('sources', 'None')}\")\n\nprint(\"\\n=== Example 3: Streaming ===\")\nfor state in graph.stream({\"question\": \"What is 2+2?\"}):\n    print(f\"State update: category={state.get('category')}, \"\n          f\"has_answer={bool(state.get('answer'))}\")\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#error-handling","title":"Error Handling","text":""},{"location":"building-blocks/langgraph/compilation/#handling-node-errors","title":"Handling Node Errors","text":"<p>Wrap nodes in try/except:</p> <pre><code>def safe_node(state):\n    try:\n        result = risky_operation(state)\n        return {\"result\": result, \"error\": None}\n    except Exception as e:\n        return {\"result\": None, \"error\": str(e)}\n\n# Route based on error\ndef error_router(state):\n    if state.get(\"error\"):\n        return \"error_handler\"\n    return \"next_step\"\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#validation","title":"Validation","text":"<p>Validate state before execution:</p> <pre><code>def validate_input(state):\n    \"\"\"Validate input before processing.\"\"\"\n    if not state.get(\"question\"):\n        raise ValueError(\"Question is required\")\n\n    if len(state[\"question\"]) &gt; 1000:\n        raise ValueError(\"Question too long\")\n\n    return {}\n\n# Add as first node\nworkflow.add_edge(ma.START, \"validate\")\nworkflow.add_node(\"validate\", validate_input)\nworkflow.add_edge(\"validate\", \"main_flow\")\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#performance-optimization","title":"Performance Optimization","text":""},{"location":"building-blocks/langgraph/compilation/#reuse-compiled-graphs","title":"Reuse Compiled Graphs","text":"<pre><code># \u2705 Good: Global compiled graph\nGRAPH = workflow.compile()\n\ndef process_request(input_data):\n    return GRAPH.invoke(input_data)\n\n# \u274c Bad: Recompile each time\ndef process_request(input_data):\n    graph = workflow.compile()  # Slow!\n    return graph.invoke(input_data)\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#batch-processing","title":"Batch Processing","text":"<p>Process multiple inputs efficiently:</p> <pre><code>inputs = [\n    {\"question\": \"Q1\"},\n    {\"question\": \"Q2\"},\n    {\"question\": \"Q3\"}\n]\n\n# Sequential\nresults = [graph.invoke(inp) for inp in inputs]\n\n# Parallel (if graph is thread-safe)\nfrom concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(graph.invoke, inputs))\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#caching","title":"Caching","text":"<p>Cache compiled graphs:</p> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_graph():\n    workflow = build_workflow()  # Your build logic\n    return workflow.compile()\n\n# Always returns same compiled graph\ngraph = get_graph()\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#debugging-execution","title":"Debugging Execution","text":""},{"location":"building-blocks/langgraph/compilation/#print-state-at-each-node","title":"Print State at Each Node","text":"<pre><code>def debug_node(state):\n    import json\n    print(f\"=== State at debug point ===\")\n    print(json.dumps(state, indent=2))\n    return {}\n\nworkflow.add_node(\"debug\", debug_node)\nworkflow.add_edge(\"some_node\", \"debug\")\nworkflow.add_edge(\"debug\", \"next_node\")\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#use-langfuse-tracing","title":"Use Langfuse Tracing","text":"<pre><code>ma.tracing.init()\n\n# All execution is automatically traced!\nresult = graph.invoke({\"input\": \"data\"})\n\n# View in Langfuse UI:\n# - Full execution trace\n# - State at each node\n# - LLM calls\n# - Timing information\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#stream-for-visibility","title":"Stream for Visibility","text":"<pre><code>for state in graph.stream({\"input\": \"data\"}):\n    print(f\"After node: {state}\")\n    if \"error\" in state:\n        print(f\"ERROR: {state['error']}\")\n        break\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#testing-workflows","title":"Testing Workflows","text":""},{"location":"building-blocks/langgraph/compilation/#unit-testing-nodes","title":"Unit Testing Nodes","text":"<p>Test individual nodes:</p> <pre><code>import unittest\n\nclass TestNodes(unittest.TestCase):\n    def test_categorize(self):\n        state = {\"question\": \"What is code?\"}\n        result = categorize(state)\n        self.assertEqual(result[\"category\"], \"programming\")\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#integration-testing","title":"Integration Testing","text":"<p>Test full workflow:</p> <pre><code>def test_workflow():\n    graph = workflow.compile()\n\n    result = graph.invoke({\"question\": \"Test\"})\n\n    assert \"answer\" in result\n    assert result[\"answer\"] is not None\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#property-testing","title":"Property Testing","text":"<p>Test workflow properties:</p> <pre><code>def test_workflow_always_produces_answer():\n    graph = workflow.compile()\n\n    test_questions = [\n        \"What is AI?\",\n        \"How does Python work?\",\n        \"Explain quantum physics\"\n    ]\n\n    for question in test_questions:\n        result = graph.invoke({\"question\": question})\n        assert result[\"answer\"], f\"No answer for: {question}\"\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#async-execution","title":"Async Execution","text":"<p>For async operations:</p> <pre><code>import asyncio\n\n# Use async nodes\nasync def async_node(state):\n    result = await async_api_call(state[\"input\"])\n    return {\"result\": result}\n\n# Use ainvoke\nresult = await graph.ainvoke({\"input\": \"data\"})\n\n# Use astream\nasync for state in graph.astream({\"input\": \"data\"}):\n    print(state)\n</code></pre>"},{"location":"building-blocks/langgraph/compilation/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"building-blocks/langgraph/compilation/#do","title":"\u2705 Do:","text":"<ol> <li>Compile once, execute many</li> <li>Use streaming for long workflows</li> <li>Handle errors gracefully</li> <li>Validate inputs</li> <li>Cache compiled graphs</li> <li>Use tracing for debugging</li> </ol>"},{"location":"building-blocks/langgraph/compilation/#dont","title":"\u274c Don't:","text":"<ol> <li>Recompile unnecessarily</li> <li>Ignore errors</li> <li>Mutate state directly</li> <li>Block without timeout</li> <li>Skip input validation</li> </ol>"},{"location":"building-blocks/langgraph/compilation/#next-steps","title":"Next Steps","text":"<ul> <li>Visualization \u2192 Visualize your workflows</li> <li>LangGraph Overview \u2192 Review core concepts</li> <li>Best Practices \u2192 Production patterns</li> </ul>"},{"location":"building-blocks/langgraph/compilation/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph Execution - Official how-tos</li> <li>LangGraph API - API reference</li> </ul> <p>Next: Visualize workflows with Visualization \u2192</p>"},{"location":"building-blocks/langgraph/conditional-routing/","title":"LangGraph Conditional Routing","text":"<p>TL;DR: Conditional routing dynamically chooses the next node based on state\u2014enabling branching, loops, and adaptive workflows.</p>"},{"location":"building-blocks/langgraph/conditional-routing/#what-is-conditional-routing","title":"What is Conditional Routing?","text":"<p>Conditional routing allows your workflow to make decisions based on the current state. Instead of a fixed path, your agent can:</p> <ul> <li>Branch: Choose between multiple paths</li> <li>Loop: Repeat steps until a condition is met</li> <li>Exit: End early based on criteria</li> <li>Adapt: Change behavior dynamically</li> </ul> <p>Think of it as if/else logic for workflows.</p>"},{"location":"building-blocks/langgraph/conditional-routing/#basic-conditional-edges","title":"Basic Conditional Edges","text":"<p>Use <code>add_conditional_edges()</code> to route based on state:</p> <pre><code>def router_function(state):\n    \"\"\"Decide which node to go to next.\"\"\"\n    if state[\"condition\"]:\n        return \"node_a\"\n    else:\n        return \"node_b\"\n\nworkflow.add_conditional_edges(\"source_node\", router_function)\n</code></pre> <p>How it works: 1. <code>source_node</code> executes and updates state 2. <code>router_function</code> receives the updated state 3. Returns the name of the next node to execute 4. LangGraph routes to that node</p>"},{"location":"building-blocks/langgraph/conditional-routing/#routing-patterns","title":"Routing Patterns","text":""},{"location":"building-blocks/langgraph/conditional-routing/#1-binary-branch","title":"1. Binary Branch","text":"<p>Choose between two paths:</p> <pre><code>def binary_router(state):\n    \"\"\"Simple yes/no routing.\"\"\"\n    if state[\"needs_research\"]:\n        return \"research_agent\"\n    else:\n        return \"direct_answer\"\n\nworkflow.add_conditional_edges(\"classifier\", binary_router)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#2-multi-way-branch","title":"2. Multi-Way Branch","text":"<p>Choose from multiple paths:</p> <pre><code>def multi_router(state):\n    \"\"\"Route to different specialists.\"\"\"\n    category = state[\"category\"]\n\n    if category == \"math\":\n        return \"calculator\"\n    elif category == \"research\":\n        return \"researcher\"\n    elif category == \"code\":\n        return \"code_generator\"\n    else:\n        return \"general_qa\"\n\nworkflow.add_conditional_edges(\"categorizer\", multi_router)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#3-retry-loop","title":"3. Retry Loop","text":"<p>Loop until success:</p> <pre><code>def retry_logic(state):\n    \"\"\"Retry if not good enough.\"\"\"\n    # Check if we've tried too many times\n    if state.get(\"attempts\", 0) &gt;= 3:\n        return ma.END  # Give up\n\n    # Check quality\n    if state.get(\"quality_score\", 0) &lt; 0.8:\n        return \"generate\"  # Try again\n\n    return ma.END  # Success!\n\nworkflow.add_conditional_edges(\"quality_check\", retry_logic)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#4-early-exit","title":"4. Early Exit","text":"<p>Skip remaining steps if done:</p> <pre><code>def early_exit(state):\n    \"\"\"Exit early if we have an answer.\"\"\"\n    if state.get(\"answer\") and state.get(\"confidence\", 0) &gt; 0.9:\n        return ma.END  # Skip remaining steps\n\n    return \"next_step\"  # Continue processing\n\nworkflow.add_conditional_edges(\"initial_check\", early_exit)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#5-dynamic-path-selection","title":"5. Dynamic Path Selection","text":"<p>Choose path based on multiple factors:</p> <pre><code>def smart_router(state):\n    \\\"\\\"\\\"Route based on complexity and urgency.\\\"\\\"\\\"\n    complexity = len(state[\"question\"].split())\n    urgent = state.get(\"urgent\", False)\n\n    if urgent and complexity &lt; 10:\n        return \"fast_qa\"  # Quick answer\n    elif complexity &gt; 50:\n        return \"detailed_research\"  # Deep dive\n    elif state.get(\"has_code\"):\n        return \"code_analyzer\"  # Code-specific\n    else:\n        return \"standard_qa\"  # Normal processing\n\nworkflow.add_conditional_edges(\"triage\", smart_router)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"building-blocks/langgraph/conditional-routing/#conditional-looping-with-state-tracking","title":"Conditional Looping with State Tracking","text":"<p>Loop with iteration counter:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    question: str\n    answer: Optional[str]\n    iteration: int\n    max_iterations: int\n\ndef increment_counter(state):\n    \\\"\\\"\\\"Increment iteration count.\\\"\\\"\\\"\n    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n\ndef should_continue(state):\n    \\\"\\\"\\\"Continue if not done and under max iterations.\\\"\\\"\\\"\n    if state.get(\"iteration\", 0) &gt;= state.get(\"max_iterations\", 5):\n        return ma.END\n\n    if state.get(\"answer_quality\", 0) &gt;= 0.8:\n        return ma.END\n\n    return \"generate_answer\"\n\n# Build loop\nworkflow.add_node(\"increment\", increment_counter)\nworkflow.add_node(\"generate_answer\", generate_answer)\nworkflow.add_node(\"check_quality\", check_quality)\n\nworkflow.add_edge(ma.START, \"increment\")\nworkflow.add_edge(\"increment\", \"generate_answer\")\nworkflow.add_edge(\"generate_answer\", \"check_quality\")\nworkflow.add_conditional_edges(\"check_quality\", should_continue)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#fallback-chains","title":"Fallback Chains","text":"<p>Try multiple strategies until one works:</p> <pre><code>def fallback_router(state):\n    \\\"\\\"\\\"Try progressively more expensive strategies.\\\"\\\"\\\"\n    attempt = state.get(\"attempt\", 1)\n\n    if attempt == 1:\n        return \"cheap_model\"\n    elif attempt == 2:\n        return \"medium_model\"\n    elif attempt == 3:\n        return \"expensive_model\"\n    else:\n        return ma.END  # All strategies failed\n\ndef check_success(state):\n    \\\"\\\"\\\"Route based on success.\\\"\\\"\\\"\n    if state.get(\"success\"):\n        return ma.END\n    else:\n        # Increment attempt and try next strategy\n        return \"increment_attempt\"\n\nworkflow.add_conditional_edges(\"evaluator\", check_success)\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#state-machine-routing","title":"State-Machine Routing","text":"<p>Implement a full state machine:</p> <pre><code>def state_machine_router(state):\n    \\\"\\\"\\\"Route based on current phase.\\\"\\\"\\\"\n    phase = state.get(\"phase\", \"init\")\n\n    if phase == \"init\":\n        return \"initialize\"\n    elif phase == \"collect\":\n        return \"collect_data\"\n    elif phase == \"process\":\n        return \"process_data\"\n    elif phase == \"validate\":\n        return \"validate_results\"\n    elif phase == \"done\":\n        return ma.END\n    else:\n        # Unknown phase, go to error handler\n        return \"error_handler\"\n\n# Each node updates the phase\ndef initialize(state):\n    # ... initialization logic\n    return {\"phase\": \"collect\"}\n\ndef collect_data(state):\n    # ... collection logic\n    return {\"phase\": \"process\"}\n\n# etc.\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#router-functions-in-detail","title":"Router Functions in Detail","text":""},{"location":"building-blocks/langgraph/conditional-routing/#return-values","title":"Return Values","text":"<p>Router functions must return:</p> <ol> <li> <p>Node name (string): Go to that node    <pre><code>return \"next_node\"\n</code></pre></p> </li> <li> <p>END: Terminate workflow    <pre><code>return ma.END\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/conditional-routing/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/conditional-routing/#keep-routers-pure","title":"\u2705 Keep Routers Pure","text":"<pre><code># \u2705 Good: Pure function\ndef router(state):\n    return \"next\" if state[\"value\"] &gt; 0 else \"other\"\n\n# \u274c Bad: Side effects\ndef router(state):\n    global counter\n    counter += 1  # Side effect!\n    return \"next\"\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#handle-missing-keys","title":"\u2705 Handle Missing Keys","text":"<pre><code># \u2705 Good: Safe access\ndef router(state):\n    value = state.get(\"key\", default_value)\n    if value &gt; threshold:\n        return \"high_path\"\n    return \"low_path\"\n\n# \u274c Bad: Can crash\ndef router(state):\n    if state[\"key\"] &gt; threshold:  # KeyError if missing!\n        return \"high_path\"\n    return \"low_path\"\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#use-type-hints","title":"\u2705 Use Type Hints","text":"<pre><code># \u2705 Good: Clear types\ndef router(state: MyState) -&gt; str:\n    \\\"\\\"\\\"\n    Route based on category.\n\n    Returns:\n        Name of next node\n    \\\"\\\"\\\"\n    return \"next_node\"\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#complete-example-self-improving-agent","title":"Complete Example: Self-Improving Agent","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# State\nclass AgentState(TypedDict):\n    question: str\n    attempt: int\n    strategy: str\n    answer: Optional[str]\n    quality_score: Optional[float]\n    done: bool\n\n# Nodes\n@ma.dspy_node\nclass SimpleQA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass ComplexQA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityChecker(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.checker = dspy.Predict(\"question, answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, question, answer):\n        result = self.checker(question=question, answer=answer)\n        return {\n            \"quality_score\": float(result.quality_score),\n            \"done\": float(result.quality_score) &gt;= 0.7\n        }\n\ndef increment_attempt(state: AgentState) -&gt; dict:\n    return {\"attempt\": state.get(\"attempt\", 0) + 1}\n\n# Router functions\ndef strategy_selector(state: AgentState) -&gt; str:\n    \\\"\\\"\\\"Select strategy based on attempt.\\\"\\\"\\\"\n    attempt = state.get(\"attempt\", 1)\n\n    if attempt == 1:\n        return \"simple_qa\"\n    elif attempt &gt;= 2:\n        return \"complex_qa\"\n    else:\n        return ma.END\n\ndef should_retry(state: AgentState) -&gt; str:\n    \\\"\\\"\\\"Decide if we should retry.\\\"\\\"\\\"\n    # Check if done\n    if state.get(\"done\", False):\n        return ma.END\n\n    # Check max attempts\n    if state.get(\"attempt\", 0) &gt;= 3:\n        return ma.END  # Give up\n\n    # Retry with different strategy\n    return \"increment\"\n\n# Build workflow\nworkflow = ma.graph.StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"increment\", increment_attempt)\nworkflow.add_node(\"simple_qa\", SimpleQA())\nworkflow.add_node(\"complex_qa\", ComplexQA())\nworkflow.add_node(\"quality_check\", QualityChecker())\n\n# Build flow with conditional routing\nworkflow.add_edge(ma.START, \"increment\")\nworkflow.add_conditional_edges(\"increment\", strategy_selector)\nworkflow.add_edge(\"simple_qa\", \"quality_check\")\nworkflow.add_edge(\"complex_qa\", \"quality_check\")\nworkflow.add_conditional_edges(\"quality_check\", should_retry)\n\n# Compile\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\n    \"question\": \"Explain quantum entanglement simply.\",\n    \"attempt\": 0,\n    \"done\": False\n})\n\nprint(f\"Final answer: {result['answer']}\")\nprint(f\"Quality: {result['quality_score']}\")\nprint(f\"Attempts: {result['attempt']}\")\nprint(f\"Strategy: Simple QA\\\" if result['attempt'] == 1 else \\\"Complex QA\\\")\")\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#common-routing-scenarios","title":"Common Routing Scenarios","text":""},{"location":"building-blocks/langgraph/conditional-routing/#1-quality-gate","title":"1. Quality Gate","text":"<p>Only proceed if quality threshold is met:</p> <pre><code>def quality_gate(state):\n    quality = state.get(\"quality_score\", 0)\n\n    if quality &gt;= 0.9:\n        return \"finalize\"  # High quality, finish\n    elif quality &gt;= 0.7:\n        return \"polish\"  # Good, needs polish\n    else:\n        return \"regenerate\"  # Low quality, try again\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#2-resource-based-routing","title":"2. Resource-Based Routing","text":"<p>Route based on available resources:</p> <pre><code>def resource_router(state):\n    budget_remaining = state.get(\"budget\", 0)\n\n    if budget_remaining &gt; 100:\n        return \"expensive_model\"  # Can afford best\n    elif budget_remaining &gt; 10:\n        return \"medium_model\"  # Mid-tier\n    else:\n        return \"cheap_model\"  # Low budget\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#3-time-based-routing","title":"3. Time-Based Routing","text":"<p>Route based on urgency:</p> <pre><code>import time\n\ndef time_router(state):\n    deadline = state.get(\"deadline\", float('inf'))\n    time_left = deadline - time.time()\n\n    if time_left &lt; 60:  # Less than 1 minute\n        return \"fast_path\"\n    elif time_left &lt; 300:  # Less than 5 minutes\n        return \"balanced_path\"\n    else:\n        return \"thorough_path\"\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#4-error-recovery","title":"4. Error Recovery","text":"<p>Handle errors gracefully:</p> <pre><code>def error_recovery(state):\n    error = state.get(\"error\")\n    retry_count = state.get(\"retry_count\", 0)\n\n    if error is None:\n        return \"success\"  # No error\n    elif retry_count &lt; 3:\n        return \"retry\"  # Try again\n    else:\n        return \"fallback\"  # Use fallback strategy\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#debugging-conditional-routes","title":"Debugging Conditional Routes","text":""},{"location":"building-blocks/langgraph/conditional-routing/#log-routing-decisions","title":"Log Routing Decisions","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef logged_router(state):\n    \\\"\\\"\\\"Router with logging.\\\"\\\"\\\"\n    decision = make_routing_decision(state)\n    logger.info(f\"Routing to {decision} based on state: {state}\")\n    return decision\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#test-routers-independently","title":"Test Routers Independently","text":"<pre><code>import unittest\n\nclass TestRouters(unittest.TestCase):\n    def test_quality_router(self):\n        # High quality \u2192 END\n        state = {\"quality_score\": 0.9}\n        self.assertEqual(quality_router(state), ma.END)\n\n        # Low quality \u2192 retry\n        state = {\"quality_score\": 0.3, \"attempt\": 1}\n        self.assertEqual(quality_router(state), \"retry\")\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#visualize-routes","title":"Visualize Routes","text":"<p>Use visualization (covered in next section) to see all possible paths.</p>"},{"location":"building-blocks/langgraph/conditional-routing/#performance-considerations","title":"Performance Considerations","text":""},{"location":"building-blocks/langgraph/conditional-routing/#avoid-expensive-operations-in-routers","title":"Avoid Expensive Operations in Routers","text":"<pre><code># \u274c Bad: Expensive operation\ndef slow_router(state):\n    # Don't do this!\n    expensive_result = call_llm_to_decide(state)\n    return expensive_result\n\n# \u2705 Good: Use state values\ndef fast_router(state):\n    # State already has what we need\n    return \"next\" if state[\"ready\"] else \"wait\"\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#minimize-router-complexity","title":"Minimize Router Complexity","text":"<pre><code># \u274c Bad: Too complex\ndef complex_router(state):\n    # 50 lines of nested if/else...\n    pass\n\n# \u2705 Good: Extract logic to nodes\ndef simple_router(state):\n    # Router just checks a flag\n    return state[\"next_node_name\"]\n\n# Let a node compute the routing decision\ndef decision_node(state):\n    # Complex logic here\n    next_node = complex_decision_logic(state)\n    return {\"next_node_name\": next_node}\n</code></pre>"},{"location":"building-blocks/langgraph/conditional-routing/#next-steps","title":"Next Steps","text":"<ul> <li>Compilation &amp; Execution \u2192 Run your workflows</li> <li>Visualization \u2192 See your routing paths</li> <li>State Management \u2192 Review state patterns</li> </ul>"},{"location":"building-blocks/langgraph/conditional-routing/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph Conditional Edges - Official guide</li> <li>LangGraph Tutorials - Routing examples</li> </ul> <p>Next: Learn about Compilation &amp; Execution \u2192</p>"},{"location":"building-blocks/langgraph/nodes-edges/","title":"LangGraph Nodes &amp; Edges","text":"<p>TL;DR: Nodes are functions or agents that process state; edges define how state flows between them.</p>"},{"location":"building-blocks/langgraph/nodes-edges/#what-are-nodes","title":"What are Nodes?","text":"<p>Nodes are the processing units in your workflow. Each node: - Receives the current state - Performs some operation (LLM call, tool use, logic) - Returns an update to merge into state</p> <p>Think of nodes as steps in your agent's workflow.</p>"},{"location":"building-blocks/langgraph/nodes-edges/#creating-nodes","title":"Creating Nodes","text":""},{"location":"building-blocks/langgraph/nodes-edges/#function-nodes","title":"Function Nodes","text":"<p>The simplest type of node is a regular Python function:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\n\nclass State(TypedDict):\n    question: str\n    answer: Optional[str]\n\ndef answer_question(state: State) -&gt; dict:\n    \"\"\"A simple function node.\"\"\"\n    question = state[\"question\"]\n    answer = f\"The answer to '{question}' is...\"\n    return {\"answer\": answer}\n\n# Add to workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"answerer\", answer_question)\n</code></pre> <p>Key points: - Takes <code>state</code> as input - Returns a dict with updates - Updates are automatically merged into state</p>"},{"location":"building-blocks/langgraph/nodes-edges/#dspy-nodes","title":"DSPy Nodes","text":"<p>Use <code>@ma.dspy_node</code> for DSPy modules:</p> <pre><code>@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\nworkflow.add_node(\"qa\", QA())\n</code></pre> <p>Advantages: - Automatic state extraction - Signature-based field mapping - Built-in Langfuse tracing - Clean, declarative code</p>"},{"location":"building-blocks/langgraph/nodes-edges/#class-based-nodes","title":"Class-Based Nodes","text":"<p>Create reusable node classes:</p> <pre><code>class CustomNode:\n    def __init__(self, config):\n        self.config = config\n\n    def __call__(self, state: State) -&gt; dict:\n        \"\"\"Make the class callable.\"\"\"\n        # Your logic here\n        return {\"answer\": \"Result\"}\n\nnode = CustomNode(config={\"param\": \"value\"})\nworkflow.add_node(\"custom\", node)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#node-patterns","title":"Node Patterns","text":""},{"location":"building-blocks/langgraph/nodes-edges/#1-data-transformation","title":"1. Data Transformation","text":"<p>Transform state data:</p> <pre><code>def clean_input(state):\n    \"\"\"Clean and validate input.\"\"\"\n    question = state[\"question\"].strip().lower()\n    return {\"question\": question}\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#2-llm-calls","title":"2. LLM Calls","text":"<p>Call language models:</p> <pre><code>@ma.dspy_node\nclass Summarizer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.summarizer = dspy.ChainOfThought(\"text -&gt; summary\")\n\n    def forward(self, text):\n        return self.summarizer(text=text)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#3-tool-use","title":"3. Tool Use","text":"<p>Execute external tools:</p> <pre><code>def search_web(state):\n    \"\"\"Search the web for information.\"\"\"\n    query = state[\"search_query\"]\n    results = search_api(query)  # Your API call\n    return {\"search_results\": results}\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#4-conditional-logic","title":"4. Conditional Logic","text":"<p>Add branching logic:</p> <pre><code>def classifier(state):\n    \"\"\"Classify input type.\"\"\"\n    question = state[\"question\"]\n\n    if \"calculate\" in question.lower():\n        return {\"category\": \"math\"}\n    elif \"search\" in question.lower():\n        return {\"category\": \"research\"}\n    else:\n        return {\"category\": \"general\"}\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#5-state-aggregation","title":"5. State Aggregation","text":"<p>Combine multiple state fields:</p> <pre><code>def aggregate_results(state):\n    \"\"\"Combine results from multiple sources.\"\"\"\n    findings = state.get(\"findings\", [])\n    sources = state.get(\"sources\", [])\n\n    combined = {\n        \"final_answer\": \"\\n\".join(findings),\n        \"total_sources\": len(sources)\n    }\n    return combined\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#what-are-edges","title":"What are Edges?","text":"<p>Edges define how state flows between nodes. There are three types:</p> <ol> <li>Normal edges: Direct connections</li> <li>Conditional edges: Branch based on state</li> <li>Special edges: START and END</li> </ol>"},{"location":"building-blocks/langgraph/nodes-edges/#normal-edges","title":"Normal Edges","text":"<p>Connect nodes directly:</p> <pre><code>workflow.add_edge(\"node_a\", \"node_b\")\n# State flows: node_a \u2192 node_b\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#example-linear-pipeline","title":"Example: Linear Pipeline","text":"<pre><code>workflow.add_edge(ma.START, \"step1\")\nworkflow.add_edge(\"step1\", \"step2\")\nworkflow.add_edge(\"step2\", \"step3\")\nworkflow.add_edge(\"step3\", ma.END)\n\n# Flow: START \u2192 step1 \u2192 step2 \u2192 step3 \u2192 END\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#conditional-edges","title":"Conditional Edges","text":"<p>Branch based on state:</p> <pre><code>def router(state):\n    \"\"\"Decide which node to go to next.\"\"\"\n    if state[\"category\"] == \"math\":\n        return \"calculator\"\n    elif state[\"category\"] == \"research\":\n        return \"researcher\"\n    else:\n        return \"general_qa\"\n\nworkflow.add_conditional_edges(\"classifier\", router)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#example-retry-logic","title":"Example: Retry Logic","text":"<pre><code>def should_retry(state):\n    \"\"\"Retry if quality is low.\"\"\"\n    if state.get(\"iteration\", 0) &gt;= 3:\n        return ma.END  # Max retries reached\n\n    quality = state.get(\"quality_score\", 0)\n    if quality &lt; 0.7:\n        return \"generate_answer\"  # Retry\n    return ma.END  # Success\n\nworkflow.add_conditional_edges(\"quality_check\", should_retry)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#example-multi-path-routing","title":"Example: Multi-Path Routing","text":"<pre><code>def route_by_complexity(state):\n    \"\"\"Route based on question complexity.\"\"\"\n    question = state[\"question\"]\n\n    if len(question.split()) &lt; 5:\n        return \"simple_qa\"\n    elif \"research\" in question.lower():\n        return \"research_agent\"\n    else:\n        return \"advanced_qa\"\n\nworkflow.add_conditional_edges(\"router\", route_by_complexity)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#special-edges","title":"Special Edges","text":""},{"location":"building-blocks/langgraph/nodes-edges/#start","title":"START","text":"<p>Entry point of the workflow:</p> <pre><code>workflow.add_edge(ma.START, \"first_node\")\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#end","title":"END","text":"<p>Exit point of the workflow:</p> <pre><code>workflow.add_edge(\"last_node\", ma.END)\n\n# Or conditional\ndef maybe_end(state):\n    if state[\"done\"]:\n        return ma.END\n    return \"continue\"\n\nworkflow.add_conditional_edges(\"checker\", maybe_end)\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#complete-example","title":"Complete Example","text":"<p>Here's a full workflow with multiple node types and edges:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# State\nclass QAState(TypedDict):\n    question: str\n    category: Optional[str]\n    search_results: Optional[str]\n    answer: Optional[str]\n    quality_score: Optional[float]\n    iteration: int\n\n# Nodes\ndef classifier(state: QAState) -&gt; dict:\n    \"\"\"Classify question type.\"\"\"\n    question = state[\"question\"]\n    if \"search\" in question.lower():\n        return {\"category\": \"research\"}\n    return {\"category\": \"direct\"}\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.research = dspy.ChainOfThought(\"question -&gt; search_results\")\n\n    def forward(self, question):\n        return self.research(question=question)\n\n@ma.dspy_node\nclass DirectQA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityChecker(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.checker = dspy.Predict(\"question, answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, question, answer):\n        return self.checker(question=question, answer=answer)\n\ndef increment_iteration(state: QAState) -&gt; dict:\n    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n\n# Routing functions\ndef route_by_category(state: QAState):\n    \"\"\"Route based on classification.\"\"\"\n    if state[\"category\"] == \"research\":\n        return \"researcher\"\n    return \"direct_qa\"\n\ndef should_retry(state: QAState):\n    \"\"\"Check if we should retry.\"\"\"\n    if state.get(\"iteration\", 0) &gt;= 2:\n        return ma.END\n\n    quality = float(state.get(\"quality_score\", 0))\n    if quality &lt; 0.7:\n        return \"increment\"  # Retry\n    return ma.END\n\n# Build workflow\nworkflow = ma.graph.StateGraph(QAState)\n\n# Add nodes\nworkflow.add_node(\"classifier\", classifier)\nworkflow.add_node(\"researcher\", Researcher())\nworkflow.add_node(\"direct_qa\", DirectQA())\nworkflow.add_node(\"quality_check\", QualityChecker())\nworkflow.add_node(\"increment\", increment_iteration)\n\n# Add edges\nworkflow.add_edge(ma.START, \"classifier\")\nworkflow.add_conditional_edges(\"classifier\", route_by_category)\nworkflow.add_edge(\"researcher\", \"quality_check\")\nworkflow.add_edge(\"direct_qa\", \"quality_check\")\nworkflow.add_conditional_edges(\"quality_check\", should_retry)\nworkflow.add_edge(\"increment\", \"classifier\")  # Loop back\n\n# Compile\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\n    \"question\": \"Search for information about LangGraph\",\n    \"iteration\": 0\n})\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Quality: {result['quality_score']}\")\nprint(f\"Iterations: {result['iteration']}\")\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/nodes-edges/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Keep nodes focused <pre><code># \u2705 Single responsibility\ndef extract_entities(state):\n    # Only extracts entities\n    pass\n\ndef classify_entities(state):\n    # Only classifies\n    pass\n</code></pre></p> </li> <li> <p>Return only updates <pre><code># \u2705 Clean update\ndef node(state):\n    return {\"answer\": \"Paris\"}\n\n# \u274c Redundant\ndef node(state):\n    return {**state, \"answer\": \"Paris\"}\n</code></pre></p> </li> <li> <p>Use descriptive node names <pre><code># \u2705 Clear purpose\nworkflow.add_node(\"extract_entities\", extract_entities)\nworkflow.add_node(\"classify_sentiment\", classify_sentiment)\n\n# \u274c Vague\nworkflow.add_node(\"node1\", func1)\n</code></pre></p> </li> <li> <p>Handle missing state gracefully <pre><code># \u2705 Safe access\ndef node(state):\n    value = state.get(\"field\", default_value)\n    return {\"result\": process(value)}\n</code></pre></p> </li> <li> <p>Use conditional edges for branching <pre><code># \u2705 Explicit routing\ndef router(state):\n    if condition:\n        return \"path_a\"\n    return \"path_b\"\n\nworkflow.add_conditional_edges(\"router\", router)\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/nodes-edges/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Never do this\ndef node(state):\n    state[\"answer\"] = \"Paris\"\n    return state\n</code></pre></p> </li> <li> <p>Create side effects <pre><code># \u274c Side effects make debugging hard\nglobal_var = None\n\ndef node(state):\n    global global_var\n    global_var = state[\"value\"]  # Bad!\n    return {}\n</code></pre></p> </li> <li> <p>Use long, complex nodes <pre><code># \u274c Too much in one node\ndef mega_node(state):\n    # 100 lines of code...\n    pass\n\n# \u2705 Split into smaller nodes\ndef step1(state): pass\ndef step2(state): pass\ndef step3(state): pass\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/nodes-edges/#debugging-nodes","title":"Debugging Nodes","text":""},{"location":"building-blocks/langgraph/nodes-edges/#print-state","title":"Print State","text":"<pre><code>def debug_node(state):\n    print(f\"State at debug point: {state}\")\n    return {}\n\nworkflow.add_node(\"debug\", debug_node)\nworkflow.add_edge(\"some_node\", \"debug\")\nworkflow.add_edge(\"debug\", \"next_node\")\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#use-langfuse-tracing","title":"Use Langfuse Tracing","text":"<pre><code>ma.tracing.init()\n\n# All nodes are automatically traced!\n# Check Langfuse UI to see:\n# - State at each node\n# - Node execution times\n# - LLM calls\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#test-nodes-independently","title":"Test Nodes Independently","text":"<pre><code>import unittest\n\nclass TestNodes(unittest.TestCase):\n    def test_classifier(self):\n        state = {\"question\": \"Search for cats\"}\n        result = classifier(state)\n        self.assertEqual(result[\"category\"], \"research\")\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#performance-tips","title":"Performance Tips","text":""},{"location":"building-blocks/langgraph/nodes-edges/#1-parallelize-independent-nodes","title":"1. Parallelize Independent Nodes","text":"<p>LangGraph can run independent nodes in parallel (advanced feature):</p> <pre><code># These can run in parallel\nworkflow.add_node(\"fetch_data_a\", fetch_a)\nworkflow.add_node(\"fetch_data_b\", fetch_b)\nworkflow.add_edge(ma.START, \"fetch_data_a\")\nworkflow.add_edge(ma.START, \"fetch_data_b\")\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#2-minimize-state-size","title":"2. Minimize State Size","text":"<p>Only include necessary fields:</p> <pre><code># \u2705 Minimal state\nclass State(TypedDict):\n    question: str\n    answer: str\n\n# \u274c Too much\nclass State(TypedDict):\n    question: str\n    answer: str\n    intermediate_result_1: str\n    intermediate_result_2: str\n    # ... many more fields\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#3-cache-expensive-operations","title":"3. Cache Expensive Operations","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef expensive_lookup(key: str):\n    # Expensive operation\n    return result\n\ndef node(state):\n    result = expensive_lookup(state[\"key\"])\n    return {\"result\": result}\n</code></pre>"},{"location":"building-blocks/langgraph/nodes-edges/#next-steps","title":"Next Steps","text":"<ul> <li>Conditional Routing \u2192 Advanced routing patterns</li> <li>Compilation &amp; Execution \u2192 Run your workflows</li> <li>Visualization \u2192 Visualize your graphs</li> <li>Your First Agent \u2192 Build a complete agent</li> </ul>"},{"location":"building-blocks/langgraph/nodes-edges/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph Nodes Docs - Official guide</li> <li>LangGraph Edges Docs - Official guide</li> </ul> <p>Next: Master Conditional Routing \u2192</p>"},{"location":"building-blocks/langgraph/overview/","title":"LangGraph Overview","text":"<p>TL;DR: LangGraph builds stateful, cyclical workflows for LLM agents\u2014think state machines for AI.</p>"},{"location":"building-blocks/langgraph/overview/#what-is-langgraph","title":"What is LangGraph?","text":"<p>LangGraph is a framework for building stateful, multi-step workflows with LLMs. Unlike simple chains (input \u2192 LLM \u2192 output), LangGraph enables:</p> <ul> <li>Cycles: Agents can loop, retry, and refine</li> <li>State: Persistent memory across steps</li> <li>Branching: Conditional routing based on outputs</li> <li>Parallelism: Run multiple nodes concurrently</li> </ul> <p>Think of it as a state machine where each node is an AI agent or tool.</p>"},{"location":"building-blocks/langgraph/overview/#why-langgraph","title":"Why LangGraph?","text":""},{"location":"building-blocks/langgraph/overview/#the-problem-with-chains","title":"The Problem with Chains","text":"<p>Traditional LLM chains are linear:</p> <pre><code># \u274c Linear chain - can't loop or branch\nquery \u2192 retrieve_docs \u2192 generate_answer \u2192 done\n</code></pre> <p>Real agents need to: - Loop until a condition is met - Branch based on intermediate results - Maintain state across steps</p>"},{"location":"building-blocks/langgraph/overview/#the-langgraph-solution","title":"The LangGraph Solution","text":"<pre><code># \u2705 Cyclic workflow with branching\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  generate   \u2502\n       \u2502   query     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   search    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502 synthesize  \u2502      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n       \u2502   check     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502  quality    \u2502 if poor, retry\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502 if good\n              \u25bc\n            END\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"building-blocks/langgraph/overview/#1-state","title":"1. State","text":"<p>State is a <code>TypedDict</code> that flows through your workflow:</p> <pre><code>from typing import TypedDict, Optional\n\nclass ResearchState(TypedDict):\n    question: str\n    search_query: Optional[str]\n    findings: Optional[str]\n    answer: Optional[str]\n</code></pre> <p>Learn more about State \u2192</p>"},{"location":"building-blocks/langgraph/overview/#2-nodes","title":"2. Nodes","text":"<p>Nodes are functions or agents that process state:</p> <pre><code>import mahsm as ma\n\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\\n        self.research = dspy.ChainOfThought(\"question -&gt; findings\")\n\n    def forward(self, question):\n        return self.research(question=question)\n</code></pre> <p>Learn more about Nodes &amp; Edges \u2192</p>"},{"location":"building-blocks/langgraph/overview/#3-edges","title":"3. Edges","text":"<p>Edges connect nodes:</p> <pre><code># Simple edge\nworkflow.add_edge(\"node_a\", \"node_b\")\n\n# Conditional edge\nworkflow.add_conditional_edges(\n    \"checker\",\n    lambda state: \"retry\" if state[\"quality\"] &lt; 0.7 else END\n)\n</code></pre> <p>Learn more about Conditional Routing \u2192</p>"},{"location":"building-blocks/langgraph/overview/#4-graph-compilation","title":"4. Graph Compilation","text":"<p>Compile the workflow into an executable graph:</p> <pre><code>workflow = ma.graph.StateGraph(MyState)\nworkflow.add_node(\"agent\", my_agent)\nworkflow.add_edge(ma.START, \"agent\")\nworkflow.add_edge(\"agent\", ma.END)\n\ngraph = workflow.compile()  # \u2705 Ready to run\n</code></pre> <p>Learn more about Compilation \u2192</p>"},{"location":"building-blocks/langgraph/overview/#quick-example","title":"Quick Example","text":"<p>Let's build a self-correcting Q&amp;A agent:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\nma.tracing.init()\n\n# 1. Define state\nclass QAState(TypedDict):\n    question: str\n    answer: Optional[str]\n    quality_score: Optional[float]\n    iteration: int\n\n# 2. Define nodes\n@ma.dspy_node\nclass Answerer(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityChecker(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.checker = dspy.Predict(\"question, answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, question, answer):\n        return self.checker(question=question, answer=answer)\n\ndef increment_iteration(state: QAState) -&gt; QAState:\n    \"\"\"Increment iteration counter.\"\"\"\n    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n\n# 3. Define routing\ndef should_retry(state: QAState):\n    \"\"\"Retry if quality is low and we haven't exceeded max iterations.\"\"\"\n    if state.get(\"iteration\", 0) &gt;= 3:\n        return ma.END  # Give up after 3 tries\n\n    quality = float(state.get(\"quality_score\", 0))\n    if quality &lt; 0.7:\n        return \"answer\"  # Retry\n    return ma.END  # Good enough!\n\n# 4. Build graph\nworkflow = ma.graph.StateGraph(QAState)\n\nworkflow.add_node(\"answer\", Answerer())\nworkflow.add_node(\"check\", QualityChecker())\nworkflow.add_node(\"increment\", increment_iteration)\n\nworkflow.add_edge(ma.START, \"increment\")\nworkflow.add_edge(\"increment\", \"answer\")\nworkflow.add_edge(\"answer\", \"check\")\nworkflow.add_conditional_edges(\"check\", should_retry)\n\ngraph = workflow.compile()\n\n# 5. Run\nresult = graph.invoke({\n    \"question\": \"Explain quantum entanglement simply.\",\n    \"iteration\": 0\n})\n\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Quality: {result['quality_score']}\")\nprint(f\"Iterations: {result['iteration']}\")\n# \u2705 Agent retries until quality threshold is met!\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#langgraph-in-mahsm","title":"LangGraph in mahsm","text":"<p>mahsm enhances LangGraph with:</p>"},{"location":"building-blocks/langgraph/overview/#1-simplified-node-creation","title":"1. Simplified Node Creation","text":"<pre><code># Without mahsm\ndef my_node(state):\n    # Manual state extraction\n    question = state[\"question\"]\n    # Call LLM\n    response = llm.complete(question)\n    # Manual state update\n    return {\"answer\": response}\n\n# With mahsm\n@ma.dspy_node\nclass MyNode(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n# \u2705 State extraction/merging handled automatically\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-automatic-tracing","title":"2. Automatic Tracing","text":"<pre><code>ma.tracing.init()  # One line\n# \u2705 All LangGraph nodes traced to Langfuse\n# \u2705 All DSPy calls traced\n# \u2705 Custom functions with @observe traced\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-type-safe-state","title":"3. Type-Safe State","text":"<pre><code>class MyState(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 IDE autocomplete\n# \u2705 Static type checking\n# \u2705 Runtime validation\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#when-to-use-langgraph","title":"When to Use LangGraph","text":""},{"location":"building-blocks/langgraph/overview/#great-for","title":"\u2705 Great For:","text":"<ul> <li>Multi-step agents that need memory</li> <li>Cyclical workflows (retry, refine, iterate)</li> <li>Conditional branching based on outputs</li> <li>Complex orchestration of multiple agents</li> <li>Human-in-the-loop systems</li> </ul>"},{"location":"building-blocks/langgraph/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple one-shot completions (use DSPy directly)</li> <li>Purely stateless operations (no need for state management)</li> <li>Real-time streaming (LangGraph is batch-oriented)</li> </ul>"},{"location":"building-blocks/langgraph/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"building-blocks/langgraph/overview/#1-linear-pipeline","title":"1. Linear Pipeline","text":"<pre><code>START \u2192 agent1 \u2192 agent2 \u2192 agent3 \u2192 END\n</code></pre> <pre><code>workflow.add_edge(ma.START, \"agent1\")\nworkflow.add_edge(\"agent1\", \"agent2\")\nworkflow.add_edge(\"agent2\", \"agent3\")\nworkflow.add_edge(\"agent3\", ma.END)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#2-conditional-branching","title":"2. Conditional Branching","text":"<pre><code>                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      START \u2500\u2500\u2500\u2500\u2500\u25ba\u2502 router  \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                 \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 path_a \u2502       \u2502 path_b \u2502\n         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n                    END\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"router\",\n    lambda state: \"path_a\" if condition(state) else \"path_b\"\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#3-retry-loop","title":"3. Retry Loop","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502              \u2502\n        \u25bc              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  try  \u2502\u2500\u2500\u2500\u25ba\u2502   check   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u25bc\n                    END (if success)\n</code></pre> <pre><code>workflow.add_conditional_edges(\n    \"check\",\n    lambda state: \"try\" if not success(state) else ma.END\n)\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#4-multi-agent-collaboration","title":"4. Multi-Agent Collaboration","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u250c\u2500\u2500\u25ba\u2502 researcher\u2502\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n    \u2502                     \u25bc\n    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2502coordinator\u2502\u25c4\u2500\u2500\u2502synthesizer\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"building-blocks/langgraph/overview/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/overview/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict for state <pre><code>class State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Keep nodes focused <pre><code># \u2705 Single responsibility\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    # Only generates queries\n    pass\n</code></pre></p> </li> <li> <p>Handle missing state gracefully <pre><code>def my_router(state):\n    value = state.get(\"key\", default_value)\n    # ...\n</code></pre></p> </li> <li> <p>Use conditional edges for routing <pre><code>workflow.add_conditional_edges(\"checker\", route_function)\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Don't do this\ndef node(state):\n    state[\"key\"] = \"value\"  # Mutates input!\n    return state\n\n# \u2705 Do this\ndef node(state):\n    return {\"key\": \"value\"}  # Returns update\n</code></pre></p> </li> <li> <p>Create infinite loops without exit conditions <pre><code># \u274c No way to exit\nworkflow.add_conditional_edges(\"node\", lambda s: \"node\")\n\n# \u2705 Add exit condition\ndef router(state):\n    if state[\"count\"] &gt; 10:\n        return ma.END\n    return \"node\"\n</code></pre></p> </li> <li> <p>Over-complicate the graph <pre><code># \u274c Too many branches\n# Keep it simple and readable\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/overview/#next-steps","title":"Next Steps","text":"<ul> <li>State Management \u2192 Learn about TypedDict and state updates</li> <li>Nodes &amp; Edges \u2192 Build your graph components</li> <li>Conditional Routing \u2192 Add branching logic</li> <li>Compilation &amp; Execution \u2192 Run your workflows</li> <li>Visualization \u2192 Visualize your graphs</li> </ul>"},{"location":"building-blocks/langgraph/overview/#external-resources","title":"External Resources","text":"<ul> <li>Official LangGraph Docs - Comprehensive guide</li> <li>LangGraph GitHub - Source code and examples</li> <li>LangGraph Tutorials - Step-by-step guides</li> </ul> <p>Ready to dive deeper? Start with State Management \u2192</p>"},{"location":"building-blocks/langgraph/state/","title":"LangGraph State Management","text":"<p>TL;DR: State in LangGraph is a TypedDict that flows through your workflow, carrying data between nodes.</p>"},{"location":"building-blocks/langgraph/state/#what-is-state","title":"What is State?","text":"<p>In LangGraph, state is a dictionary that: - Flows through your workflow - Is read by nodes - Is updated by nodes - Maintains type safety with <code>TypedDict</code></p> <p>Think of it as shared memory for your agent workflow.</p>"},{"location":"building-blocks/langgraph/state/#defining-state","title":"Defining State","text":"<p>Use Python's <code>TypedDict</code> to define your state schema:</p> <pre><code>from typing import TypedDict, Optional\n\nclass MyState(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]\n</code></pre> <p>Benefits: - \u2705 IDE autocomplete - \u2705 Type checking - \u2705 Clear documentation - \u2705 Runtime validation</p>"},{"location":"building-blocks/langgraph/state/#state-flow","title":"State Flow","text":"<p>State flows through nodes in your workflow:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    input: str\n    intermediate: str\n    output: str\n\n# Node 1: Reads 'input', writes 'intermediate'\ndef node1(state: State) -&gt; dict:\n    result = process(state[\"input\"])\n    return {\"intermediate\": result}\n\n# Node 2: Reads 'intermediate', writes 'output'\ndef node2(state: State) -&gt; dict:\n    result = finalize(state[\"intermediate\"])\n    return {\"output\": result}\n\n# Build workflow\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"node1\", node1)\nworkflow.add_node(\"node2\", node2)\nworkflow.add_edge(ma.START, \"node1\")\nworkflow.add_edge(\"node1\", \"node2\")\nworkflow.add_edge(\"node2\", ma.END)\ngraph = workflow.compile()\n\n# Run\nresult = graph.invoke({\"input\": \"Hello\"})\n# State flows: {\"input\": \"Hello\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\"} \u2192 {\"input\": \"Hello\", \"intermediate\": \"...\", \"output\": \"...\"}\nprint(result[\"output\"])\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-updates","title":"State Updates","text":""},{"location":"building-blocks/langgraph/state/#immutable-updates","title":"Immutable Updates","text":"<p>Nodes return updates, not full state:</p> <pre><code># \u274c DON'T: Mutate state directly\ndef bad_node(state):\n    state[\"answer\"] = \"Paris\"  # Mutates input!\n    return state\n\n# \u2705 DO: Return updates\ndef good_node(state):\n    return {\"answer\": \"Paris\"}  # Returns update\n</code></pre> <p>LangGraph merges your update into the state automatically:</p> <pre><code># Before node\nstate = {\"question\": \"What is the capital of France?\"}\n\n# Node returns\nupdate = {\"answer\": \"Paris\"}\n\n# After node (automatic merge)\nstate = {\n    \"question\": \"What is the capital of France?\",\n    \"answer\": \"Paris\"\n}\n</code></pre>"},{"location":"building-blocks/langgraph/state/#optional-vs-required-fields","title":"Optional vs Required Fields","text":"<p>Use <code>Optional</code> for fields that may not exist initially:</p> <pre><code>from typing import TypedDict, Optional\n\nclass State(TypedDict):\n    # Required fields (must be in initial input)\n    question: str\n\n    # Optional fields (nodes will populate)\n    answer: Optional[str]\n    reasoning: Optional[str]\n    confidence: Optional[float]\n</code></pre>"},{"location":"building-blocks/langgraph/state/#complex-state-types","title":"Complex State Types","text":""},{"location":"building-blocks/langgraph/state/#lists","title":"Lists","text":"<pre><code>from typing import List\n\nclass State(TypedDict):\n    messages: List[str]\n    findings: List[dict]\n</code></pre> <p>Appending to lists:</p> <pre><code>def add_message(state: State) -&gt; dict:\n    # Option 1: Replace entire list\n    new_messages = state[\"messages\"] + [\"New message\"]\n    return {\"messages\": new_messages}\n\n    # Option 2: Use Annotated for automatic appending (advanced)\n    # See LangGraph docs for details\n</code></pre>"},{"location":"building-blocks/langgraph/state/#nested-dicts","title":"Nested Dicts","text":"<pre><code>class State(TypedDict):\n    user: dict  # {\"name\": str, \"email\": str}\n    config: dict\n</code></pre>"},{"location":"building-blocks/langgraph/state/#custom-classes","title":"Custom Classes","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass User:\n    name: str\n    email: str\n\nclass State(TypedDict):\n    user: User\n    active: bool\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-in-mahsm","title":"State in mahsm","text":""},{"location":"building-blocks/langgraph/state/#with-dspy_node","title":"With @dspy_node","text":"<p><code>@dspy_node</code> automatically extracts and updates state:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\n\nclass QAState(TypedDict):\n    question: str\n    reasoning: Optional[str]\n    answer: Optional[str]\n\n@ma.dspy_node\nclass QA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; reasoning, answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n# In workflow\nworkflow = ma.graph.StateGraph(QAState)\nworkflow.add_node(\"qa\", QA())\n\n# When QA runs:\n# 1. Extracts 'question' from state\n# 2. Calls forward(question=state[\"question\"])\n# 3. Merges {\"reasoning\": \"...\", \"answer\": \"...\"} into state\n</code></pre>"},{"location":"building-blocks/langgraph/state/#with-regular-functions","title":"With Regular Functions","text":"<pre><code>def my_node(state: QAState) -&gt; dict:\n    \"\"\"Regular function node.\"\"\"\n    question = state[\"question\"]\n    # Process...\n    return {\"answer\": \"Paris\"}\n\nworkflow.add_node(\"my_node\", my_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-initialization","title":"State Initialization","text":""},{"location":"building-blocks/langgraph/state/#basic-initialization","title":"Basic Initialization","text":"<pre><code># Invoke with initial state\nresult = graph.invoke({\n    \"question\": \"What is DSPy?\",\n    \"confidence\": 0.0\n})\n</code></pre>"},{"location":"building-blocks/langgraph/state/#from-user-input","title":"From User Input","text":"<pre><code>def create_initial_state(user_input: str) -&gt; dict:\n    \"\"\"Create initial state from user input.\"\"\"\n    return {\n        \"question\": user_input,\n        \"iteration\": 0,\n        \"history\": []\n    }\n\nstate = create_initial_state(\"What is LangGraph?\")\nresult = graph.invoke(state)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-persistence","title":"State Persistence","text":"<p>State is immutable within a single execution:</p> <pre><code># Single execution\nresult = graph.invoke({\"question\": \"Hello\"})\n# State flows through workflow and is returned\n\n# New execution (fresh state)\nresult2 = graph.invoke({\"question\": \"Goodbye\"})\n# Independent from result\n</code></pre> <p>For persistent state across executions, use LangGraph's checkpointing (advanced):</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\n# Compile with memory\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\n# Run with thread ID\nconfig = {\"configurable\": {\"thread_id\": \"user_123\"}}\nresult1 = graph.invoke({\"question\": \"Hello\"}, config=config)\nresult2 = graph.invoke({\"question\": \"Continue...\"}, config=config)\n# result2 has access to result1's state!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/state/#do","title":"\u2705 Do:","text":"<ol> <li> <p>Use TypedDict <pre><code># \u2705 Type-safe\nclass State(TypedDict):\n    field: str\n</code></pre></p> </li> <li> <p>Make node outputs Optional <pre><code># \u2705 Clear that these are populated later\nanswer: Optional[str]\n</code></pre></p> </li> <li> <p>Return only updates <pre><code># \u2705 Clean updates\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> <li> <p>Use descriptive field names <pre><code># \u2705 Clear purpose\nuser_question: str\ngenerated_answer: str\nquality_score: float\n</code></pre></p> </li> <li> <p>Match DSPy signatures to state keys <pre><code>class State(TypedDict):\n    question: str\n    answer: str\n\n# \u2705 Signature matches\ndspy.ChainOfThought(\"question -&gt; answer\")\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#dont","title":"\u274c Don't:","text":"<ol> <li> <p>Mutate state directly <pre><code># \u274c Never do this\ndef node(state):\n    state[\"answer\"] = \"Paris\"\n    return state\n</code></pre></p> </li> <li> <p>Use vague names <pre><code># \u274c What is this?\nresult: str\ndata: dict\n</code></pre></p> </li> <li> <p>Make everything required <pre><code># \u274c Will error if not in initial input\nclass State(TypedDict):\n    question: str\n    answer: str  # Should be Optional[str]\n</code></pre></p> </li> <li> <p>Return full state unnecessarily <pre><code># \u274c Redundant\ndef node(state):\n    return {**state, \"answer\": \"Paris\"}\n\n# \u2705 Just return update\ndef node(state):\n    return {\"answer\": \"Paris\"}\n</code></pre></p> </li> </ol>"},{"location":"building-blocks/langgraph/state/#debugging-state","title":"Debugging State","text":""},{"location":"building-blocks/langgraph/state/#print-state-in-nodes","title":"Print State in Nodes","text":"<pre><code>def debug_node(state):\n    print(f\"Current state: {state}\")\n    return {}\n\nworkflow.add_node(\"debug\", debug_node)\n</code></pre>"},{"location":"building-blocks/langgraph/state/#trace-state-flow","title":"Trace State Flow","text":"<pre><code># Run with verbose output\nresult = graph.invoke({\"question\": \"Hello\"})\nprint(f\"Final state: {result}\")\n</code></pre>"},{"location":"building-blocks/langgraph/state/#use-langfuse","title":"Use Langfuse","text":"<p>With <code>ma.tracing.init()</code>, state is automatically logged:</p> <pre><code>ma.tracing.init()\nresult = graph.invoke({\"question\": \"Hello\"})\n# Check Langfuse UI to see state at each node!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#advanced-state-reducers","title":"Advanced: State Reducers","text":"<p>For complex state updates (like appending to lists), use reducers:</p> <pre><code>from typing import Annotated\nfrom langgraph.graph import add\n\nclass State(TypedDict):\n    # Normal field\n    question: str\n\n    # Auto-appending list (uses 'add' reducer)\n    messages: Annotated[List[str], add]\n\n# Now nodes can just return new messages\ndef add_message(state):\n    return {\"messages\": [\"New message\"]}\n# LangGraph automatically appends to existing messages!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#state-schema-evolution","title":"State Schema Evolution","text":"<p>As your workflow grows, extend your state:</p> <pre><code># v1\nclass StateV1(TypedDict):\n    question: str\n    answer: Optional[str]\n\n# v2 (add new fields)\nclass StateV2(TypedDict):\n    question: str\n    answer: Optional[str]\n    confidence: Optional[float]  # New field\n    sources: Optional[List[str]]  # New field\n</code></pre> <p>Existing nodes continue working (they ignore new fields).</p>"},{"location":"building-blocks/langgraph/state/#example-multi-step-research-state","title":"Example: Multi-Step Research State","text":"<pre><code>from typing import TypedDict, Optional, List\n\nclass ResearchState(TypedDict):\n    # Input\n    question: str\n\n    # Intermediate\n    search_queries: Optional[List[str]]\n    raw_findings: Optional[List[dict]]\n\n    # Output\n    synthesized_answer: Optional[str]\n    sources: Optional[List[str]]\n    confidence: Optional[float]\n\n    # Metadata\n    iteration: int\n    total_tokens: int\n\n# Use in workflow\n@ma.dspy_node\nclass QueryGenerator(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.gen = dspy.Predict(\"question -&gt; search_queries: list of queries\")\n\n    def forward(self, question):\n        return self.gen(question=question)\n\n# Workflow automatically manages all state fields!\n</code></pre>"},{"location":"building-blocks/langgraph/state/#next-steps","title":"Next Steps","text":"<ul> <li>Nodes &amp; Edges \u2192 Learn how nodes interact with state</li> <li>Conditional Routing \u2192 Route based on state</li> <li>Your First Agent \u2192 Build a stateful agent</li> </ul>"},{"location":"building-blocks/langgraph/state/#external-resources","title":"External Resources","text":"<ul> <li>LangGraph State Docs - Official guide</li> <li>TypedDict Documentation - Python docs</li> </ul> <p>Next: Learn about Nodes &amp; Edges \u2192</p>"},{"location":"building-blocks/langgraph/visualization/","title":"LangGraph Visualization","text":"<p>TL;DR: Visualize your workflow structure to understand, debug, and share your agent architectures.</p>"},{"location":"building-blocks/langgraph/visualization/#why-visualize","title":"Why Visualize?","text":"<p>Visualization helps you:</p> <ul> <li>Understand complex workflows at a glance</li> <li>Debug routing and flow issues</li> <li>Document your agent architecture</li> <li>Communicate designs to your team</li> <li>Validate workflow structure before execution</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#basic-visualization","title":"Basic Visualization","text":""},{"location":"building-blocks/langgraph/visualization/#get-mermaid-diagram","title":"Get Mermaid Diagram","text":"<p>LangGraph can generate Mermaid diagrams:</p> <pre><code>import mahsm as ma\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    question: str\n    answer: str\n\nworkflow = ma.graph.StateGraph(State)\nworkflow.add_node(\"qa\", qa_func)\nworkflow.add_edge(ma.START, \"qa\")\nworkflow.add_edge(\"qa\", ma.END)\n\n# Compile\ngraph = workflow.compile()\n\n# Get Mermaid diagram\ndiagram = graph.get_graph().draw_mermaid()\nprint(diagram)\n</code></pre> <p>Output: <pre><code>graph TD\n    __start__([START]) --&gt; qa\n    qa --&gt; __end__([END])\n</code></pre></p>"},{"location":"building-blocks/langgraph/visualization/#view-in-jupyter","title":"View in Jupyter","text":"<p>Display in notebooks:</p> <pre><code>from IPython.display import display, Markdown\n\n# Get diagram\ndiagram = graph.get_graph().draw_mermaid()\n\n# Display\ndisplay(Markdown(f\"```mermaid\\n{diagram}\\n```\"))\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#save-to-file","title":"Save to File","text":"<p>Save diagram for documentation:</p> <pre><code>diagram = graph.get_graph().draw_mermaid()\n\nwith open(\"workflow.mmd\", \"w\") as f:\n    f.write(diagram)\n\n# Or save as markdown\nwith open(\"workflow.md\", \"w\") as f:\n    f.write(f\"# Workflow Diagram\\n\\n```mermaid\\n{diagram}\\n```\")\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#complete-example","title":"Complete Example","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# Configure\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# State\nclass AgentState(TypedDict):\n    question: str\n    category: Optional[str]\n    answer: Optional[str]\n    quality_score: Optional[float]\n\n# Nodes\ndef categorize(state):\n    if \"code\" in state[\"question\"].lower():\n        return {\"category\": \"programming\"}\n    return {\"category\": \"general\"}\n\n@ma.dspy_node\nclass ProgrammingQA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass GeneralQA(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.qa = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.qa(question=question)\n\n@ma.dspy_node\nclass QualityCheck(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.check = dspy.Predict(\"answer -&gt; quality_score: float 0-1\")\n\n    def forward(self, answer):\n        return self.check(answer=answer)\n\n# Routing\ndef route_by_category(state):\n    if state[\"category\"] == \"programming\":\n        return \"programming_qa\"\n    return \"general_qa\"\n\ndef should_retry(state):\n    if float(state.get(\"quality_score\", 0)) &lt; 0.7:\n        return \"categorize\"  # Retry\n    return ma.END\n\n# Build workflow\nworkflow = ma.graph.StateGraph(AgentState)\n\nworkflow.add_node(\"categorize\", categorize)\nworkflow.add_node(\"programming_qa\", ProgrammingQA())\nworkflow.add_node(\"general_qa\", GeneralQA())\nworkflow.add_node(\"quality_check\", QualityCheck())\n\nworkflow.add_edge(ma.START, \"categorize\")\nworkflow.add_conditional_edges(\"categorize\", route_by_category)\nworkflow.add_edge(\"programming_qa\", \"quality_check\")\nworkflow.add_edge(\"general_qa\", \"quality_check\")\nworkflow.add_conditional_edges(\"quality_check\", should_retry)\n\n# Compile\ngraph = workflow.compile()\n\n# Visualize\ndiagram = graph.get_graph().draw_mermaid()\nprint(diagram)\n</code></pre> <p>Output: <pre><code>graph TD\n    __start__([START]) --&gt; categorize\n    categorize -.-&gt; programming_qa\n    categorize -.-&gt; general_qa\n    programming_qa --&gt; quality_check\n    general_qa --&gt; quality_check\n    quality_check -.-&gt; categorize\n    quality_check -.-&gt; __end__([END])\n</code></pre></p> <p>Key: Solid lines (\u2192) are normal edges, dashed lines (-.\u2192) are conditional edges.</p>"},{"location":"building-blocks/langgraph/visualization/#visualization-tools","title":"Visualization Tools","text":""},{"location":"building-blocks/langgraph/visualization/#online-mermaid-editors","title":"Online Mermaid Editors","text":"<p>View and edit diagrams:</p> <ol> <li>Mermaid Live Editor - Official editor</li> <li>Mermaid Chart - Advanced features</li> <li>GitHub - Renders Mermaid in markdown files</li> </ol>"},{"location":"building-blocks/langgraph/visualization/#ide-support","title":"IDE Support","text":"<p>Many IDEs support Mermaid:</p> <ul> <li>VS Code - Markdown Preview Mermaid Support extension</li> <li>PyCharm - Mermaid plugin</li> <li>Obsidian - Built-in support</li> <li>Notion - Code block with \"mermaid\" language</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#generate-images","title":"Generate Images","text":"<p>Convert to PNG/SVG:</p> <pre><code># Install mermaid-cli\nnpm install -g @mermaid-js/mermaid-cli\n\n# Convert to PNG\nmmdc -i workflow.mmd -o workflow.png\n\n# Convert to SVG\nmmdc -i workflow.mmd -o workflow.svg\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#understanding-diagrams","title":"Understanding Diagrams","text":""},{"location":"building-blocks/langgraph/visualization/#node-types","title":"Node Types","text":"<pre><code>graph TD\n    START([START]) --&gt; regular_node\n    regular_node[Regular Node]\n    regular_node --&gt; END([END])\n</code></pre> <ul> <li>Rounded boxes <code>([...])</code>: START/END</li> <li>Rectangles <code>[...]</code>: Regular nodes</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#edge-types","title":"Edge Types","text":"<pre><code>graph TD\n    A[Node A] --&gt; B[Node B]\n    B -.-&gt; C[Node C]\n    B -.-&gt; D[Node D]\n</code></pre> <ul> <li>Solid arrow <code>--&gt;</code>: Normal edge (always follows)</li> <li>Dashed arrow <code>-.-&gt;</code>: Conditional edge (depends on state)</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#complex-flows","title":"Complex Flows","text":"<pre><code>graph TD\n    START([START]) --&gt; init[Initialize]\n    init --&gt; process[Process]\n    process -.-&gt; retry{Retry?}\n    retry -.-&gt; process\n    retry -.-&gt; finalize[Finalize]\n    finalize --&gt; END([END])\n</code></pre> <p>Shows loops and branching clearly.</p>"},{"location":"building-blocks/langgraph/visualization/#debugging-with-visualization","title":"Debugging with Visualization","text":""},{"location":"building-blocks/langgraph/visualization/#identify-issues","title":"Identify Issues","text":"<p>Common problems visualizations reveal:</p> <p>1. Unreachable Nodes <pre><code>graph TD\n    START([START]) --&gt; A[Node A]\n    A --&gt; END([END])\n    B[Orphan Node]\n</code></pre> Node B is never reached!</p> <p>2. Missing Exit <pre><code>graph TD\n    START([START]) --&gt; A[Node A]\n    A --&gt; B[Node B]\n    B --&gt; A\n</code></pre> Infinite loop - no path to END!</p> <p>3. Overly Complex Routing <pre><code>graph TD\n    A[Router] -.-&gt; B[Path 1]\n    A -.-&gt; C[Path 2]\n    A -.-&gt; D[Path 3]\n    A -.-&gt; E[Path 4]\n    A -.-&gt; F[Path 5]\n    A -.-&gt; G[Path 6]\n</code></pre> Too many branches - consider simplifying</p>"},{"location":"building-blocks/langgraph/visualization/#validation-checklist","title":"Validation Checklist","text":"<p>Use visualization to check:</p> <ul> <li>[ ] Every node is reachable from START</li> <li>[ ] There's at least one path to END</li> <li>[ ] Conditional edges have all possible outcomes</li> <li>[ ] No unnecessary complexity</li> <li>[ ] Clear, logical flow</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#documentation-best-practices","title":"Documentation Best Practices","text":""},{"location":"building-blocks/langgraph/visualization/#1-embed-in-readme","title":"1. Embed in README","text":"<pre><code># My Agent\n\n## Architecture\n\n```mermaid\ngraph TD\n    START([START]) --&gt; classify[Classify Question]\n    classify -.-&gt; simple[Simple QA]\n    classify -.-&gt; complex[Complex QA]\n    simple --&gt; END([END])\n    complex --&gt; END\n\\```\n\n## How It Works\n\n1. **Classify**: Determines question complexity\n2. **Simple QA**: Fast path for easy questions\n3. **Complex QA**: Deep analysis for hard questions\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#2-version-control","title":"2. Version Control","text":"<p>Track architecture changes:</p> <pre><code># Save diagram with version\ngit add workflow-v1.0.mmd\ngit commit -m \"feat: Initial workflow design\"\n\n# Later, after changes\ngit add workflow-v2.0.mmd\ngit commit -m \"feat: Added retry logic\"\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#3-team-communication","title":"3. Team Communication","text":"<p>Share diagrams in: - Pull requests - Show what changed - Design docs - Illustrate proposals - Onboarding - Help new team members understand - Meetings - Discuss architecture visually</p>"},{"location":"building-blocks/langgraph/visualization/#advanced-custom-styling","title":"Advanced: Custom Styling","text":"<p>Customize Mermaid diagrams:</p> <pre><code>diagram = graph.get_graph().draw_mermaid()\n\n# Add custom styling\nstyled_diagram = f\"\"\"\n%%{{init: {{'theme':'dark', 'themeVariables': {{'primaryColor':'#6366f1'}}}}}}%%\n{diagram}\n\"\"\"\n\nprint(styled_diagram)\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#themes","title":"Themes","text":"<p>Available themes: - <code>default</code> - Light theme - <code>dark</code> - Dark theme - <code>forest</code> - Green theme - <code>neutral</code> - Grayscale</p>"},{"location":"building-blocks/langgraph/visualization/#example-with-styling","title":"Example with Styling","text":"<pre><code>```mermaid\n%%{init: {'theme':'dark'}}%%\ngraph TD\n    START([START]) --&gt; A[Process]\n    A --&gt; END([END])\n\n    style START fill:#22c55e\n    style END fill:#ef4444\n    style A fill:#3b82f6\n\\```\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#integration-with-langfuse","title":"Integration with Langfuse","text":"<p>Combine visualization with tracing:</p> <pre><code>import mahsm as ma\n\n# Initialize tracing\nma.tracing.init()\n\n# Build and compile\ngraph = workflow.compile()\n\n# Save diagram for reference\ndiagram = graph.get_graph().draw_mermaid()\nwith open(\"docs/architecture.md\", \"w\") as f:\n    f.write(f\"# Agent Architecture\\n\\n```mermaid\\n{diagram}\\n```\")\n\n# Run with tracing\nresult = graph.invoke({\"question\": \"Test\"})\n\n# Now you can:\n# 1. View structure in architecture.md\n# 2. View actual execution in Langfuse UI\n# 3. Compare expected vs actual flow\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#comparing-workflows","title":"Comparing Workflows","text":""},{"location":"building-blocks/langgraph/visualization/#before-optimization","title":"Before Optimization","text":"<pre><code>graph TD\n    START([START]) --&gt; A[Fetch Data]\n    A --&gt; B[Process 1]\n    B --&gt; C[Process 2]\n    C --&gt; D[Process 3]\n    D --&gt; END([END])\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#after-optimization","title":"After Optimization","text":"<pre><code>graph TD\n    START([START]) --&gt; A[Fetch &amp; Process]\n    A -.-&gt; fast[Fast Path]\n    A -.-&gt; thorough[Thorough Path]\n    fast --&gt; END([END])\n    thorough --&gt; END\n</code></pre> <p>Simplified and added conditional routing</p>"},{"location":"building-blocks/langgraph/visualization/#real-world-examples","title":"Real-World Examples","text":""},{"location":"building-blocks/langgraph/visualization/#research-agent","title":"Research Agent","text":"<pre><code>graph TD\n    START([START]) --&gt; classify[Classify Query]\n    classify -.-&gt; simple[Direct Answer]\n    classify -.-&gt; research[Research Needed]\n\n    research --&gt; search[Web Search]\n    search --&gt; synthesize[Synthesize Results]\n    synthesize --&gt; quality[Quality Check]\n\n    quality -.-&gt; good[High Quality]\n    quality -.-&gt; retry[Low Quality]\n    retry --&gt; search\n\n    simple --&gt; END([END])\n    good --&gt; END\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#code-generation-agent","title":"Code Generation Agent","text":"<pre><code>graph TD\n    START([START]) --&gt; parse[Parse Request]\n    parse --&gt; plan[Generate Plan]\n    plan --&gt; code[Generate Code]\n    code --&gt; test[Run Tests]\n\n    test -.-&gt; pass[Tests Pass]\n    test -.-&gt; fail[Tests Fail]\n\n    fail --&gt; debug[Debug]\n    debug --&gt; code\n\n    pass --&gt; review[Code Review]\n    review -.-&gt; approved[Approved]\n    review -.-&gt; revise[Needs Revision]\n\n    revise --&gt; code\n    approved --&gt; END([END])\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#customer-support-agent","title":"Customer Support Agent","text":"<pre><code>graph TD\n    START([START]) --&gt; intent[Classify Intent]\n\n    intent -.-&gt; faq[FAQ]\n    intent -.-&gt; support[Human Support]\n    intent -.-&gt; technical[Technical Issue]\n\n    faq --&gt; respond[Generate Response]\n    respond --&gt; END([END])\n\n    technical --&gt; diagnose[Diagnose]\n    diagnose -.-&gt; solved[Issue Solved]\n    diagnose -.-&gt; escalate[Escalate]\n\n    solved --&gt; respond\n    escalate --&gt; support\n    support --&gt; END\n</code></pre>"},{"location":"building-blocks/langgraph/visualization/#best-practices","title":"Best Practices","text":""},{"location":"building-blocks/langgraph/visualization/#do","title":"\u2705 Do:","text":"<ol> <li>Visualize before implementation - Catch design issues early</li> <li>Keep diagrams updated - Reflect actual implementation</li> <li>Use meaningful node names - Make diagrams self-documenting</li> <li>Simplify complex flows - If diagram is confusing, workflow probably is too</li> <li>Version control diagrams - Track architecture evolution</li> </ol>"},{"location":"building-blocks/langgraph/visualization/#dont","title":"\u274c Don't:","text":"<ol> <li>Let diagrams get stale - Update when workflow changes</li> <li>Over-complicate - If visualization is a mess, simplify workflow</li> <li>Skip validation - Use diagrams to spot structural issues</li> <li>Forget to share - Diagrams are for communication</li> </ol>"},{"location":"building-blocks/langgraph/visualization/#next-steps","title":"Next Steps","text":"<ul> <li>Build Your First Agent \u2192 Put it all together</li> <li>LangGraph Overview \u2192 Review concepts</li> <li>Examples \u2192 See complete projects</li> </ul>"},{"location":"building-blocks/langgraph/visualization/#external-resources","title":"External Resources","text":"<ul> <li>Mermaid Documentation - Full Mermaid guide</li> <li>Mermaid Live Editor - Test diagrams online</li> <li>LangGraph Examples - Official examples</li> </ul> <p>Congratulations! \ud83c\udf89 You've completed the LangGraph Building Blocks section. Ready to build your first agent? Get Started \u2192</p>"},{"location":"concepts/declarative-design/","title":"Declarative by Design","text":"<p>The central philosophy of <code>mahsm</code> is its declarative approach. Instead of manually writing imperative \"glue code\" to connect different libraries, you declare the components of your system, and <code>mahsm</code> handles the integration and boilerplate.</p> <p>This \"convention over configuration\" approach is designed to let you focus entirely on your agent's business logic, not the plumbing.</p>"},{"location":"concepts/declarative-design/#the-mahsm-approach","title":"The <code>mahsm</code> Approach","text":"<ul> <li>You declare your agent's reasoning by writing standard <code>dspy.Module</code> classes. The powerful <code>@ma.dspy_node</code> decorator instantly makes them compatible with the orchestration layer.</li> <li>You declare your workflow's structure by adding your nodes to a <code>ma.graph.StateGraph</code> and defining the edges between them.</li> <li>You declare your evaluation criteria by configuring the <code>ma.testing.PytestHarness</code> to run your graph against a dataset.</li> </ul>"},{"location":"concepts/declarative-design/#the-benefits","title":"The Benefits","text":"<p>This philosophy drastically reduces boilerplate, improves code readability, and embeds best practices for observability and testing directly into the development process. The result is a workflow that is faster, more robust, and produces systems that are understandable by default.</p>"},{"location":"concepts/four-pillars/","title":"The Four Pillars of <code>mahsm</code>","text":"<p><code>mahsm</code> achieves its power by deeply integrating four essential, best-in-class libraries into a single, seamless experience. All functionality is exposed through the unified <code>import mahsm as ma</code> API, giving you a consistent and clean developer experience.</p>"},{"location":"concepts/four-pillars/#1-dspy-the-reasoning-engine-madspy","title":"1. DSPy: The Reasoning Engine (<code>ma.dspy</code>)","text":"<ul> <li>What it is: A framework from Stanford NLP for programming\u2014not just prompting\u2014language models. It separates program flow from parameters (prompts and model weights) and uses optimizers to tune them for maximum performance.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> treats DSPy modules as the fundamental building blocks of agent intelligence. The core innovation is the <code>@ma.dspy_node</code> decorator. This tool instantly transforms any <code>dspy.Module</code> into a fully compliant LangGraph node, automatically handling the complex mapping of data from the shared graph <code>State</code> to the module's inputs and back.</li> </ul>"},{"location":"concepts/four-pillars/#2-langgraph-the-orchestration-scaffolding-magraph","title":"2. LangGraph: The Orchestration Scaffolding (<code>ma.graph</code>)","text":"<ul> <li>What it is: A library for building stateful, multi-agent applications by representing them as cyclical graphs. It provides the primitives of <code>State</code>, <code>Nodes</code>, and <code>Edges</code> to create complex, long-running agentic workflows.</li> <li>How <code>mahsm</code> Fuses it: LangGraph provides the skeleton, and <code>mahsm</code> provides the intelligent organs. By making DSPy modules the primary type of \"thinking\" node, <code>mahsm</code> supercharges LangGraph development. You define your application's <code>State</code> and use <code>ma.graph.StateGraph</code> to wire together your <code>@ma.dspy_node</code> agents.</li> </ul>"},{"location":"concepts/four-pillars/#3-langfuse-the-unified-observability-layer","title":"3. LangFuse: The Unified Observability Layer","text":"<ul> <li>What it is: A comprehensive open-source platform for LLM observability, providing detailed tracing, debugging, and analytics for AI applications.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> makes deep, hierarchical tracing an automatic, zero-effort feature. The single <code>ma.init()</code> function simultaneously instruments both LangGraph and DSPy. When you run your graph, <code>mahsm</code> creates a single, unified trace in LangFuse that captures both the high-level graph flow and the low-level DSPy execution details (prompts, tool calls, etc.), solving the massive pain point of achieving end-to-end observability.</li> </ul>"},{"location":"concepts/four-pillars/#4-evalprotocol-the-quality-control-testing-framework-matesting","title":"4. EvalProtocol: The Quality Control &amp; Testing Framework (<code>ma.testing</code>)","text":"<ul> <li>What it is: A standardized, <code>pytest</code>-based framework for evaluating the performance of AI systems using LLM-as-a-judge and other metrics.</li> <li>How <code>mahsm</code> Fuses it: <code>mahsm</code> bridges the gap between your built application and your test suite. The <code>ma.testing.PytestHarness</code> class radically simplifies setup by automatically generating the boilerplate processors required by <code>eval-protocol</code>. The harness can even pull evaluation datasets directly from your production LangFuse traces, enabling a tight, continuous loop of deploying, observing, and evaluating your system's real-world performance.</li> </ul>"},{"location":"concepts/workflow/","title":"The <code>mahsm</code> Development Workflow","text":"<p>Developing with <code>mahsm</code> follows a simple, iterative, and powerful three-step loop: Build, Trace, and Evaluate. This cycle is designed to be fast and data-driven, ensuring you are creating high-quality, robust systems.</p> <p> </p>"},{"location":"concepts/workflow/#1-build","title":"1. Build","text":"<p>This is the core development step where you write idiomatic <code>mahsm</code> code. - Define State: Create a <code>TypedDict</code> that represents the shared state of your application. - Create Nodes: Write <code>dspy.Module</code> classes to encapsulate the reasoning logic for your agents. Decorate them with <code>@ma.dspy_node</code>. - Wire Graph: Add your nodes to a <code>ma.graph.StateGraph</code> and define the edges to control the flow of execution. - Compile: Call <code>.compile()</code> on your graph to create the runnable application.</p>"},{"location":"concepts/workflow/#2-trace","title":"2. Trace","text":"<p>Once built, you run your application.     *   With <code>ma.init()</code> called at the start of your script, every execution is automatically and deeply traced in LangFuse.     *   You use the LangFuse UI to inspect the full decision-making process of your agent, debug issues, understand latency, and analyze token usage.     *   You can tag interesting traces to save them as examples for regression testing or for creating evaluation datasets.</p>"},{"location":"concepts/workflow/#3-evaluate","title":"3. Evaluate","text":"<p>Finally, you verify the quality of your agent's output.     *   Write a Test File: Create a standard <code>pytest</code> file.     *   Configure the Harness: Use the <code>ma.testing.PytestHarness</code> to connect your compiled <code>mahsm</code> graph to the evaluation protocol.     *   Run Eval: Use datasets (potentially generated from your production traces in LangFuse) to run an evaluation.     *   Analyze &amp; Iterate: Use the evaluation leaderboards and results to identify weaknesses in your agent's logic, then go back to the BUILD step to improve it.</p>"},{"location":"design/ma.tuning-spec/","title":"Ma.tuning spec","text":"<p># mahsm.tuning \u2014 A single-module abstraction for SFT/DPO/RL/LoRA</p> <p>Status: Draft (for initial PR)</p> <p>Owners: @SohumKothavade</p> <p>Last updated: 2025-11-01</p> <p>## Executive summary</p> <p>Goal: Add one concise, universal module \u2014 <code>mahsm.tuning</code> \u2014 that lets any mahsm agent participate in post\u2011training (SFT, DPO, RL) with adapter publication (LoRA or full weights), while keeping the runtime fast and simple. We adopt training\u2013agent disaggregation: the agent runtime emits learning events; a remote trainer consumes events and returns artefacts via an OpenAI\u2011compatible endpoint. This follows the pattern demonstrated by Microsoft Agent Lightning (trainer/agent disaggregation with OpenAI\u2011style serving) and Arbor (DSPy\u2011centric RL server) [1][2][3].</p> <p>Design anchors:  - Stable abstraction is the event/trajectory schema, not any single algorithm [1][3][4].  - One \u201cstage\u201d contract for all methods: collect \u2192 curate \u2192 optimize \u2192 apply.  - Trainer adapters (TRL, Agent Lightning/veRL, Arbor) implement a tiny <code>fit(dataset, config) -&gt; artefact</code> API.  - Publishing is pluggable (LoRA adapter, new model version, policy swap).</p> <p>Non\u2011goals (initial PR): implement full PPO/GRPO in\u2011process, build a bespoke trace store, or replace LangFuse.</p> <p>## Reusing existing mahsm integrations (zero extra plumbing)</p> <ul> <li>Traces: we keep using LangFuse; <code>tuning.emit()</code> serializes to LangFuse so downstream is unchanged [7][8].</li> <li>Evals: reuse <code>mahsm.testing.PytestHarness</code> (EvalProtocol) to pull traces/datasets and run graph rollouts as guards.</li> </ul> <p>Example guard flow tying evals into a stage:</p> <pre><code>from mahsm import testing\nfrom mahsm import tuning as mt\n\nplan = mt.Plan(...)\n\ndef guard_ok(graph, metrics_req: dict) -&gt; bool:\n    harness = testing.PytestHarness(graph)\n    harness.from_langfuse(project=\"proj1\", task=\"codegen\")\n    # Run rollouts via EvalProtocol; compute metrics of interest (pseudo)\n    results = harness.rollout_processor.run(harness.data_loaders)\n    return results.metrics &gt;= metrics_req\n\n# Inside mt.run(): after s.apply(artefact), call guard_ok(graph, plan.guard)\n</code></pre> <p>This keeps LangGraph/DSPy/ LangFuse/EvalProtocol exactly as\u2011is; <code>mahsm.tuning</code> just coordinates stages.</p> <p>## Why this now</p> <ul> <li>State of the art converges on: (a) decoupled runtime vs trainer, (b) unified MDP\u2011like trace schema, (c) OpenAI\u2011style serving of the optimized policy [1][3][4].</li> <li>Existing trainers (TRL for SFT/DPO; veRL/LightningRL for RL; Arbor for DSPy programs) can be consumed through thin adapters rather than re\u2011implemented [2][4][5][6].</li> </ul> <p>## Architecture (three planes)</p> <pre><code>graph LR\n  subgraph Runtime plane (mahsm graphs)\n    A[Agent graph (LangGraph + DSPy)] -- emits --&gt; E((Learning events))\n    A -- queries --&gt; Svc(OpenAI\u2011ish Inference Endpoint)\n  end\n\n  subgraph Data/trace plane\n    E\n    Store[(Trace store e.g., LangFuse)]\n    E -- normalized schema --&gt; Store\n  end\n\n  subgraph Learning plane (remote trainer)\n    T[Trainer adapters: TRL / Agent Lightning(veRL) / Arbor]\n    T -- read --&gt; Store\n    T -- artefacts --&gt; Pub[(Artefacts: LoRA, weights)]\n    Pub -- served as --&gt; Svc\n  end\n</code></pre> <p>Consequence: the agent stays responsive; training scales independently; algorithms are hot\u2011swappable.</p> <p>## Disaggregated trainer\u2013agent: what actually happens</p> <p>Steps (typical online RL loop):  1) Runtime executes graph; <code>tuning.emit()</code> logs step/episode events to LangFuse.  2) Trainer service (e.g., VERL/Lightning) reads trajectories from LangFuse (or a mirrored store) [1][4].  3) Trainer optimizes (GRPO/PPO/DAPO/\u2026); outputs an artefact (LoRA or weights) [4].  4) Publisher exposes the artefact behind an OpenAI\u2011compatible endpoint (could be vLLM/SGLang) [1][4].  5) Runtime continues calling the same endpoint; it now serves updated weights. No runtime code changes.</p> <p>Sequence diagram:</p> <pre><code>sequenceDiagram\n  participant User\n  participant Runtime as mahsm runtime (LangGraph+DSPy)\n  participant LangFuse as Trace Store\n  participant Trainer as Trainer (veRL/Lightning/TRL+server)\n  participant Inference as OpenAI\u2011style Inference\n\n  User-&gt;&gt;Runtime: invoke graph(input)\n  Runtime-&gt;&gt;LangFuse: emit(Event: step/start/tool/reward)\n  Runtime-&gt;&gt;Inference: POST /chat/completions\n  Inference--&gt;&gt;Runtime: completion(tokens)\n  Note over Runtime,LangFuse: episode completes\n  Trainer-&gt;&gt;LangFuse: query trajectories\n  Trainer-&gt;&gt;Trainer: optimize (GRPO/PPO/DPO/SFT)\n  Trainer--&gt;&gt;Inference: publish artefact (LoRA/weights)\n  User-&gt;&gt;Runtime: next request (same code)\n  Runtime-&gt;&gt;Inference: POST /chat/completions\n  Inference--&gt;&gt;Runtime: improved policy\n</code></pre> <p>Offline SFT/DPO is identical except source is historic traces/datasets and trainer runs batch jobs (TRL) [5][6][7].</p> <p>## Minimal surface area (single module)</p> <p>Add <code>mahsm/tuning.py</code> with four concepts and a small public API. No new top\u2011level packages.</p> <p>### 1) Event schema (stable core)</p> <p>A minimal MDP-ish schema sufficient for SFT/DPO/RL collection.</p> <pre><code># sketch only \u2014 implemented as dataclasses or TypedDicts\nclass Episode: id: str; task: str|None; metadata: dict\nclass Step: idx: int; input: dict; output: dict; tool_calls: list[dict]; lat_ms: int|None\nclass Reward: value: float; source: str  # e.g., \"user\", \"auto-metric\", \"rm\"\nclass Event: episode_id: str; step: Step|None; reward: Reward|None; tags: list[str]; ts: float\n</code></pre> <p>Emission policy:  - Node start, node end, tool call(s), final answer, score/reward events.  - API: <code>ma.tuning.emit(event)</code> and convenience wrappers for common node hooks.</p> <p>Mapping to LangFuse:  - Provide a serializer: <code>Event -&gt; LangFuse trace/observation</code> and back, keeping all downstream datasets uniform [7][8].</p> <p>### 2) Dataset transforms</p> <p>Three canonical transforms from events or traces:  - <code>to_sft(events|trace_query, *, template=...) -&gt; Iterable[SFTExample]</code>  - <code>to_preferences(events|trace_query, *, pair_by=..., filters=...) -&gt; Iterable[DPOExample]</code>  - <code>to_trajectories(events|stream, *, window=episode|n_steps, filters=...) -&gt; Iterable[Trajectory]</code></p> <p>Each supports simple, composable filters: top\u2011k by reward, dedupe identical prompts, drop unsafe, tool\u2011success\u2011only, etc.</p> <p>### 3) Trainer adapters</p> <p>Unify all algorithms behind a single protocol.</p> <pre><code>class Artefact:\n    kind: Literal[\"lora\", \"full_weights\", \"prompt\", \"policy\"]\n    uri: str  # where to fetch it\n\nclass TrainerAdapter(Protocol):\n    def fit(self, dataset: Iterable, config: dict) -&gt; Artefact: ...\n\n# Built\u2011ins (thin wrappers)\nTRL_SFT, TRL_DPO, VERL_RL, Arbor_RL\n</code></pre> <ul> <li>TRL: calls HF TRL\u2019s SFT/DPO trainers [5][6].</li> <li>VERL/Lightning: consumes trajectories, supports disaggregated rollout; expects OpenAI\u2011ish serving in front [4].</li> <li>Arbor: remote DSPy optimizer server; feeds DSPy programs/episodes [2].</li> </ul> <p>### 4) Publishing (apply)</p> <p>Minimal application targets:  - <code>apply_lora(artefact)</code> \u2014 attach LoRA to the active LM in DSPy (<code>dspy.settings.configure(lm=...)</code>).  - <code>apply_model(artefact)</code> \u2014 switch to a new base model/endpoint.  - <code>apply_policy(artefact)</code> \u2014 hot\u2011swap a graph policy (advanced/future).</p> <p>The apply functions use Mahsm\u2019s existing DSPy/graph wrappers; no trainer coupling.</p> <p>## One universal \u201cstage\u201d abstraction</p> <p>A stage composes: source \u2192 curate \u2192 optimize \u2192 apply.</p> <pre><code>@dataclass\nclass Source:\n    uri: str  # e.g., live://graph/&lt;id&gt;, trace://langfuse/&lt;project&gt;/&lt;task&gt;, dataset://hf/&lt;name&gt;, replay://buffer\n\n@dataclass\nclass Stage:\n    name: str\n    source: Source                      # where experience comes from\n    curate: Callable[..., Iterable]     # to_sft / to_preferences / to_trajectories\n    optimize: TrainerAdapter            # SFT / DPO / RL (via adapter)\n    apply: Callable[[Artefact], None]   # adapter://lora, model://version, policy://swap\n\n@dataclass\nclass Plan:\n    stages: list[Stage]\n    guard: dict | None = None  # simple acceptance criteria, e.g., min metrics\n\ndef run(plan: Plan):\n    for s in plan.stages:\n        ds = s.curate(resolve(s.source))\n        artefact = s.optimize.fit(ds, config={})\n        s.apply(artefact)\n</code></pre> <p>This keeps all methods uniform. SFT is just <code>trace -&gt; to_sft -&gt; TRL_SFT -&gt; apply_lora</code>. Online RL is <code>live -&gt; to_trajectories -&gt; VERL_RL -&gt; apply_lora</code>.</p> <p>## Algorithm catalog (what each needs and how it plugs in)</p> Algorithm Family Needs Online/Offline Typical adapter Publish Notes SFT Supervised (prompt, output) pairs Offline TRL SFT [6] LoRA/full Easiest bootstrap from traces. DPO Preferences (prompt, chosen, rejected) Offline TRL DPO [5][6] LoRA/full Direct preference optimization; no RM. ORPO Preferences (prompt, chosen, rejected) Offline TRL ORPO [9] LoRA/full Odds\u2011ratio variant; stable. IPO Preferences (prompt, chosen, rejected) Offline TRL IPO [9] LoRA/full Inverse preference opt. KTO Preferences (prompt, output, utility) Offline TRL KTO [10] LoRA/full Prospect\u2011theory\u2011inspired. SimPO Preferences (prompt, chosen, rejected) Offline TRL SimPO [9] LoRA/full Simple preference opt. PPO RL trajectories + reward fn/RM Online/Offline TRL PPO [11], VERL PPO [4] LoRA/full Classic on\u2011policy RL. GRPO RL trajectories + relative rewards Online VERL/Lightning [4], ART [12] LoRA/full Efficient group\u2011relative PPO. RLOO RL trajectories + rewards Online TRL RLOO [9] LoRA/full Leave\u2011one\u2011out RL. XPO RL trajectories + rewards Online TRL XPO [9] LoRA/full Cross\u2011policy opt. Online DPO Preferences streaming prefs Online TRL Online DPO [9] LoRA/full Preference online. NashMD RL multi\u2011agent trajectories Online TRL NashMD [9] LoRA/full Nash mean\u2011field dynamics. DAPO RL trajectories + action advantage Online VERL DAPO [4] LoRA/full Advantage\u2011based. PRM / RewardModel RM (prompt, chosen, rejected) Offline TRL Reward/PRM [9] n/a Trains RM for RLHF. BCO Offline RL logged behavior Offline TRL BCO [9] LoRA/full Behavioral cloning variants. CPO Constrained RL trajectories + constraints Offline TRL CPO [9] LoRA/full Constraint satisfaction. <p>All of these reduce to choosing: source (traces/live) \u2192 curate (to_sft/to_preferences/to_trajectories) \u2192 optimize (adapter) \u2192 apply (LoRA/model).</p> <p>## Public API sketch (what users write)</p> <pre><code>import mahsm as ma\nfrom mahsm import tuning as mt\n\n# 1) Wrap existing graph (no changes to nodes)\ngraph = build_graph_somewhere()\n\n# 2) Start event collection (LangFuse is already set up in ma.tracing)\ncollector = mt.collect_from(graph, project=\"proj1\", task=\"codegen\")\n\n# 3) Define a staged plan\nplan = mt.Plan(\n    stages=[\n        mt.Stage(\n            name=\"bootstrap-sft\",\n            source=mt.Source(\"trace://langfuse/proj1/codegen\"),\n            curate=mt.to_sft,\n            optimize=mt.TRL_SFT(model=\"Qwen/Qwen2.5-7B\", lora=True),\n            apply=mt.apply_lora,\n        ),\n        mt.Stage(\n            name=\"align-dpo\",\n            source=mt.Source(\"trace://langfuse/proj1/codegen\"),\n            curate=mt.to_preferences,\n            optimize=mt.TRL_DPO(model=\"Qwen/Qwen2.5-7B\", lora=True),\n            apply=mt.apply_lora,\n        ),\n        mt.Stage(\n            name=\"online-rl\",\n            source=mt.Source(\"live://graph/codegen\"),\n            curate=mt.to_trajectories,\n            optimize=mt.VERL_RL(endpoint=\"http://trainer:8080\"),\n            apply=mt.apply_lora,\n        ),\n    ],\n    guard={\"qa.accuracy\": 0.9},\n)\n\nmt.run(plan)\n</code></pre> <p>## Integration points in mahsm today</p> <ul> <li>Graph + nodes: <code>mahsm.core.dspy_node</code> already unifies node IO; tuning can wrap these with event emit hooks.</li> <li>Tracing: <code>mahsm.tracing</code> (LangFuse) provides spans; tuning serializes its <code>Event</code> schema to the same backend [7].</li> <li>Testing/evals: <code>mahsm.testing.PytestHarness</code> can be the guard runner, logging back via LangFuse.</li> </ul> <p>Minimal code changes to existing modules: none. <code>mahsm.tuning</code> imports <code>mahsm.graph</code>, <code>mahsm.dspy</code>, and optional <code>mahsm.tracing</code>.</p> <p>## MVP scope (first PR)</p> <p>1) Ship <code>mahsm/tuning.py</code> with:     - Event datatypes + <code>emit()</code> + LangFuse serializers.     - <code>to_sft</code>, <code>to_preferences</code>, <code>to_trajectories</code> with basic filters.     - Adapters: <code>TRL_SFT</code>, <code>TRL_DPO</code> (local, single\u2011GPU happy path; LoRA via PEFT). Config dict passthrough.     - <code>apply_lora</code> (DSPy LM adaptor swap) and <code>apply_model</code> (endpoint swap).     - <code>Stage</code>, <code>Plan</code>, and <code>run()</code>.</p> <p>2) Example in <code>docs/getting-started/</code>: \u201cTune your existing mahsm agent in 20 lines\u201d.</p> <p>3) Optional: a tiny <code>LangFuse -&gt; SFT examples</code> cookbook.</p> <p>Out of scope for MVP: online RL training loop, remote rollout orchestration. Those come via adapters next.</p> <p>## Adapters vs. in\u2011house implementations</p> <p>Positioning: start with adapters (fast, low maintenance, inherit upstream advances). As we standardize our event/dataset contracts and find gaps, we can internalize stable algorithms (copy/port) behind the same <code>TrainerAdapter</code> interface without breaking users.</p> <p>## Phase 2 (follow\u2011up PRs)</p> <ul> <li>Adapter: <code>VERL_RL</code> (consumes trajectories, talks to a remote trainer; OpenAI\u2011compatible serving) [4].</li> <li>Adapter: <code>Arbor_RL</code> (connect to Arbor server; DSPy program optimization) [2].</li> <li>Reward model and auto\u2011metrics wiring (generative RM prototypes; or use existing evaluator outputs) [4].</li> <li>Policy hot\u2011swap for LangGraph (advanced publisher).</li> </ul> <p>## Design decisions and rationale</p> <ul> <li>Event schema first: Algorithms churn; trajectories (episode \u2192 steps \u2192 rewards \u2192 metadata) are the durable contract [1][3][4].</li> <li>Single \u201cstage\u201d abstraction: reduces all tuning to four verbs; easiest conceptual on\u2011ramp.</li> <li>Adapters over implementations: leverage TRL/veRL/Arbor and inherit their improvements [2][4][5].</li> <li>Disaggregation by default: keeps mahsm runtime responsive; trainers scale independently [1][3][4].</li> </ul> <p>## Open questions for confirmation</p> <p>1) Default trace store: standardize on LangFuse for now (yes/no)? If no, ship a neutral JSONL store as fallback.  2) First adapters to land: TRL SFT+DPO (MVP), then VERL RL, then Arbor \u2014 agree?  3) Publish targets: prioritize LoRA for size/mobility; defer full\u2011weights? Any preferred PEFT backend/models?  4) Guard API: reuse <code>mahsm.testing</code> evaluators, or keep a lightweight metric callback in <code>tuning.run()</code>?</p> <p>## References</p> <p>[1] Agent Lightning: Train ANY AI Agents with Reinforcement Learning (arXiv, 2025) \u2014 https://arxiv.org/abs/2508.03680</p> <p>[2] Ziems/arbor: A framework for optimizing DSPy programs with RL \u2014 https://github.com/ziems/arbor</p> <p>[3] Agent Lightning project &amp; docs \u2014 https://github.com/microsoft/agent-lightning and https://microsoft.github.io/agent-lightning/latest/</p> <p>[4] veRL/VERL (Volcano Engine RL for LLMs) \u2014 https://github.com/volcengine/verl and release notes \u2014 https://github.com/volcengine/verl/releases</p> <p>[5] Hugging Face TRL (SFT/DPO/RL) \u2014 https://github.com/huggingface/trl</p> <p>[6] TRL documentation: SFT Trainer and DPO Trainer \u2014 https://huggingface.co/docs/trl/en/sft_trainer and https://huggingface.co/docs/trl/en/dpo_trainer</p> <p>[7] LangFuse data/API: query traces via SDKs \u2014 https://langfuse.com/docs/api-and-data-platform/features/query-via-sdk</p> <p>[8] LangFuse export options \u2014 https://langfuse.com/docs/api-and-data-platform/features/export-from-ui</p> <p>[9] TRL index (algorithms overview) \u2014 https://huggingface.co/docs/trl/en/index</p> <p>[10] TRL KTO Trainer \u2014 https://huggingface.co/docs/trl/main/en/kto_trainer</p> <p>[11] TRL PPO Trainer \u2014 https://huggingface.co/docs/trl/main/en/ppo_trainer</p> <p>[12] OpenPipe ART (Agent Reinforcement Trainer, GRPO for agents) \u2014 https://github.com/OpenPipe/ART</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Getting <code>mahsm</code> installed is quick and easy. We recommend using a virtual environment to manage your project's dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>uv</code> (or <code>pip</code>) package installer</li> </ul>"},{"location":"getting-started/installation/#installing-mahsm","title":"Installing <code>mahsm</code>","text":"<p>To install the core <code>mahsm</code> library, run the following command:</p> <pre><code>uv pip install mahsm\n</code></pre> <p>This will install mahsm and its core dependencies, including DSPy, LangGraph, LangFuse, and EvalProtocol.</p>"},{"location":"getting-started/installation/#setting-up-observability-langfuse","title":"Setting Up Observability (LangFuse)","text":"<p>One of the core features of mahsm is its deep integration with LangFuse for observability. To enable it, you need to set the following environment variables: <pre><code>export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nexport LANGFUSE_SECRET_KEY=\"sk-lf-...\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\" # Or your self-hosted instance\n</code></pre></p> <p>You can find your keys in your LangFuse project settings.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart: Building Your First mahsm Agent","text":"<p>Let's build a simple research agent in 60 seconds to see how mahsm works. This example demonstrates the declarative nature of the framework.</p>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<pre><code>import mahsm as ma\nfrom typing import TypedDict, Optional\nimport dspy\nimport os\n\n# 1. Configure DSPy\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n\n# 2. Initialize tracing (do this once at the start of your app)\nma.tracing.init()\n\n# 3. Define the shared state\nclass AgentState(TypedDict):\n    question: str\n    research_result: Optional[str]\n\n# 4. Create a reasoning node with @ma.dspy_node\n@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"question -&gt; research_result\")\n\n    def forward(self, question):\n        return self.predictor(question=question)\n\n# 5. Build and compile the graph\nworkflow = ma.graph.StateGraph(AgentState)\nworkflow.add_node(\"researcher\", Researcher())\nworkflow.add_edge(ma.START, \"researcher\")\nworkflow.add_edge(\"researcher\", ma.END)\ngraph = workflow.compile()\n\n# 6. Run your agent\nresult = graph.invoke({\"question\": \"What is the future of multi-agent AI systems?\"})\nprint(result['research_result'])\n# \u2705 Automatically traced in Langfuse!\n</code></pre> <p>That's it! You've built a fully observable and testable agent with minimal boilerplate.</p>"},{"location":"getting-started/quickstart/#breaking-it-down","title":"Breaking It Down","text":""},{"location":"getting-started/quickstart/#1-configure-dspy","title":"1. Configure DSPy","text":"<pre><code>import dspy\nimport os\n\nlm = dspy.LM('openai/gpt-4o-mini', api_key=os.getenv(\"OPENAI_API_KEY\"))\ndspy.configure(lm=lm)\n</code></pre> <p>What's happening: - Configure the language model that DSPy will use - <code>dspy.LM()</code> supports OpenAI, Anthropic, local models, and more - Use environment variables for API keys (never hardcode!)</p>"},{"location":"getting-started/quickstart/#2-initialize-tracing","title":"2. Initialize Tracing","text":"<pre><code>ma.tracing.init()\n</code></pre> <p>What's happening: - One line enables automatic tracing for all LLM calls - Traces are sent to Langfuse for observability - Make sure you have <code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, and <code>LANGFUSE_BASE_URL</code> in your environment</p>"},{"location":"getting-started/quickstart/#3-define-state","title":"3. Define State","text":"<pre><code>from typing import TypedDict, Optional\n\nclass AgentState(TypedDict):\n    question: str\n    research_result: Optional[str]\n</code></pre> <p>What's happening: - State is a TypedDict that flows through your workflow - <code>question</code> is required (input) - <code>research_result</code> is optional (populated by nodes) - Type-safe and IDE-friendly</p>"},{"location":"getting-started/quickstart/#4-create-a-dspy-node","title":"4. Create a DSPy Node","text":"<pre><code>@ma.dspy_node\nclass Researcher(ma.Module):\n    def __init__(self):\n        super().__init__()\n        self.predictor = dspy.ChainOfThought(\"question -&gt; research_result\")\n\n    def forward(self, question):\n        return self.predictor(question=question)\n</code></pre> <p>What's happening: - <code>@ma.dspy_node</code> makes your DSPy module work with LangGraph - <code>ChainOfThought</code> adds reasoning before answering - Signature <code>\"question -&gt; research_result\"</code> matches state keys - <code>forward()</code> method contains your logic</p>"},{"location":"getting-started/quickstart/#5-build-the-workflow","title":"5. Build the Workflow","text":"<pre><code>workflow = ma.graph.StateGraph(AgentState)\nworkflow.add_node(\"researcher\", Researcher())\nworkflow.add_edge(ma.START, \"researcher\")\nworkflow.add_edge(\"researcher\", ma.END)\ngraph = workflow.compile()\n</code></pre> <p>What's happening: - <code>StateGraph(AgentState)</code> creates a workflow with your state schema - <code>add_node()</code> adds your Researcher to the graph - <code>add_edge()</code> defines the flow: START \u2192 researcher \u2192 END - <code>compile()</code> turns the workflow into an executable graph</p>"},{"location":"getting-started/quickstart/#6-run-your-agent","title":"6. Run Your Agent","text":"<pre><code>result = graph.invoke({\"question\": \"What is the future of multi-agent AI systems?\"})\nprint(result['research_result'])\n</code></pre> <p>What's happening: - <code>invoke()</code> runs the workflow with initial state - State flows through nodes, getting updated along the way - Returns the final state with all populated fields - All LLM calls are automatically traced to Langfuse!</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts \u2192 Understand the mahsm philosophy</li> <li>DSPy Overview \u2192 Learn about DSPy modules</li> <li>LangGraph Overview \u2192 Learn about workflows</li> <li>Your First Agent \u2192 Build a complete multi-step agent</li> </ul> <p>Ready to build more? Explore the Building Blocks! \ud83d\ude80</p>"}]}